# GUIA DE USO RÃPIDO - digiNEV v.final

**Atualizado:** 04 de outubro de 2025
**Status:** âœ… Sistema operacional e validado

## ğŸš€ EXECUÃ‡ÃƒO IMEDIATA

### Comando Principal
```bash
python run_pipeline.py
```
> Processa automaticamente todos os 11 datasets encontrados em `data/`

### Teste RÃ¡pido (2 minutos)
```bash
python test_clean_analyzer.py
```
> Valida o sistema completo com dados controlados

### Dataset EspecÃ­fico
```bash
python run_pipeline.py --dataset data/controlled_test_100.csv
python run_pipeline.py --dataset data/1_2019-2021-govbolso.csv
```

### Dashboard
```bash
python src/dashboard/start_dashboard.py
```
> VisualizaÃ§Ã£o dos resultados processados

## ğŸ“Š O QUE ESPERAR

### Resultado do Processamento
```
âœ… EXECUTION COMPLETED
â±ï¸  Total duration: X.Xs
ğŸ“Š Datasets processed: X
ğŸ“ˆ Records processed: X
ğŸ”§ Stages executed: 17
ğŸ“Š Final progress: 100.0%
```

### Colunas Geradas: **102 colunas**
- **Estruturais:** id, body, channel, datetime, etc.
- **Features:** hashtags, URLs, mentions, emojis
- **Qualidade:** dupli_freq, content_quality_score
- **PolÃ­tica:** political_orientation, political_keywords
- **LinguÃ­stica:** spacy_tokens, lemmatized_text
- **TF-IDF:** tfidf_top_terms, tfidf_score_max
- **Clustering:** cluster_id, cluster_distance
- **Temporal:** hour, day_of_week, month
- **Rede:** coordination_score, temporal_pattern
- **SemÃ¢ntica:** sentiment_polarity, emotion_intensity

## ğŸ”„ PIPELINE EM 4 FASES

### Fase 1: PreparaÃ§Ã£o (Stages 01-02)
```
01. Feature Extraction â†’ Detecta colunas automaticamente
02. Text Preprocessing â†’ Limpa e normaliza texto
```

### Fase 2: ReduÃ§Ã£o Inteligente (Stages 03-06)
```
03. Deduplication â†’ Remove 40-50% duplicatas
04. Statistical Analysis â†’ AnÃ¡lise comparativa
05. Quality Filter â†’ Remove 15-25% baixa qualidade
06. Political Filter â†’ MantÃ©m apenas conteÃºdo polÃ­tico (30-40% reduÃ§Ã£o)
```

### Fase 3: LinguÃ­stica Otimizada (Stages 07-09)
```
07. spaCy Processing â†’ Tokens, lemmas, POS, entidades
08. Political Classification â†’ Categorias polÃ­ticas brasileiras
09. TF-IDF â†’ VetorizaÃ§Ã£o e top termos
```

### Fase 4: AnÃ¡lises AvanÃ§adas (Stages 10-17)
```
10. Clustering â†’ K-Means clustering
11. Topic Modeling â†’ LDA topic modeling
12. Semantic Analysis â†’ AnÃ¡lise semÃ¢ntica
13. Temporal Analysis â†’ PadrÃµes temporais
14. Network Analysis â†’ CoordenaÃ§Ã£o de rede
15. Domain Analysis â†’ AnÃ¡lise de domÃ­nios
16. Event Context â†’ Contextos polÃ­ticos
17. Channel Analysis â†’ AnÃ¡lise de canais
```

## ğŸ“ DATASETS DISPONÃVEIS

```
data/
â”œâ”€â”€ controlled_test_100.csv (0.0 MB) â† TESTE VALIDADO
â”œâ”€â”€ 1_2019-2021-govbolso.csv (135.9 MB)
â”œâ”€â”€ 2_2021-2022-pandemia.csv (230.0 MB)
â”œâ”€â”€ 3_2022-2023-poseleic.csv (93.2 MB)
â”œâ”€â”€ 4_2022-2023-elec.csv (54.2 MB)
â””â”€â”€ 5_2022-2023-elec-extra.csv (25.2 MB)
```

## âš¡ OTIMIZAÃ‡Ã•ES ATIVAS (100%)

- âœ… **Week 1-2:** Cache inteligente + checkpoints
- âœ… **Week 3:** Processamento paralelo + streaming
- âœ… **Week 4:** Monitoramento em tempo real
- âœ… **Week 5:** GestÃ£o de memÃ³ria + auto-chunking

## ğŸ”§ LOGS E MONITORAMENTO

### Logs TÃ­picos de Sucesso
```
INFO:Analyzer:ğŸ”¬ Iniciando anÃ¡lise OTIMIZADA: X registros
INFO:Analyzer:ğŸ” STAGE 01: Feature Extraction
INFO:Analyzer:âœ… Features detectadas: ['hashtags', 'urls', 'mentions']
INFO:Analyzer:ğŸ”„ STAGE 03: Cross-Dataset Deduplication
INFO:Analyzer:âœ… DeduplicaÃ§Ã£o concluÃ­da: X â†’ Y registros (Z% reduÃ§Ã£o)
INFO:Analyzer:ğŸ¯ STAGE 05: Content Quality Filter
INFO:Analyzer:âœ… Filtro aplicado: X â†’ Y registros (Z% reduÃ§Ã£o)
INFO:Analyzer:âœ… AnÃ¡lise OTIMIZADA concluÃ­da: 102 colunas, 17 stages
```

### VerificaÃ§Ã£o de Sucesso
```bash
python test_clean_analyzer.py
```
Deve mostrar:
```
âœ… TESTE CONCLUÃDO COM SUCESSO!
âœ… Analyzer v.final estÃ¡ funcionalmente correto
âœ… Pipeline interligado e sem reprocessamento
âœ… Apenas dados reais nas colunas geradas
```

## ğŸš¨ RESOLUÃ‡ÃƒO RÃPIDA DE PROBLEMAS

### Erro "No datasets found"
```bash
ls data/*.csv  # Verificar se existem arquivos
```

### Erro "Error tokenizing data"
```bash
# Testar com dataset menor primeiro
python run_pipeline.py --dataset data/controlled_test_100.csv
```

### Erro de memÃ³ria
- Sistema usa auto-chunking automaticamente
- Configurado para atÃ© 4GB RAM

### Pipeline interrompido
- Sistema retoma automaticamente do Ãºltimo checkpoint
- Use `python run_pipeline.py` para continuar

## ğŸ“ˆ ANÃLISE DOS RESULTADOS

### Arquivo de SaÃ­da
```
src/dashboard/data/dashboard_results/pipeline_results_YYYYMMDD_HHMMSS.json
```

### Principais MÃ©tricas
- **ReduÃ§Ã£o de volume:** ~80% total (300k â†’ 60k registros tÃ­pico)
- **Colunas geradas:** 102 colunas com dados reais
- **ClassificaÃ§Ã£o polÃ­tica:** extrema-direita, direita, centro, esquerda, neutral
- **Quality score:** 0-100 (mÃ©dia ~85 para dados filtrados)
- **DuplicaÃ§Ã£o:** Detectada e quantificada com dupli_freq

### Dashboard
```bash
python src/dashboard/start_dashboard.py
```
- Acesse via navegador (URL mostrada no terminal)
- VisualizaÃ§Ãµes automÃ¡ticas dos dados processados
- GrÃ¡ficos de distribuiÃ§Ã£o polÃ­tica, temporal, qualidade

## ğŸ¯ COMANDOS ESSENCIAIS

```bash
# ExecuÃ§Ã£o completa
python run_pipeline.py

# Teste rÃ¡pido
python test_clean_analyzer.py

# Dataset especÃ­fico
python run_pipeline.py --dataset data/controlled_test_100.csv

# Dashboard
python src/dashboard/start_dashboard.py

# Verificar dados
ls data/*.csv

# Verificar resultados
ls src/dashboard/data/dashboard_results/
```

## ğŸ’¡ DICAS DE USO

1. **Primeira execuÃ§Ã£o:** Sempre usar `controlled_test_100.csv` para validar
2. **Datasets grandes:** Sistema processa automaticamente em chunks
3. **InterrupÃ§Ã£o:** Pipeline retoma do Ãºltimo checkpoint
4. **Logs detalhados:** Acompanhar progresso em tempo real
5. **Resultados:** Verificar dashboard para visualizaÃ§Ãµes

---

**Sistema validado e operacional** âœ…
**Ãšltima validaÃ§Ã£o:** 04/10/2025
**Commit:** d9acb89