# Processing Configuration for digiNEV v5.1.0
# Configuration for pipeline processing, timeouts, and concurrent operations

# Pipeline Timeouts Configuration
pipeline_timeouts:
  # Stage-specific timeouts (in seconds)
  stage_01_chunk_processing: 60
  stage_02_encoding_validation: 30
  stage_03_deduplication: 120
  stage_04_validation: 45
  stage_05_political_analysis: 180  # Optimized for Brazilian data (reduced from 300s)
  stage_06_text_cleaning: 180      # AI-enhanced
  stage_07_spacy_processing: 240
  stage_08_sentiment_analysis: 300  # AI-enhanced
  stage_09_topic_modeling: 240     # AI-enhanced
  stage_10_tfidf_analysis: 180     # AI-enhanced
  stage_11_clustering: 240         # AI-enhanced
  stage_12_hashtag_analysis: 120
  stage_13_domain_analysis: 120
  stage_14_temporal_analysis: 90
  stage_15_network_analysis: 300   # AI-enhanced
  stage_16_qualitative_coding: 300 # AI-enhanced
  stage_17_pattern_detection: 180
  stage_18_correlation_analysis: 120
  stage_19_semantic_search: 240    # AI-enhanced
  stage_20_validation_cross: 120
  stage_21_aggregation: 90
  stage_22_final_report: 120

  # Global pipeline timeout (22 stages total)
  complete_pipeline: 3600  # 1 hour maximum for full pipeline

  # Quick analysis timeout (optimized pipeline)
  quick_analysis: 300  # 5 minutes for 3-stage optimized

  # Scientific analysis timeout (13 stages)
  scientific_analysis: 1800  # 30 minutes for scientific methods

# Batch Processing Configuration
batch_processing:
  # Maximum number of worker threads
  max_workers: 4

  # Chunk sizes for different data sizes
  small_dataset_chunk_size: 100    # < 1K records
  medium_dataset_chunk_size: 250   # 1K-10K records
  large_dataset_chunk_size: 500    # > 10K records

  # Memory management
  max_memory_per_worker_mb: 1000
  memory_check_interval: 30

  # I/O optimization
  enable_async_io: true
  max_concurrent_reads: 2
  max_concurrent_writes: 2

# Processing Types and Concurrency Limits
processing_types:
  # AI-enhanced processing (API-based)
  ai_analysis:
    max_concurrent_requests: 3
    retry_count: 3
    retry_delay_seconds: 2
    timeout_seconds: 300

  # Heuristic processing (local computation)
  heuristic_analysis:
    max_concurrent_requests: 6
    retry_count: 2
    retry_delay_seconds: 1
    timeout_seconds: 120

  # I/O operations
  file_operations:
    max_concurrent_requests: 4
    retry_count: 2
    retry_delay_seconds: 0.5
    timeout_seconds: 60

  # Network operations (embeddings, external APIs)
  network_operations:
    max_concurrent_requests: 2
    retry_count: 3
    retry_delay_seconds: 5
    timeout_seconds: 600

# Adaptive Chunking Configuration
adaptive_chunking:
  # Enable adaptive chunk sizing based on content
  enabled: true

  # Base chunk sizes
  min_chunk_size: 50
  max_chunk_size: 1000
  target_chunk_size: 250

  # Content-based adjustment factors
  text_density_factor: 1.2  # Increase chunk size for text-heavy content
  metadata_factor: 0.8      # Decrease chunk size for metadata-heavy content

  # Memory-based adjustments
  memory_pressure_threshold: 0.8  # Reduce chunk size when memory > 80%
  memory_adjustment_factor: 0.7

  # Performance-based adjustments
  slow_processing_threshold: 10.0  # seconds per chunk
  performance_adjustment_factor: 1.5

# Cache Configuration
cache_processing:
  # Cache settings for different stages (optimized for cost reduction)
  enable_stage_caching: true
  cache_size_mb: 800            # Increased cache for better hit rate
  cache_ttl_seconds: 7200       # Extended TTL for API cost reduction

  # Cache strategies by stage type (Brazilian data optimized)
  ai_stage_cache_ttl: 14400     # 4 hours for AI results (cost optimization)
  heuristic_stage_cache_ttl: 3600  # 1 hour for heuristic results
  file_cache_ttl: 1800          # 30 minutes for file operations
  political_analysis_cache_ttl: 21600  # 6 hours for political analysis (expensive)

# Error Handling and Recovery
error_handling:
  # Maximum retries for different error types
  api_error_retries: 3
  network_error_retries: 2
  memory_error_retries: 1

  # Backoff strategies
  exponential_backoff_base: 2
  max_backoff_seconds: 60

  # Fallback behavior - DISABLED BY USER REQUEST
  enable_fallbacks: false
  fallback_to_heuristic: false
  continue_on_non_critical_errors: false
  strict_implementation_mode: true
  fail_on_missing_implementation: true

  # Critical stages that must succeed (ALL stages are now critical)
  critical_stages:
    - "stage_01_chunk_processing"
    - "stage_02_encoding_validation"
    - "stage_03_deduplication"
    - "stage_04_validation"
    - "stage_05_political_analysis"
    - "stage_06_text_cleaning"
    - "stage_07_linguistic_processing"
    - "stage_08_sentiment_analysis"
    - "stage_09_topic_modeling"
    - "stage_10_tfidf_extraction"
    - "stage_11_clustering"
    - "stage_12_hashtag_normalization"
    - "stage_13_domain_analysis"
    - "stage_14_temporal_analysis"
    - "stage_15_network_analysis"
    - "stage_16_qualitative_analysis"
    - "stage_17_pattern_detection"
    - "stage_18_anomaly_detection"
    - "stage_19_semantic_search"
    - "stage_20_pipeline_validation"
    - "stage_21_result_aggregation"
    - "stage_22_final_report"

# Quality Control
quality_control:
  # Data validation thresholds
  min_text_length: 10
  max_text_length: 10000
  min_confidence_score: 0.3

  # Result validation
  validate_output_schema: true
  require_minimum_columns: true
  check_data_integrity: true

  # Performance monitoring
  track_execution_time: true
  track_memory_usage: true
  log_slow_operations: true
  slow_operation_threshold: 30  # seconds

# Academic Research Settings
academic_settings:
  # Citation and provenance tracking
  track_method_provenance: true
  generate_method_citations: true

  # Reproducibility
  set_random_seeds: true
  log_all_parameters: true
  save_intermediate_results: false  # Set to true for debugging

  # Validation requirements
  require_statistical_validation: true
  minimum_sample_size: 50
  confidence_level: 0.95

# Brazilian Portuguese Optimization Settings
portuguese_optimization:
  # Language-specific processing
  spacy_model: "pt_core_news_lg"    # Portuguese large model
  encoding: "utf-8"                 # Ensure UTF-8 for Portuguese characters
  text_normalization: true          # Normalize Portuguese text variants

  # Political filtering optimization (reduce from 85% to 25-35% retention)
  political_filtering:
    retention_target_min: 0.25      # Minimum 25% retention
    retention_target_max: 0.35      # Maximum 35% retention
    aggressive_filtering: false     # Disable aggressive 85% filtering
    context_aware_filtering: true   # Use contextual political analysis

  # Brazilian political lexicon optimization
  lexicon_settings:
    use_hierarchical_lexicon: true
    context_window: 50              # Characters around political terms
    confidence_threshold: 0.6       # Lower threshold for better retention

# System Resource Limits
resource_limits:
  # Memory limits (aligned with 4GB target - adaptive management)
  max_total_memory_mb: 3800     # Reduced from 4GB for safety margin
  warning_memory_threshold_mb: 2800  # Earlier warning for large datasets
  critical_memory_threshold_mb: 3400  # More conservative critical threshold
  adaptive_memory_scaling: true # Enable dynamic memory scaling for Brazilian datasets

  # CPU limits
  max_cpu_usage_percent: 80
  cpu_monitoring_interval: 10

  # Disk space
  min_free_disk_gb: 1
  temp_file_cleanup: true
  max_temp_file_age_hours: 24

# Debugging and Development
debug_settings:
  # Logging levels for different components
  pipeline_log_level: "INFO"
  stage_log_level: "INFO"
  cache_log_level: "WARNING"

  # Development features
  enable_profiling: false
  save_profiling_data: false
  detailed_error_traces: true

  # Testing overrides
  test_mode: false
  mock_api_calls: false
  use_small_datasets: false