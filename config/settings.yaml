# Academic Research Configuration v.final - UNIFIED
# ===================================================
# Master configuration for digiNEV v.final consolidated system
# Optimized for Analyzer v.final with 81+ columns output
#
# CONSOLIDAÇÃO FINAL: Sistema único centralizado
# Eliminates: All parallel architectures and fragmented configurations

# =============================================================================
# ACADEMIC RESEARCH SETTINGS - CORE
# =============================================================================
academic:
  enabled: true
  version: "v.final"
  monthly_budget: 50.0                    # Conservative academic budget
  memory_limit_gb: 4.0                    # RAM constraint
  research_focus: "brazilian_politics"
  portuguese_optimization: true           # pt-BR text optimization
  cache_optimization: true                # Week 1-2 optimizations (40% cost reduction)
  cost_control_priority: "high"
  intelligent_sampling: true

  # Scientific Analyzer Configuration
  scientific_analyzer:
    target_columns: 64                     # Expand from 34 to 64+ columns
    stages_enabled: 13                     # All 13 scientific stages
    fallback_mode: "heuristic"             # Academic heuristics when API fails
    citation_tracking: true                # Track method citations

  # Research categories for Brazilian political analysis
  political_categories:
    - "extrema-direita"
    - "direita"
    - "centro-direita"
    - "centro"
    - "centro-esquerda"
    - "esquerda"

# =============================================================================
# API CONFIGURATION - CONSOLIDATED
# =============================================================================
anthropic:
  enable_api_integration: true
  api_key: "${ANTHROPIC_API_KEY}"

  # Model hierarchy (cost-optimized for academic use)
  models:
    primary: "claude-3-5-haiku-20241022"      # Most cost-effective
    enhanced: "claude-3-5-sonnet-20241022"   # For complex analysis
    fallback: "claude-3-5-haiku-20241022"    # Always fallback to cheapest

  # Academic processing limits
  rate_limit: 20                              # Conservative rate limiting
  max_tokens: 1000                            # Balanced output length
  temperature: 0.3                            # Consistent results for research
  batch_size: 100                             # Optimize for cost
  requests_per_minute: 50
  tokens_per_minute: 40000

  # Stage-specific configurations
  stage_configurations:
    political_analysis:
      model: "claude-3-5-haiku-20241022"
      temperature: 0.1
      max_tokens: 3000
      confidence_threshold: 0.7
    sentiment_analysis:
      model: "claude-3-5-haiku-20241022"
      temperature: 0.2
      max_tokens: 2200
    linguistic_analysis:
      model: "claude-3-5-haiku-20241022"
      temperature: 0.1
      max_tokens: 2500

# =============================================================================
# CACHE CONFIGURATION - OPTIMIZED FOR COST REDUCTION
# =============================================================================
cache:
  # Emergency embeddings cache (40% cost reduction - Week 1)
  emergency_cache:
    enabled: true
    cache_dir: "cache/academic_embeddings"
    ttl_hours: 48                            # 2 days for research consistency
    max_memory_mb: 256                       # Conservative memory usage
    compression: true

  # Smart Claude cache (Week 2 optimizations)
  smart_cache:
    enabled: true
    cache_dir: "cache/academic_claude"
    ttl_hours: 72                            # 3 days for reproducibility
    semantic_similarity_threshold: 0.85
    portuguese_normalization: true          # Brazilian political terms

  # Unified cache configuration
  unified_cache:
    enabled: true
    size_mb: 800                             # Increased for better hit rate
    ttl_seconds: 7200                        # Extended TTL for cost reduction
    ai_stage_cache_ttl: 14400                # 4 hours for AI results
    heuristic_stage_cache_ttl: 3600          # 1 hour for heuristic results
    political_analysis_cache_ttl: 21600      # 6 hours (expensive operations)
  
# =============================================================================
# VOYAGE.AI CONFIGURATION
# =============================================================================
voyage_embeddings:
  model: "voyage-3.5-lite"                   # Most economical option
  cache_enabled: true
  batch_size: 128
  sampling_rate: 0.1                         # 90% sampling for better coverage
  input_type: "document"
  dimensions: 1024
  max_tokens: 32000
  similarity_threshold: 0.75
  requests_per_minute: 300
  
# =============================================================================
# ANALYZER PIPELINE - 14 STAGES
# =============================================================================
analyzer_pipeline:
  # Analysis stages (Analyzer v.final)
  stages_enabled:
    - "stage_01_preprocessing"
    - "stage_02_text_mining"            # Political classification
    - "stage_03_statistical_analysis"
    - "stage_04_semantic_analysis"
    - "stage_05_tfidf_analysis"         # BM25 ranking
    - "stage_06_clustering"             # HDBSCAN clustering
    - "stage_07_topic_modeling"         # Topic discovery
    - "stage_08_evolution_analysis"     # Temporal analysis
    - "stage_09_network_coordination"   # Network detection
    - "stage_10_domain_url_analysis"    # Domain authority
    - "stage_11_event_context"          # Event detection
    - "stage_12_channel_analysis"       # Channel classification
    - "stage_13_linguistic_analysis"    # spaCy processing

  # Academic processing configuration
  chunk_size: 500                       # Memory-optimized chunks
  deduplication_enabled: true
  political_analysis_depth: "full"
  brazilian_optimization: true

  # Timeout configuration (seconds)
  timeouts:
    preprocessing: 60
    political_analysis: 180
    statistical_analysis: 120
    semantic_analysis: 240
    clustering: 300
    linguistic_analysis: 240
    complete_pipeline: 1800             # 30 minutes total
  
# =============================================================================
# OUTPUT CONFIGURATION - 64+ SCIENTIFIC COLUMNS
# =============================================================================
output:
  # Format specifications
  format: "csv"
  encoding: "utf-8"
  separator: ";"                             # Brazilian CSV standard
  include_metadata: true
  preserve_original_text: true               # Research integrity
  academic_headers: true
  target_columns: 64                         # Expand from 34 to 64+

  # Column categories (totaling 64+ columns)
  column_categories:
    # Political Analysis (12 columns)
    political_analysis:
      - "political_spectrum"                 # extrema-direita → esquerda
      - "political_category"                 # Heuristic classification
      - "frame_conflito"                     # Conflict frame score
      - "frame_responsabilizacao"            # Responsibility frame
      - "frame_moralista"                    # Moralist frame
      - "frame_economico"                    # Economic frame
      - "political_entities"                 # Detected political entities
      - "political_entity_count"             # Count of entities
      - "political_entity_density"           # Density ratio
      - "political_term_count"               # Political terms found
      - "political_intensity"                # Political discourse intensity
      - "polarization_score"                 # Polarization measure

    # Linguistic Analysis (15 columns)
    linguistic_analysis:
      - "text_normalized"                    # Cleaned/normalized text
      - "text_length"                        # Character count
      - "word_count"                         # Word count
      - "sentence_count"                     # Sentence count
      - "avg_word_length"                    # Average word length
      - "spacy_tokens_count"                 # spaCy token count
      - "spacy_sentences_count"              # spaCy sentence count
      - "spacy_lemmas"                       # Lemmatized text
      - "tokens_category"                    # short/medium/long
      - "complexity_category"                # simple/medium/complex
      - "lexical_richness"                   # Lexical diversity
      - "spacy_linguistic_complexity"        # Complexity score
      - "spacy_lexical_diversity"            # Diversity score

    # Semantic Analysis (12 columns)
    semantic_analysis:
      - "liwc_affect_positive"               # Positive affect %
      - "liwc_affect_negative"               # Negative affect %
      - "liwc_social"                        # Social processes %
      - "liwc_cognitive"                     # Cognitive processes %
      - "liwc_power"                         # Power language %
      - "semantic_economia"                  # Economic semantics
      - "semantic_politica"                  # Political semantics
      - "semantic_social"                    # Social semantics
      - "political_sentiment"                # positive/negative/neutral
      - "sentiment_intensity"                # Intensity score
      - "positive_score"                     # Positive sentiment count
      - "negative_score"                     # Negative sentiment count

    # Technical Analysis (10 columns)
    technical_analysis:
      - "tfidf_top_terms"                    # Top TF-IDF terms
      - "tfidf_score"                        # TF-IDF relevance score
      - "cluster_id"                         # Cluster assignment
      - "cluster_size"                       # Size of cluster
      - "cluster_confidence"                 # Clustering confidence
      - "topics"                             # Discovered topics
      - "topic_count"                        # Number of topics
      - "bm25_score"                         # BM25 ranking score
      - "hashtag_count"                      # Number of hashtags
      - "mention_count"                      # Number of mentions

    # Temporal & Network Analysis (8 columns)
    temporal_network:
      - "timestamp"                          # Original timestamp
      - "hour"                               # Hour of day
      - "day_of_week"                        # Day of week
      - "month"                              # Month
      - "url_count"                          # Number of URLs
      - "domains_found"                      # Extracted domains
      - "potential_forward"                  # Forward indicator
      - "cascade_participation"              # Information cascade

    # Metadata & Quality (7+ columns)
    metadata_quality:
      - "processing_timestamp"               # When processed
      - "method_citations"                   # Scientific methods used
      - "confidence_score"                   # Overall confidence
      - "quality_score"                      # Data quality score
      - "stage_completion"                   # Completed stages
      - "processing_time_ms"                 # Processing duration
      - "data_source"                        # Source identification

# =============================================================================
# LOGGING CONFIGURATION - UNIFIED ACADEMIC
# =============================================================================
logging:
  # Basic configuration
  level: "INFO"
  format: "academic"                         # Academic-friendly format
  academic_metrics: true                     # Track academic metrics
  cost_tracking: true                        # Budget monitoring
  cache_performance: true                    # Cache hit rates
  method_provenance: true                    # Track scientific methods

  # File handlers (unified from logging.yaml)
  handlers:
    console:
      level: "INFO"
      format: "%(asctime)s [%(levelname)s] %(name)s: %(message)s"

    scientific_file:
      level: "DEBUG"
      filename: "logs/scientific/scientific_analyzer.log"
      max_bytes: 10485760                    # 10MB
      backup_count: 5
      format: "%(asctime)s [%(levelname)s] %(name)s (%(filename)s:%(lineno)d): %(message)s"

    performance_file:
      level: "INFO"
      filename: "logs/performance/performance.log"
      max_bytes: 5242880                     # 5MB
      backup_count: 3

    error_file:
      level: "ERROR"
      filename: "logs/errors/scientific_errors.log"
      max_bytes: 10485760                    # 10MB
      backup_count: 10

  # Rotation configuration
  rotation:
    daily: true
    max_age_days: 30
    cleanup_on_startup: true
  
# =============================================================================
# BUDGET & RESOURCE MANAGEMENT
# =============================================================================
budget_management:
  # Academic budget constraints
  monthly_budget: 50.0                      # Conservative academic budget
  monthly_threshold: 0.8                    # Alert at 80% usage
  weekly_threshold: 0.2                     # Alert at 20% weekly usage
  enable_auto_downgrade: true                # Auto-switch to cheaper models

  # Cost tracking
  track_api_costs: true
  track_processing_time: true
  cost_per_record_target: 0.01              # Target: $0.01 per record

# =============================================================================
# PERFORMANCE & RESOURCE LIMITS
# =============================================================================
performance:
  # Processing constraints
  max_workers: 4                            # Conservative parallelization
  memory_limit_gb: 4.0                      # Academic computing limit
  timeout_minutes: 30                       # Stage timeout
  complete_pipeline_timeout: 45             # Total pipeline timeout

  # Adaptive management
  adaptive_chunking: true
  memory_pressure_monitoring: true
  performance_degradation_detection: true

# =============================================================================
# BRAZILIAN PORTUGUESE OPTIMIZATION
# =============================================================================
portuguese:
  # Language configuration
  language_code: "pt-BR"                    # Brazilian Portuguese
  enabled: true
  spacy_model: "pt_core_news_lg"             # Large Portuguese model
  political_entity_recognition: true
  brazilian_variants: true
  social_media_normalization: true

  # Text preprocessing
  text_preprocessing:
    normalize_accents: false                 # Preserve accents for accuracy
    handle_informal_language: true           # Social media language
    political_term_expansion: true           # Expand abbreviations
    preserve_emojis: true                    # Political emojis

  # Memory optimization for Brazilian datasets
  memory_efficient_processing: true
  chunk_brazilian_text: true                # Portuguese-aware chunking

  # Political analysis optimization
  political_analysis:
    multi_tier_filtering: true              # Graduated filtering
    context_preservation: true              # Preserve context
    confidence_based_retention: true        # Smart retention (25-35%)
    retention_target_min: 0.25              # Minimum 25% retention
    retention_target_max: 0.35              # Maximum 35% retention

# =============================================================================
# RESEARCH QUALITY & REPRODUCIBILITY
# =============================================================================
research_quality:
  # Scientific rigor
  reproducibility: true                     # Ensure reproducible results
  data_integrity_checks: true
  bias_monitoring: true                     # Monitor analytical bias
  validation_sampling: 0.1                  # 10% validation sample
  method_citation_tracking: true

  # Quality thresholds
  minimum_confidence_score: 0.7
  minimum_sample_size: 50
  confidence_level: 0.95
  quality_assurance_checks: true

# =============================================================================
# ERROR HANDLING & FALLBACKS
# =============================================================================
error_handling:
  # Academic fallback strategy
  enable_heuristic_fallbacks: true          # Allow heuristic methods
  strict_quality_mode: true                 # Maintain quality standards
  fail_gracefully: true                     # Continue with partial results
  log_all_errors: true                      # Comprehensive error logging

  # Retry configuration
  max_retries: 3
  retry_delay_seconds: 2
  exponential_backoff: true
  fallback_chain_enabled: true

# =============================================================================
# FINAL CONFIGURATION SUMMARY
# =============================================================================
# Total target: 64+ columns across 5 categories
# - Political Analysis: 12 columns
# - Linguistic Analysis: 15 columns
# - Semantic Analysis: 12 columns
# - Technical Analysis: 10 columns
# - Temporal & Network: 8 columns
# - Metadata & Quality: 7+ columns
# TOTAL: 64+ scientific columns for comprehensive analysis