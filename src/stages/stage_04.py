#!/usr/bin/env python3
"""
digiNEV Pipeline ‚Äî stage_04.py
Auto-extracted from analyzer.py (TAREFA 11 modulariza√ß√£o)
"""

import pandas as pd
import numpy as np
import re
import logging
from typing import Dict, List, Optional, Any


def _stage_04_statistical_analysis(df: pd.DataFrame) -> pd.DataFrame:
    """
    STAGE 04: Statistical Analysis
    
    Comparar in√≠cio do dataset com o dataset reduzido.
    Gerar estat√≠sticas para classifica√ß√£o e gr√°ficos.
    
    Processamentos:
    - Contagem de dados antes e depois
    - Propor√ß√£o de duplicadas
    - Propor√ß√£o de hashtags
    - Detec√ß√£o de repeti√ß√µes excessivas para tabela com 10 principais casos
    """
    try:
        ctx.logger.info("üìä STAGE 04: Statistical Analysis")
        
        text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
        
        # === AN√ÅLISE DE DUPLICA√á√ÉO ===
        total_registros = len(df)
        registros_unicos = len(df[df['dupli_freq'] == 1])
        registros_duplicados = total_registros - registros_unicos
        
        duplicacao_pct = (registros_duplicados / total_registros * 100) if total_registros > 0 else 0
        
        # === AN√ÅLISE DE HASHTAGS ===
        # FIX: usar coluna 'hashtags_extracted' (Stage 01) ou 'body' (# removido de normalized_text)
        has_hashtags = 0
        if 'hashtags_extracted' in df.columns:
            has_hashtags = df['hashtags_extracted'].apply(
                lambda x: len(x) > 0 if isinstance(x, list) else bool(x)
            ).sum()
        elif 'body' in df.columns:
            has_hashtags = df['body'].str.contains('#', na=False).sum()
        elif text_column in df.columns:
            has_hashtags = df[text_column].str.contains('#', na=False).sum()
        
        hashtag_pct = (has_hashtags / total_registros * 100) if total_registros > 0 else 0
        
        # === TOP 10 REPETI√á√ïES EXCESSIVAS ===
        top_duplicates = df[df['dupli_freq'] > 1].nlargest(10, 'dupli_freq')[
            [text_column, 'dupli_freq', 'channels_found', 'date_span_days']
        ].to_dict('records')
        
        # === ESTAT√çSTICAS B√ÅSICAS DE TEXTO ===
        if text_column in df.columns:
            char_counts = df[text_column].str.len().fillna(0)
            word_counts = df[text_column].str.split().str.len().fillna(0)
            
            df['char_count'] = char_counts
            df['word_count'] = word_counts
            
            avg_chars = char_counts.mean()
            avg_words = word_counts.mean()
        else:
            avg_chars = 0
            avg_words = 0
            df['char_count'] = 0
            df['word_count'] = 0
        
        # === PROPOR√á√ïES DE QUALIDADE ===
        # FIX: emoji_ratio e caps_ratio devem usar 'body' (texto cru) ‚Äî normalized_text
        # √© lowercase e sem emojis, o que faz essas m√©tricas retornarem sempre 0.0
        raw_col = 'body' if 'body' in df.columns else text_column
        if raw_col in df.columns:
            df['emoji_ratio'] = df[raw_col].apply(_calculate_emoji_ratio)
            df['caps_ratio'] = df[raw_col].apply(_calculate_caps_ratio)
            df['repetition_ratio'] = df[raw_col].apply(_calculate_repetition_ratio)

            # Detec√ß√£o de idioma b√°sica (pode usar normalized_text ‚Äî lowercase ok)
            df['likely_portuguese'] = df[text_column].apply(_detect_portuguese) if text_column in df.columns else True
        else:
            df['emoji_ratio'] = 0.0
            df['caps_ratio'] = 0.0
            df['repetition_ratio'] = 0.0
            df['likely_portuguese'] = True
        
        # === CONSOLIDA√á√ÉO DE ESTAT√çSTICAS ===
        # Consolidar estat√≠sticas globais em objeto summary
        summary_stats = {
            'total_dataset_size': total_registros,
            'unique_texts_count': registros_unicos,
            'duplication_percentage': round(duplicacao_pct, 2),
            'hashtag_percentage': round(hashtag_pct, 2),
            'avg_chars_per_text': round(avg_chars, 1),
            'avg_words_per_text': round(avg_words, 1)
        }

        # Salvar no contexto para acesso posterior
        ctx.global_stats = summary_stats
        
        # Log das estat√≠sticas
        ctx.logger.info(f"‚úÖ An√°lise estat√≠stica conclu√≠da:")
        ctx.logger.info(f"   üìä Total de registros: {total_registros:,}")
        ctx.logger.info(f"   üîÑ Duplica√ß√£o: {duplicacao_pct:.1f}%")
        ctx.logger.info(f"   # Hashtags: {hashtag_pct:.1f}%")
        ctx.logger.info(f"   üìù M√©dia: {avg_words:.1f} palavras, {avg_chars:.0f} chars")
        
        if top_duplicates:
            ctx.logger.info(f"   üîù Maior repeti√ß√£o: {top_duplicates[0]['dupli_freq']} ocorr√™ncias")
        
        ctx.stats['stages_completed'] += 1
        ctx.stats['features_extracted'] += 11
        
        return df
        
    except Exception as e:
        ctx.logger.error(f"‚ùå Erro Stage 04: {e}")
        ctx.stats['processing_errors'] += 1
        return df

