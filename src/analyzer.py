#!/usr/bin/env python3
"""
digiNEV Analyzer v.final
=======================

Sistema consolidado √∫nico de an√°lise de discurso pol√≠tico brasileiro.
Pipeline com 17 est√°gios interligados gerando 102+ colunas de an√°lise.

ARQUITETURA CONSOLIDADA:
- Sistema √∫nico centralizado (elimina estruturas paralelas)
- 17 est√°gios cient√≠ficos sequenciais
- Dados reais processados (sem m√©tricas inventadas)
- Configura√ß√£o unificada via config/settings.yaml

MODULARIZA√á√ÉO (TAREFA 11):
- Cada stage foi extra√≠do como m√≥dulo independente em src/stages/
- Registry de stages: from stages import STAGE_REGISTRY
- Helpers compartilhados: from stages.helpers import _calculate_emoji_ratio, etc.
- Os m√©todos inline neste arquivo s√£o a vers√£o autoritativa (source of truth)
- Os m√≥dulos em stages/ s√£o a vers√£o modular de refer√™ncia, 1:1 com os inline
- Migra√ß√£o completa: substituir self._stage_XX por import de stages.stage_XX

Author: digiNEV Academic Research Team
Version: v.final (Consolida√ß√£o Final + Modulariza√ß√£o)
"""

import pandas as pd
import numpy as np
import logging
import re
import json
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Machine Learning imports (sempre dispon√≠veis)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics.pairwise import cosine_similarity

# Standard library imports
import unicodedata
import time
from collections import Counter
from urllib.parse import urlparse

# Lexicon loader
from src.lexicon_loader import LexiconLoader

# Memory monitoring (optional)
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# Dependency validation decorator
def validate_stage_dependencies(required_columns=None, required_attrs=None):
    """
    Decorator para validar depend√™ncias entre stages.

    Args:
        required_columns: Lista de colunas obrigat√≥rias no DataFrame
        required_attrs: Lista de atributos obrigat√≥rios na inst√¢ncia
    """
    def decorator(func):
        def wrapper(self, df, *args, **kwargs):
            # Skip validation if processing chunks
            if getattr(self, '_skip_validation', False):
                return func(self, df, *args, **kwargs)

            stage_name = func.__name__.replace('_stage_', 'Stage ').replace('_', ' ').title()

            # Validar colunas obrigat√≥rias
            if required_columns:
                missing_cols = [col for col in required_columns if col not in df.columns]
                if missing_cols:
                    self.logger.error(f"‚ùå {stage_name}: Colunas obrigat√≥rias ausentes: {missing_cols}")
                    raise ValueError(f"Colunas obrigat√≥rias ausentes para {stage_name}: {missing_cols}")

            # Validar atributos obrigat√≥rios na inst√¢ncia
            if required_attrs:
                missing_attrs = [attr for attr in required_attrs if not hasattr(self, attr)]
                if missing_attrs:
                    self.logger.error(f"‚ùå {stage_name}: Atributos obrigat√≥rios ausentes: {missing_attrs}")
                    raise ValueError(f"Atributos obrigat√≥rios ausentes para {stage_name}: {missing_attrs}")

            # Executar fun√ß√£o se valida√ß√µes passaram
            return func(self, df, *args, **kwargs)
        return wrapper
    return decorator

# spaCy para processamento lingu√≠stico em portugu√™s
try:
    import spacy
    # Tentar carregar modelo portugu√™s
    try:
        nlp = spacy.load("pt_core_news_lg")
        SPACY_AVAILABLE = True
    except OSError:
        try:
            nlp = spacy.load("pt_core_news_sm")
            SPACY_AVAILABLE = True
        except OSError:
            nlp = None
            SPACY_AVAILABLE = False
except ImportError:
    nlp = None
    SPACY_AVAILABLE = False


class Analyzer:
    """
    Analisador consolidado com 14 stages sequenciais interligados.

    STAGES IMPLEMENTADOS (sem fallbacks confusos):
    01. feature_extraction (Python puro) - Detec√ß√£o autom√°tica de colunas e features
    02. text_preprocessing (Python puro) - Limpeza b√°sica de texto em portugu√™s
    03. linguistic_processing (spaCy) - Processamento lingu√≠stico avan√ßado LOGO AP√ìS limpeza
    04. statistical_analysis (Python puro) - An√°lise estat√≠stica com dados spaCy
    05. political_classification (Lexicon real) - Classifica√ß√£o pol√≠tica brasileira
    06. tfidf_vectorization (scikit-learn) - TF-IDF com tokens spaCy
    07. clustering_analysis (scikit-learn) - Clustering baseado em features lingu√≠sticas
    08. topic_modeling (scikit-learn) - Topic modeling com embeddings
    09. temporal_analysis (Python puro) - An√°lise temporal
    10. network_analysis (Python puro) - Coordena√ß√£o e padr√µes de rede
    """

    def __init__(self, chunk_size: int = 2000, memory_limit_gb: float = 2.0, auto_chunk: bool = True,
                 political_relevance_threshold: float = 0.02):
        """
        Inicializar analyzer com capacidades de auto-chunking.

        Args:
            chunk_size: Tamanho do chunk quando auto-chunking √© necess√°rio
            memory_limit_gb: Limite de mem√≥ria para trigger de chunking
            auto_chunk: Se True, detecta automaticamente quando usar chunks
            political_relevance_threshold: Threshold m√≠nimo para relev√¢ncia pol√≠tica (padr√£o: 0.02)
        """
        self.logger = logging.getLogger(self.__class__.__name__)

        # Configura√ß√µes de chunking autom√°tico
        self.chunk_size = chunk_size
        self.memory_limit_gb = memory_limit_gb
        self.auto_chunk = auto_chunk

        # Configura√ß√µes de filtros
        self.political_relevance_threshold = political_relevance_threshold

        # Load political lexicon via LexiconLoader (unified system: 956 termos, 9 macrotemas)
        self.lexicon_loader = LexiconLoader()
        self.political_lexicon = self._load_political_lexicon()
        self._political_terms_map = self.lexicon_loader.get_terms_by_category_map()

        # Initialize ML components
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.kmeans_model = None
        self.lda_model = None

        # Stats tracking
        self.stats = {
            'stages_completed': 0,
            'features_extracted': 0,
            'processing_errors': 0,
            'chunked_processing': False,
            'total_chunks': 0
        }

        # Global statistics container
        self.global_stats = {}

        self.logger.info("‚úÖ Analyzer v.final inicializado (auto-chunking habilitado)")

    def _load_political_lexicon(self) -> Dict:
        """Carregar lexicon pol√≠tico via LexiconLoader (lexico_unified_system.json)."""
        if self.lexicon_loader.lexicon:
            terms_map = self.lexicon_loader.get_terms_by_category_map()
            total_terms = sum(len(v) for v in terms_map.values())
            self.logger.info(f"‚úÖ Lexicon unificado carregado: {len(terms_map)} categorias, {total_terms} termos")
            return self.lexicon_loader.lexicon

        self.logger.warning("‚ö†Ô∏è Lexicon unificado n√£o encontrado, usando fallback m√≠nimo")
        return {
            "lexico": {
                "identidade_patriotica": {"subtemas": {"fallback": {"palavras": ["bolsonaro", "mito", "patriota"]}}},
                "inimigos_ideologicos": {"subtemas": {"fallback": {"palavras": ["lula", "pt", "petista", "comunista"]}}}
            }
        }

    def _should_use_chunking(self, data_input):
        """
        Determinar automaticamente se deve usar processamento em chunks.
        
        Args:
            data_input: DataFrame ou caminho do arquivo
            
        Returns:
            Tuple[usar_chunks, estimativa_registros, motivo]
        """
        try:
            # Se for DataFrame, verificar tamanho direto
            if isinstance(data_input, pd.DataFrame):
                n_records = len(data_input)
                memory_usage_mb = data_input.memory_usage(deep=True).sum() / (1024**2)
                
                if n_records > 10000:
                    return True, n_records, f"Dataset grande: {n_records:,} registros"
                elif memory_usage_mb > self.memory_limit_gb * 1024:
                    return True, n_records, f"Uso de mem√≥ria alto: {memory_usage_mb:.1f}MB"
                else:
                    return False, n_records, f"Dataset pequeno: {n_records:,} registros"
            
            # Se for caminho de arquivo, estimar tamanho
            elif isinstance(data_input, (str, Path)):
                file_path = Path(data_input)
                if not file_path.exists():
                    return False, 0, "Arquivo n√£o encontrado"
                
                # Estimar n√∫mero de registros pelo tamanho do arquivo
                file_size_mb = file_path.stat().st_size / (1024**2)
                estimated_records = int(file_size_mb * 100)  # Estimativa: ~100 registros por MB
                
                if file_size_mb > 50:  # Arquivos > 50MB
                    return True, estimated_records, f"Arquivo grande: {file_size_mb:.1f}MB"
                elif estimated_records > 10000:
                    return True, estimated_records, f"Muitos registros estimados: {estimated_records:,}"
                else:
                    return False, estimated_records, f"Arquivo pequeno: {file_size_mb:.1f}MB"
            
            return False, 0, "Tipo de entrada n√£o reconhecido"
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao determinar chunking: {e}")
            return False, 0, "Erro na detec√ß√£o"

    def _check_memory_usage(self) -> bool:
        """Verificar se mem√≥ria est√° pr√≥xima do limite."""
        if not PSUTIL_AVAILABLE:
            self.logger.warning("‚ö†Ô∏è psutil n√£o dispon√≠vel para monitoramento de mem√≥ria")
            return False

        try:
            process = psutil.Process()
            memory_info = process.memory_info()
            memory_gb = memory_info.rss / (1024**3)
            memory_percent = process.memory_percent()

            # Log detalhado de mem√≥ria a cada verifica√ß√£o
            self.logger.debug(f"üíæ Mem√≥ria atual: {memory_gb:.2f}GB ({memory_percent:.1f}% do sistema)")

            if memory_gb > self.memory_limit_gb:
                self.logger.warning(f"üö® Mem√≥ria alta: {memory_gb:.1f}GB > {self.memory_limit_gb}GB ({memory_percent:.1f}%)")
                return True
            elif memory_gb > self.memory_limit_gb * 0.8:
                self.logger.info(f"‚ö†Ô∏è Mem√≥ria crescendo: {memory_gb:.1f}GB (80% do limite)")

            return False
        except Exception as e:
            self.logger.error(f"‚ùå Erro no monitoramento de mem√≥ria: {e}")
            return False

    def _clean_memory(self):
        """Limpar mem√≥ria for√ßadamente."""
        import gc
        gc.collect()
        self.logger.info("üßπ Mem√≥ria limpa")

    def analyze(self, data_input, **kwargs) -> Dict[str, Any]:
        """
        M√©todo principal unificado que detecta automaticamente o modo de processamento.
        
        Args:
            data_input: DataFrame ou caminho para arquivo CSV
            **kwargs: Argumentos adicionais (max_records, output_file, etc.)
            
        Returns:
            Resultado da an√°lise (formato unificado)
        """
        # Auto-detec√ß√£o do modo de processamento
        should_chunk, estimated_records, reason = self._should_use_chunking(data_input)
        
        self.logger.info(f"ü§ñ Auto-detec√ß√£o: {reason}")
        
        if self.auto_chunk and should_chunk:
            self.logger.info(f"‚ö° Modo CHUNKED ativado automaticamente")
            self.stats['chunked_processing'] = True
            return self._analyze_chunked(data_input, **kwargs)
        else:
            self.logger.info(f"üî¨ Modo NORMAL (in-memory)")
            self.stats['chunked_processing'] = False
            
            # Se for arquivo, carregar como DataFrame
            if isinstance(data_input, (str, Path)):
                df = self._load_dataframe(data_input, kwargs.get('max_records'))
            else:
                df = data_input.copy()
                
            return self.analyze_dataset(df)

    def _load_dataframe(self, file_path: str, max_records: Optional[int] = None) -> pd.DataFrame:
        """Carregar DataFrame com detec√ß√£o autom√°tica de separador e tratamento robusto."""
        file_path = Path(file_path)
        
        # Detectar separador lendo primeira linha
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                first_line = f.readline().strip()
                
            # Contar separadores na primeira linha
            comma_count = first_line.count(',')
            semicolon_count = first_line.count(';')
            
            # Escolher separador baseado em contagem
            if semicolon_count > comma_count:
                separator = ';'
            elif comma_count > 0:
                separator = ','
            else:
                # Fallback: tentar ambos e ver qual funciona melhor
                try:
                    test_df = pd.read_csv(file_path, sep=';', nrows=1)
                    if len(test_df.columns) > 1:
                        separator = ';'
                    else:
                        separator = ','
                except:
                    separator = ','
            
            self.logger.info(f"üîç Separador detectado: '{separator}' (v√≠rgulas: {comma_count}, ponto-v√≠rgulas: {semicolon_count})")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro na detec√ß√£o de separador: {e}, usando v√≠rgula por padr√£o")
            separator = ','
        
        # Carregar com limite de registros se especificado
        try:
            # Usar configura√ß√µes mais robustas para CSV com conte√∫do complexo
            read_kwargs = {
                'sep': separator,
                'encoding': 'utf-8',
                'quotechar': '"',
                'quoting': 1,  # QUOTE_ALL
                'skipinitialspace': True,
                'on_bad_lines': 'skip'  # Pular linhas problem√°ticas
            }
            
            if max_records:
                read_kwargs['nrows'] = max_records
                
            df = pd.read_csv(file_path, **read_kwargs)
                
            # Verificar se carregamento foi bem-sucedido
            if len(df.columns) == 1 and separator == ';':
                # Tentar com v√≠rgula se ponto-v√≠rgula resultou em uma coluna s√≥
                self.logger.warning("‚ö†Ô∏è Tentando v√≠rgula ap√≥s falha com ponto-v√≠rgula")
                read_kwargs['sep'] = ','
                df = pd.read_csv(file_path, **read_kwargs)
                separator = ','
                
            self.logger.info(f"üìÇ Dataset carregado: {len(df):,} registros, {len(df.columns)} colunas (sep='{separator}')")
            return df
            
        except Exception as e:
            # Fallback: tentar com configura√ß√µes mais permissivas
            self.logger.warning(f"‚ö†Ô∏è Erro com configura√ß√µes padr√£o: {e}, tentando modo permissivo")
            try:
                fallback_kwargs = {
                    'sep': separator,
                    'encoding': 'utf-8',
                    'quotechar': '"',
                    'quoting': 0,  # QUOTE_MINIMAL
                    'skipinitialspace': True,
                    'on_bad_lines': 'skip',
                    'error_bad_lines': False,
                    'warn_bad_lines': True
                }
                
                if max_records:
                    fallback_kwargs['nrows'] = max_records
                    
                df = pd.read_csv(file_path, **fallback_kwargs)
                self.logger.info(f"üìÇ Dataset carregado (modo permissivo): {len(df):,} registros, {len(df.columns)} colunas")
                return df
                
            except Exception as e2:
                self.logger.error(f"‚ùå Erro ao carregar arquivo mesmo com fallback: {e2}")
                raise

    def _analyze_chunked(self, data_input, **kwargs) -> Dict[str, Any]:
        """
        Processamento em chunks integrado ao Analyzer principal.
        
        Args:
            data_input: Caminho do arquivo ou DataFrame  
            **kwargs: max_records, output_file, etc.
            
        Returns:
            Estat√≠sticas consolidadas no formato padr√£o
        """
        self.logger.info("‚ö° Iniciando an√°lise chunked integrada")
        
        # Se for DataFrame, converter para modo chunked simulado
        if isinstance(data_input, pd.DataFrame):
            return self._chunked_from_dataframe(data_input, **kwargs)
        
        # Processar arquivo em chunks
        file_path = Path(data_input)
        max_records = kwargs.get('max_records')
        output_file = kwargs.get('output_file')
        
        consolidated_results = []
        total_records = 0
        total_chunks = 0
        
        # Estat√≠sticas consolidadas
        consolidated_stats = {
            'political_distribution': {},
            'temporal_stats': {'valid_timestamps': 0, 'total_records': 0},
            'coordination_stats': {'coordinated': 0, 'total_records': 0},
            'domain_stats': {'with_links': 0, 'total_records': 0},
            'stages_completed': 0,
            'features_extracted': 0,
            'processing_errors': 0
        }
        
        try:
            # Processar cada chunk
            for chunk in self._load_dataset_chunks(file_path, max_records):
                chunk_start = time.time()
                
                # Analisar chunk usando m√©todo espec√≠fico para chunks
                chunk_data = self._analyze_single_chunk(chunk.copy())
                
                # Consolidar estat√≠sticas
                total_records += len(chunk_data)
                total_chunks += 1
                
                # Consolidar distribui√ß√£o pol√≠tica
                if 'political_spectrum' in chunk_data.columns:
                    political_dist = chunk_data['political_spectrum'].value_counts()
                    for category, count in political_dist.items():
                        consolidated_stats['political_distribution'][category] = \
                            consolidated_stats['political_distribution'].get(category, 0) + count
                
                # Consolidar outras estat√≠sticas
                if 'has_temporal_data' in chunk_data.columns:
                    valid_temporal = chunk_data['has_temporal_data'].sum()
                    consolidated_stats['temporal_stats']['valid_timestamps'] += valid_temporal
                    consolidated_stats['temporal_stats']['total_records'] += len(chunk_data)
                
                if 'potential_coordination' in chunk_data.columns:
                    coordinated = chunk_data['potential_coordination'].sum()
                    consolidated_stats['coordination_stats']['coordinated'] += coordinated
                    consolidated_stats['coordination_stats']['total_records'] += len(chunk_data)
                
                if 'has_external_links' in chunk_data.columns:
                    with_links = chunk_data['has_external_links'].sum()
                    consolidated_stats['domain_stats']['with_links'] += with_links
                    consolidated_stats['domain_stats']['total_records'] += len(chunk_data)
                
                # Atualizar stats consolidadas
                consolidated_stats['stages_completed'] = max(consolidated_stats['stages_completed'], self.stats.get('stages_completed', 0))
                consolidated_stats['features_extracted'] = max(consolidated_stats['features_extracted'], self.stats.get('features_extracted', 0))
                
                # Salvar chunk se solicitado
                if output_file:
                    chunk_output = f"{output_file.replace('.csv', '')}_chunk_{total_chunks}.csv"
                    chunk_data.to_csv(chunk_output, index=False, sep=';')
                    self.logger.info(f"üíæ Chunk salvo: {chunk_output}")
                
                chunk_time = time.time() - chunk_start
                chunk_performance = len(chunk_data) / chunk_time if chunk_time > 0 else 0
                
                self.logger.info(f"‚úÖ Chunk {total_chunks} processado: {len(chunk_data):,} registros em {chunk_time:.1f}s ({chunk_performance:.1f} reg/s)")
                
                # Limpar mem√≥ria entre chunks
                del chunk_data, chunk
                if self._check_memory_usage():
                    self._clean_memory()
                
        except Exception as e:
            self.logger.error(f"‚ùå Erro na an√°lise chunked: {e}")
            consolidated_stats['processing_errors'] += 1
            raise
        
        # Atualizar stats principais
        self.stats.update({
            'total_records_processed': total_records,
            'chunks_processed': total_chunks,
            'chunked_processing': True,
            'stages_completed': consolidated_stats['stages_completed'],
            'features_extracted': consolidated_stats['features_extracted']
        })
        
        self.logger.info(f"üéâ An√°lise chunked conclu√≠da: {total_records:,} registros em {total_chunks} chunks")
        
        # Retornar no formato padr√£o do Analyzer
        return {
            'data': pd.DataFrame({'processed_records': [total_records]}),  # DataFrame m√≠nimo para compatibilidade
            'stats': self.stats.copy(),
            'consolidated_stats': consolidated_stats,
            'columns_generated': 75,  # Estimativa baseada no pipeline
            'total_records': total_records,
            'stages_completed': consolidated_stats['stages_completed']
        }

    def _analyze_single_chunk(self, chunk_df: pd.DataFrame) -> pd.DataFrame:
        """
        Analisa um chunk espec√≠fico sem recurs√£o.
        Ajusta thresholds para chunks pequenos.
        """
        try:
            # Ajustar par√¢metros para chunks pequenos
            original_threshold = getattr(self, 'political_relevance_threshold', 0.02)
            chunk_size = len(chunk_df)
            
            # Threshold mais permissivo para chunks pequenos
            if chunk_size < 1000:
                self.political_relevance_threshold = 0.01  # Mais permissivo
                self.logger.info(f"üìä Chunk pequeno ({chunk_size}): threshold ajustado para {self.political_relevance_threshold}")
            elif chunk_size < 5000:
                self.political_relevance_threshold = 0.015  # Moderadamente permissivo
            
            # Ativar flag para pular valida√ß√µes durante processamento de chunk
            self._skip_validation = True
            
            self.logger.info(f"üîÑ Processando chunk: {len(chunk_df)} registros")
            
            # Processar atrav√©s do pipeline principal
            result_chunk = chunk_df.copy()
            
            # Stages 01-02: Preprocessing
            result_chunk = self._stage_01_feature_extraction(result_chunk)
            result_chunk = self._stage_02_text_preprocessing(result_chunk)
            
            # Stages 03-06: Volume reduction (cr√≠ticos para chunks)
            result_chunk = self._stage_03_cross_dataset_deduplication(result_chunk)
            result_chunk = self._stage_04_statistical_analysis(result_chunk)
            result_chunk = self._stage_05_content_quality_filter(result_chunk)
            result_chunk = self._stage_06_affordances_classification(result_chunk)
            
            # Verificar se ainda h√° dados suficientes ap√≥s filtros
            if len(result_chunk) == 0:
                self.logger.warning("‚ö†Ô∏è Chunk completamente filtrado, retornando dados parciais")
                # Restaurar threshold original
                self.political_relevance_threshold = original_threshold
                self._skip_validation = False
                return chunk_df.copy()  # Retornar dados originais com minimal processing
            
            # Stages 07-09: Linguistic processing (ajustados para chunks pequenos)
            try:
                result_chunk = self._stage_07_linguistic_processing(result_chunk)
                result_chunk = self._stage_08_political_classification(result_chunk)
                
                # Stage 09: TF-IDF com tratamento especial para chunks pequenos
                if len(result_chunk) >= 5:  # M√≠nimo vi√°vel para TF-IDF
                    result_chunk = self._stage_09_tfidf_vectorization(result_chunk)
                else:
                    self.logger.warning(f"üîç Chunk muito pequeno ({len(result_chunk)}) para TF-IDF, preenchendo com valores padr√£o")
                    result_chunk['tfidf_score_mean'] = 0.0
                    result_chunk['tfidf_score_max'] = 0.0 
                    result_chunk['tfidf_top_terms'] = [[] for _ in range(len(result_chunk))]
                    self.stats['features_extracted'] += 3
                
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro em processamento lingu√≠stico do chunk: {e}")
                # Continuar com dados parciais
            
            # Stages 10-17: Advanced analysis (opcionais para chunks pequenos)
            try:
                if len(result_chunk) >= 10:  # S√≥ executar se chunk tem tamanho vi√°vel
                    result_chunk = self._stage_10_clustering_analysis(result_chunk)
                    result_chunk = self._stage_11_topic_modeling(result_chunk)
                    result_chunk = self._stage_12_semantic_analysis(result_chunk)
                    result_chunk = self._stage_13_temporal_analysis(result_chunk)
                    result_chunk = self._stage_14_network_analysis(result_chunk)  # CORRIGIDO
                    result_chunk = self._stage_15_domain_analysis(result_chunk)   # CORRIGIDO
                    result_chunk = self._stage_16_event_context(result_chunk)     # CORRIGIDO
                    result_chunk = self._stage_17_channel_analysis(result_chunk) # CORRIGIDO
                else:
                    self.logger.info(f"üìä Chunk pequeno ({len(result_chunk)}): pulando an√°lises avan√ßadas")
                    # Adicionar colunas padr√£o para manter consist√™ncia
                    self._add_default_columns_for_small_chunks(result_chunk)
                    
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro em an√°lises avan√ßadas do chunk: {e}")
                # Continuar com dados dispon√≠veis
            
            # Restaurar configura√ß√µes originais
            self.political_relevance_threshold = original_threshold
            self._skip_validation = False
            
            self.logger.info(f"‚úÖ Chunk processado: {len(result_chunk)} registros finais")
            return result_chunk
            
        except Exception as e:
            # Restaurar configura√ß√µes em caso de erro
            if hasattr(self, 'political_relevance_threshold'):
                self.political_relevance_threshold = getattr(self, '_original_threshold', 0.02)
            self._skip_validation = False
            
            self.logger.error(f"‚ùå Erro ao processar chunk: {e}")
            # Retornar pelo menos os dados originais com colunas b√°sicas
            return self._add_minimal_columns(chunk_df.copy())

    def _add_default_columns_for_small_chunks(self, df: pd.DataFrame) -> pd.DataFrame:
        """Adiciona colunas padr√£o para chunks pequenos que pulam an√°lises avan√ßadas."""
        try:
            # Clustering columns
            if 'cluster_id' not in df.columns:
                df['cluster_id'] = 0
                df['cluster_size'] = len(df)
                df['cluster_confidence'] = 0.5
            
            # Topic modeling columns  
            if 'topic_distribution' not in df.columns:
                df['topic_distribution'] = [[] for _ in range(len(df))]
                df['dominant_topic'] = 0
                df['topic_confidence'] = 0.5
            
            # Semantic analysis columns
            if 'semantic_density' not in df.columns:
                df['semantic_density'] = 0.5
                df['semantic_complexity'] = 0.5
                df['semantic_coherence'] = 0.5
            
            # Temporal analysis columns
            if 'hour' not in df.columns and 'datetime' in df.columns:
                try:
                    df['hour'] = pd.to_datetime(df['datetime']).dt.hour
                    df['day_of_week'] = pd.to_datetime(df['datetime']).dt.dayofweek  
                    df['month'] = pd.to_datetime(df['datetime']).dt.month
                except:
                    df['hour'] = 12  # Default noon
                    df['day_of_week'] = 1  # Default Monday
                    df['month'] = 6  # Default June
            
            # Network coordination columns
            if 'coordination_score' not in df.columns:
                df['coordination_score'] = 0.0
                df['cascade_participation'] = False
                df['influence_score'] = 0.0
            
            # Domain analysis columns
            if 'domains_found' not in df.columns:
                df['domains_found'] = [[] for _ in range(len(df))]
                df['domain_authority_score'] = 0.0
                df['url_diversity'] = 0.0
            
            # Event context columns
            if 'political_events_detected' not in df.columns:
                df['political_events_detected'] = [[] for _ in range(len(df))]
                df['event_context_score'] = 0.0
                df['temporal_relevance'] = 0.5
            
            # Channel analysis columns
            if 'channel_classification' not in df.columns:
                df['channel_classification'] = 'unknown'
                df['channel_authority'] = 0.5
                df['source_type'] = 'social_media'
            
            self.logger.info(f"üìã Colunas padr√£o adicionadas para chunk pequeno")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao adicionar colunas padr√£o: {e}")
            return df
    
    def _add_minimal_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Adiciona colunas m√≠nimas em caso de erro no processamento."""
        try:
            # Colunas essenciais
            if 'political_relevance_score' not in df.columns:
                df['political_relevance_score'] = 0.5
            if 'content_quality_score' not in df.columns:
                df['content_quality_score'] = 50.0
            if 'normalized_text' not in df.columns:
                text_col = 'body' if 'body' in df.columns else df.columns[0]
                df['normalized_text'] = df[text_col].fillna('')
            
            self.logger.info(f"üìã Colunas m√≠nimas adicionadas para recupera√ß√£o de erro")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao adicionar colunas m√≠nimas: {e}")
            return df

    def _load_dataset_chunks(self, file_path: Path, max_records: Optional[int] = None):
        """Generator para carregar dataset em chunks."""
        if not file_path.exists():
            raise FileNotFoundError(f"Dataset n√£o encontrado: {file_path}")
        
        self.logger.info(f"üìÇ Carregando dataset em chunks: {file_path}")
        
        # Detectar separador e configura√ß√£o necess√°ria
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                first_line = f.readline().strip()
                
            # Determinar separador
            comma_count = first_line.count(',')
            semicolon_count = first_line.count(';')
            separator = ';' if semicolon_count > comma_count else ','
            
            # Configura√ß√µes robustas para CSV complexo
            read_kwargs = {
                'sep': separator,
                'encoding': 'utf-8',
                'engine': 'python',  # Usar engine Python para maior flexibilidade
                'quotechar': '"',
                'quoting': 1,  # QUOTE_ALL
                'skipinitialspace': True,
                'on_bad_lines': 'skip',
                'chunksize': self.chunk_size
            }
            
            if max_records:
                read_kwargs['nrows'] = max_records
                
            self.logger.info(f"üîç Configura√ß√£o: sep='{separator}', engine=python")
            
            chunk_number = 0
            total_processed = 0
            
            try:
                # Usar pandas com engine Python para maior robustez
                for chunk in pd.read_csv(file_path, **read_kwargs):
                    chunk_number += 1
                    total_processed += len(chunk)
                    
                    self.logger.info(f"üì¶ Chunk {chunk_number}: {len(chunk)} registros (total: {total_processed})")
                    
                    # Verificar se chunk tem dados v√°lidos
                    if len(chunk) > 0 and len(chunk.columns) > 1:
                        yield chunk
                    
                    if max_records and total_processed >= max_records:
                        break
                        
            except Exception as e:
                self.logger.error(f"‚ùå Erro ao carregar dataset: {e}")
                
                # Fallback: carregar linha por linha se necess√°rio
                self.logger.warning("‚ö†Ô∏è Tentando carregamento linha por linha...")
                try:
                    import csv
                    
                    with open(file_path, 'r', encoding='utf-8') as csvfile:
                        # Detectar dialect automaticamente
                        sample = csvfile.read(1024)
                        csvfile.seek(0)
                        sniffer = csv.Sniffer()
                        dialect = sniffer.sniff(sample)
                        
                        reader = csv.DictReader(csvfile, dialect=dialect)
                        
                        chunk_data = []
                        for i, row in enumerate(reader):
                            chunk_data.append(row)
                            
                            if len(chunk_data) >= self.chunk_size:
                                df_chunk = pd.DataFrame(chunk_data)
                                chunk_number += 1
                                self.logger.info(f"üì¶ Chunk CSV {chunk_number}: {len(df_chunk)} registros")
                                yield df_chunk
                                chunk_data = []
                            
                            if max_records and i >= max_records:
                                break
                        
                        # √öltimo chunk se houver dados
                        if chunk_data:
                            df_chunk = pd.DataFrame(chunk_data)
                            chunk_number += 1
                            self.logger.info(f"üì¶ Chunk CSV final {chunk_number}: {len(df_chunk)} registros")
                            yield df_chunk
                            
                except Exception as e2:
                    self.logger.error(f"‚ùå Fallback tamb√©m falhou: {e2}")
                    raise
                
        except Exception as e:
            self.logger.error(f"‚ùå Erro cr√≠tico no carregamento: {e}")
            raise

    def _chunked_from_dataframe(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """Simular processamento chunked para DataFrame que j√° est√° em mem√≥ria."""
        self.logger.info("üîÑ Simulando chunked para DataFrame em mem√≥ria")
        
        # Se DataFrame √© pequeno, processar normalmente
        if len(df) <= self.chunk_size:
            return self._analyze_single_chunk(df)

        # Dividir DataFrame em chunks
        chunks = [df[i:i+self.chunk_size] for i in range(0, len(df), self.chunk_size)]

        consolidated_results = []
        for i, chunk in enumerate(chunks, 1):
            self.logger.info(f"üì¶ Chunk {i}: {len(chunk)} registros, Total: {len(chunks):,}")

            # Log de progresso a cada 5 chunks
            if i % 5 == 0:
                progress_pct = (i / len(chunks)) * 100
                self.logger.info(f"üîÑ Progresso: {progress_pct:.1f}% ({i}/{len(chunks)} chunks)")

            result = self._analyze_single_chunk(chunk.copy())
            consolidated_results.append(result)

            # Limpeza for√ßada de mem√≥ria ap√≥s cada chunk
            import gc
            del chunk  # Liberar chunk explicitamente
            gc.collect()

            # Verificar mem√≥ria e limpar se necess√°rio
            if self._check_memory_usage():
                self.logger.warning(f"üö® Limpeza de mem√≥ria no chunk {i}")
                self._clean_memory()

            self.logger.info(f"‚úÖ Chunk {i} processado: {len(result['data']) if 'data' in result else 0} registros finais")
        
        # Para simplificar, retornar resultado do √∫ltimo chunk com stats consolidados
        final_result = consolidated_results[-1]
        final_result['stats']['total_chunks'] = len(chunks)
        final_result['stats']['chunked_processing'] = True
        
        return final_result

    def analyze_dataset(self, data_input, max_records=None) -> Dict[str, Any]:
        """
        Analisar dataset com pipeline sequencial otimizado de 17 stages.

        NOVA SEQU√äNCIA OTIMIZADA (conforme PIPELINE_STAGES_ANALYSIS.md):
        Fase 1: Prepara√ß√£o (01-02) - estrutura b√°sica
        Fase 2: Redu√ß√£o de volume (03-06) - CR√çTICO para performance
        Fase 3: An√°lise lingu√≠stica (07-09) - volume reduzido
        Fase 4: An√°lises avan√ßadas (10-17) - dados otimizados

        Args:
            data_input: DataFrame ou caminho do arquivo para an√°lise
            max_records: Limite m√°ximo de registros para processar (opcional)

        Returns:
            Dict com resultado da an√°lise
        """
        # Verificar se deve usar chunking
        should_chunk, estimated_records, reason = self._should_use_chunking(data_input)

        # Se for caminho de arquivo e deve usar chunking, usar processamento chunked
        if self.auto_chunk and should_chunk:
            self.logger.info(f"‚ö° Chunking ativado: {reason}")
            self.stats['chunked_processing'] = True
            return self._analyze_chunked(data_input, max_records=max_records)

        # Sen√£o, carregar dados e processar normalmente
        self.stats['chunked_processing'] = False

        # Se for caminho de arquivo, carregar
        if isinstance(data_input, (str, Path)):
            df = self._load_dataframe(data_input, max_records)
        else:
            df = data_input
            if max_records and len(df) > max_records:
                df = df.head(max_records)
                self.logger.info(f"üìä Limitado a {max_records:,} registros")

        try:
            self.logger.info(f"üî¨ Iniciando an√°lise OTIMIZADA: {len(df)} registros")

            # Reset stats
            self.stats = {'stages_completed': 0, 'features_extracted': 0, 'processing_errors': 0}

            # ===========================================
            # FASE 1: PREPARA√á√ÉO E ESTRUTURA (01-02)
            # ===========================================
            
            # STAGE 01: Feature Extraction (estrutura b√°sica)
            df = self._stage_01_feature_extraction(df)

            # STAGE 02: Text Preprocessing (limpeza b√°sica)
            df = self._stage_02_text_preprocessing(df)

            # ===========================================
            # FASE 2: REDU√á√ÉO DE VOLUME (03-06) - CR√çTICO!
            # ===========================================
            
            # STAGE 03: Cross-Dataset Deduplication (40-50% redu√ß√£o)
            df = self._stage_03_cross_dataset_deduplication(df)

            # STAGE 04: Statistical Analysis (compara√ß√£o antes/depois)
            df = self._stage_04_statistical_analysis(df)

            # STAGE 05: Content Quality Filter (15-25% redu√ß√£o adicional)
            df = self._stage_05_content_quality_filter(df)

            # STAGE 06: Affordances Classification (AI-powered analysis)
            df = self._stage_06_affordances_classification(df)

            self.logger.info(f"üìä FASE 2 CONCLU√çDA: Volume reduzido para {len(df):,} registros")

            # ===========================================
            # FASE 3: AN√ÅLISE LINGU√çSTICA (07-09) - VOLUME REDUZIDO
            # ===========================================
            
            # STAGE 07: Linguistic Processing (spaCy - AGORA com volume otimizado)
            df = self._stage_07_linguistic_processing(df)  # Usar m√©todo existente

            # STAGE 08: Political Classification (usando tokens spaCy)
            df = self._stage_08_political_classification(df)  # Usar m√©todo existente

            # STAGE 09: TF-IDF Vectorization (usando lemmas spaCy)
            df = self._stage_09_tfidf_vectorization(df)  # Usar m√©todo existente

            # ===========================================
            # FASE 4: AN√ÅLISES AVAN√áADAS (10-17)
            # ===========================================
            
            # STAGE 10: Clustering Analysis
            df = self._stage_10_clustering_analysis(df)  # Usar m√©todo existente

            # STAGE 11: Topic Modeling
            df = self._stage_11_topic_modeling(df)  # Usar m√©todo existente

            # STAGE 12: Semantic Analysis
            df = self._stage_12_semantic_analysis(df)  # Usar m√©todo existente

            # STAGE 13: Temporal Analysis
            df = self._stage_13_temporal_analysis(df)  # Usar m√©todo existente

            # STAGE 14: Network Analysis
            df = self._stage_14_network_analysis(df)  # Usar m√©todo existente

            # STAGE 15: Domain Analysis
            df = self._stage_15_domain_analysis(df)  # Usar m√©todo existente

            # STAGE 16: Event Context Analysis
            df = self._stage_16_event_context(df)  # Usar m√©todo existente

            # STAGE 17: Channel Analysis
            df = self._stage_17_channel_analysis(df)  # Usar m√©todo existente

            # Final metadata
            df['processing_timestamp'] = datetime.now().isoformat()
            df['stages_completed'] = self.stats['stages_completed']
            df['features_extracted'] = self.stats['features_extracted']

            self.logger.info(f"‚úÖ An√°lise OTIMIZADA conclu√≠da: {len(df.columns)} colunas, {self.stats['stages_completed']} stages")
            self.logger.info(f"üéØ Performance: Processados {len(df):,} registros finais")

            return {
                'success': True,
                'data': df,
                'stats': self.stats.copy(),
                'columns_generated': len(df.columns),
                'total_records': len(df),
                'stages_completed': self.stats['stages_completed']
            }

        except Exception as e:
            self.logger.error(f"‚ùå Erro na an√°lise: {e}")
            raise

    def _stage_01_feature_extraction(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        STAGE 01: Extra√ß√£o e identifica√ß√£o autom√°tica de features (Python puro).

        SEMPRE O PRIMEIRO STAGE - identifica colunas dispon√≠veis e extrai features se necess√°rio.
        """
        self.logger.info("üîç STAGE 01: Feature Extraction")

        # Identificar coluna de texto principal
        text_columns = []
        for col in df.columns:
            if df[col].dtype == 'object':
                # Verificar se cont√©m texto substancial
                sample = df[col].dropna().head(10)
                if len(sample) > 0:
                    avg_length = sample.astype(str).str.len().mean()
                    if avg_length > 20:  # Textos com mais de 20 caracteres em m√©dia
                        text_columns.append(col)

        # Selecionar melhor coluna de texto
        if not text_columns:
            raise ValueError("‚ùå Nenhuma coluna de texto encontrada")

        # Priorizar colunas comuns
        priority_columns = ['text', 'body', 'message', 'content', 'texto', 'mensagem']
        main_text_column = None

        for priority in priority_columns:
            if priority in text_columns:
                main_text_column = priority
                break

        if not main_text_column:
            main_text_column = text_columns[0]

        # Identificar coluna de timestamp (se dispon√≠vel)
        timestamp_column = None
        for col in df.columns:
            if 'time' in col.lower() or 'date' in col.lower() or 'timestamp' in col.lower():
                timestamp_column = col
                break

        # === PADRONIZA√á√ÉO DE DATETIME ===
        if timestamp_column:
            df = self._standardize_datetime_column(df, timestamp_column)
            # Ap√≥s padroniza√ß√£o, a coluna se chama 'datetime'
            timestamp_column = 'datetime'

        # DETEC√á√ÉO AUTOM√ÅTICA DE FEATURES EXISTENTES
        features_detected = self._detect_existing_features(df)

        # EXTRA√á√ÉO AUTOM√ÅTICA DE FEATURES (se n√£o existem)
        df = self._extract_missing_features(df, main_text_column, features_detected)

        # === CONTAR COLUNAS DE METADADOS ===
        # Metadados = todas as colunas exceto texto principal e datetime padronizado
        metadata_columns = []
        for col in df.columns:
            if col not in [main_text_column, 'datetime'] and not col.startswith(('emojis_', 'hashtags_', 'urls_', 'mentions_')):
                metadata_columns.append(col)

        # Adicionar features identificadas
        df['main_text_column'] = main_text_column
        df['timestamp_column'] = timestamp_column if timestamp_column else 'none'
        df['metadata_columns_count'] = len(metadata_columns)
        df['has_timestamp'] = timestamp_column is not None

        self.stats['stages_completed'] += 1
        self.stats['features_extracted'] += 4 + len(features_detected['extracted'])

        self.logger.info(f"‚úÖ Features: text={main_text_column}, timestamp={timestamp_column}")
        self.logger.info(f"‚úÖ Features detectadas: {list(features_detected['existing'].keys())}")
        self.logger.info(f"‚úÖ Features extra√≠das: {features_detected['extracted']}")
        if timestamp_column:
            self.logger.info(f"üìÖ Datetime otimizado: coluna √∫nica 'datetime'")
        return df

    def _standardize_datetime_column(self, df: pd.DataFrame, timestamp_column: str) -> pd.DataFrame:
        """
        Padronizar coluna de datetime para formato √∫nico DD/MM/AAAA HH:MM:SS.
        Remove coluna original e substitui por vers√£o padronizada.
        
        Args:
            df: DataFrame com dados
            timestamp_column: Nome da coluna de timestamp identificada
            
        Returns:
            DataFrame com coluna datetime padronizada (substitui a original)
        """
        self.logger.info(f"üìÖ Padronizando datetime da coluna: {timestamp_column}")
        
        def parse_datetime(datetime_str):
            """Tentar m√∫ltiplos formatos de datetime."""
            if pd.isna(datetime_str):
                return None
                
            datetime_str = str(datetime_str).strip()
            
            # Formatos comuns para tentar
            formats_to_try = [
                '%Y-%m-%d %H:%M:%S',      # 2019-07-02 01:10:00
                '%d/%m/%Y %H:%M:%S',      # 02/07/2019 01:10:00
                '%Y-%m-%d',               # 2019-07-02
                '%d/%m/%Y',               # 02/07/2019
                '%Y-%m-%d %H:%M',         # 2019-07-02 01:10
                '%d/%m/%Y %H:%M',         # 02/07/2019 01:10
                '%Y/%m/%d %H:%M:%S',      # 2019/07/02 01:10:00
                '%m/%d/%Y %H:%M:%S',      # 07/02/2019 01:10:00 (formato americano)
            ]
            
            for fmt in formats_to_try:
                try:
                    parsed_dt = pd.to_datetime(datetime_str, format=fmt)
                    # Converter para formato padr√£o brasileiro DD/MM/AAAA HH:MM:SS
                    return parsed_dt.strftime('%d/%m/%Y %H:%M:%S')
                except (ValueError, TypeError):
                    continue
                    
            # Se nenhum formato funcionou, tentar parse gen√©rico do pandas
            try:
                parsed_dt = pd.to_datetime(datetime_str, infer_datetime_format=True)
                return parsed_dt.strftime('%d/%m/%Y %H:%M:%S')
            except:
                return None
        
        # Aplicar padroniza√ß√£o
        datetime_standardized = df[timestamp_column].apply(parse_datetime)
        
        # === SUBSTITUIR COLUNA ORIGINAL ===
        # Remover coluna original e usar nome 'datetime' para a vers√£o padronizada
        df = df.drop(columns=[timestamp_column])
        df['datetime'] = datetime_standardized
        
        # Estat√≠sticas de convers√£o
        valid_datetimes = df['datetime'].notna().sum()
        total_records = len(df)
        success_rate = (valid_datetimes / total_records) * 100
        
        self.logger.info(f"‚úÖ Datetime padronizado e substitu√≠do: {valid_datetimes}/{total_records} ({success_rate:.1f}%) v√°lidos")
        
        # Amostras do resultado
        sample_standardized = df['datetime'].dropna().head(3).tolist()
        
        self.logger.info(f"üìã Formato final:")
        for i, std in enumerate(sample_standardized):
            self.logger.info(f"   {i+1}. {std}")
        
        return df

    def _detect_existing_features(self, df: pd.DataFrame) -> Dict:
        """
        Detecta features que j√° existem como colunas no DataFrame.
        """
        existing_features = {}

        # Features de interesse para detectar
        feature_patterns = {
            'hashtags': ['hashtag', 'hashtags', 'tags'],
            'urls': ['url', 'urls', 'links', 'link'],
            'mentions': ['mention', 'mentions', 'user_mentions', 'usuarios'],
            'emojis': ['emoji', 'emojis', 'emoticon'],
            'reply_count': ['reply', 'replies', 'respostas'],
            'retweet_count': ['retweet', 'retweets', 'rt_count'],
            'like_count': ['like', 'likes', 'curtidas', 'fav'],
            'user_info': ['user', 'username', 'author', 'usuario']
        }

        for feature_name, patterns in feature_patterns.items():
            for pattern in patterns:
                matching_cols = [col for col in df.columns if pattern in col.lower()]
                if matching_cols:
                    existing_features[feature_name] = matching_cols[0]
                    break

        return {
            'existing': existing_features,
            'extracted': []
        }

    def _extract_missing_features(self, df: pd.DataFrame, text_column: str, features_info: Dict) -> pd.DataFrame:
        """
        Extrai apenas features essenciais do texto principal.
        """
        extracted_features = []

        # Verificar se a coluna de texto existe
        if text_column not in df.columns:
            self.logger.error(f"‚ùå Coluna de texto '{text_column}' n√£o encontrada no DataFrame")
            self.logger.error(f"Colunas dispon√≠veis: {list(df.columns)}")
            # Usar primeira coluna dispon√≠vel como fallback
            text_column = df.columns[0] if len(df.columns) > 0 else 'body'
            self.logger.warning(f"‚ö†Ô∏è Usando coluna '{text_column}' como fallback")

        # S√≥ extrair se n√£o existir coluna correspondente
        if 'hashtags' not in features_info['existing']:
            df['hashtags_extracted'] = df[text_column].astype(str).str.findall(r'#\w+')
            extracted_features.append('hashtags')

        if 'urls' not in features_info['existing']:
            url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
            df['urls_extracted'] = df[text_column].astype(str).str.findall(url_pattern)
            extracted_features.append('urls')

        if 'mentions' not in features_info['existing']:
            # Padr√£o para @mentions
            df['mentions_extracted'] = df[text_column].astype(str).str.findall(r'@\w+')
            extracted_features.append('mentions')

        if 'emojis' not in features_info['existing']:
            # Padr√£o b√°sico para emojis (Unicode)
            emoji_pattern = r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002600-\U000027BF]'
            df['emojis_extracted'] = df[text_column].astype(str).str.findall(emoji_pattern)
            extracted_features.append('emojis')

        # REMOVIDAS: has_interrogation, has_exclamation, has_caps_words, has_portuguese_words
        # Estas colunas n√£o s√£o necess√°rias para a an√°lise

        features_info['extracted'] = extracted_features
        return df

    def _stage_02_text_preprocessing(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        STAGE 02: Valida√ß√£o de Features + Limpeza de Texto.

        Para datasets com estrutura correta (datetime, body, url, hashtag, channel, etc):
        1. Validar features existentes contra coluna 'body' 
        2. Corrigir features incorretas/vazias
        3. Limpar body_cleaned (texto sem features)
        4. Aplicar normaliza√ß√£o de texto
        
        USA: Detec√ß√£o autom√°tica da estrutura do dataset
        """
        self.logger.info("üßπ STAGE 02: Feature Validation + Text Preprocessing")

        # === DETECTAR ESTRUTURA DO DATASET ===
        expected_columns = ['datetime', 'body', 'url', 'hashtag', 'channel', 'is_fwrd', 'mentions', 'sender', 'media_type', 'domain', 'body_cleaned']
        
        if all(col in df.columns for col in expected_columns[:5]):  # Verificar colunas essenciais
            self.logger.info("‚úÖ Dataset estruturado detectado - validando features existentes")
            
            # === FASE 1: VALIDA√á√ÉO DE FEATURES EXISTENTES ===
            df = self._extract_and_validate_features(df, 'body')
            
            # === FASE 2: LIMPEZA DE TEXTO (usar body como principal) ===
            main_text_col = 'body'
            
        else:
            self.logger.info("‚ö†Ô∏è Dataset n√£o estruturado - usando coluna principal")
            
            # Obter nome da coluna principal de texto (armazenado no Stage 01)
            if 'main_text_column' in df.columns and len(df) > 0:
                main_text_col = df['main_text_column'].iloc[0]
                self.logger.info(f"üîç Coluna principal identificada: {main_text_col}")
                
                # Verificar se a coluna existe realmente
                if main_text_col not in df.columns:
                    self.logger.warning(f"‚ö†Ô∏è Coluna '{main_text_col}' n√£o encontrada, buscando alternativa")
                    # Buscar coluna de texto v√°lida
                    text_columns = [col for col in df.columns if df[col].dtype == 'object' and col not in ['main_text_column', 'timestamp_column']]
                    if text_columns:
                        main_text_col = text_columns[0]
                        self.logger.info(f"‚úÖ Usando coluna alternativa: {main_text_col}")
                    else:
                        raise ValueError("‚ùå Nenhuma coluna de texto v√°lida encontrada")
            else:
                # Fallback: buscar coluna de texto
                text_columns = [col for col in df.columns if df[col].dtype == 'object']
                if text_columns:
                    main_text_col = text_columns[0]
                    self.logger.warning(f"‚ö†Ô∏è Usando primeira coluna de texto dispon√≠vel: {main_text_col}")
                else:
                    raise ValueError("‚ùå Nenhuma coluna de texto encontrada")
            
            # === FASE 1: EXTRA√á√ÉO DE FEATURES ===
            df = self._extract_and_validate_features(df, main_text_col)
        
        # === FASE 2: NORMALIZA√á√ÉO DE TEXTO ===
        def clean_text(text):
            """Limpar texto usando Python puro."""
            if pd.isna(text):
                return ""

            text = str(text)

            # Normalizar unicode
            text = unicodedata.normalize('NFKD', text)

            # Remover caracteres especiais mas preservar acentos
            text = re.sub(r'[^\w\s\u00C0-\u017F]', ' ', text)

            # Normalizar espa√ßos
            text = re.sub(r'\s+', ' ', text).strip()

            # Converter para lowercase
            text = text.lower()

            return text

        # Aplicar limpeza ao texto principal
        df['normalized_text'] = df[main_text_col].apply(clean_text)

        self.stats['stages_completed'] += 1
        self.stats['features_extracted'] += 2

        self.logger.info(f"‚úÖ Stage 02 conclu√≠do: {df['normalized_text'].str.len().mean():.1f} chars m√©dia")
        return df

    def _extract_and_validate_features(self, df: pd.DataFrame, main_text_col: str) -> pd.DataFrame:
        """
        Validar features existentes contra coluna 'body' e corrigir se necess√°rio.
        
        Dataset j√° tem: datetime, body, url, hashtag, channel, is_fwrd, mentions, sender, media_type, domain, body_cleaned
        """
        self.logger.info("üîç Validando features existentes contra coluna 'body'...")
        
        # === VERIFICAR SE DATASET TEM ESTRUTURA CORRETA ===
        expected_columns = ['datetime', 'body', 'url', 'hashtag', 'channel', 'is_fwrd', 'mentions', 'sender', 'media_type', 'domain', 'body_cleaned']
        
        # Se o dataset tem as colunas corretas, usar body como texto principal
        if all(col in df.columns for col in expected_columns[:5]):  # Verificar colunas essenciais
            self.logger.info("‚úÖ Dataset com estrutura correta detectado")
            
            # === REMOVER BODY_CLEANED (duplica√ß√£o desnecess√°ria) ===
            if 'body_cleaned' in df.columns:
                df = df.drop(columns=['body_cleaned'])
                self.logger.info("üóëÔ∏è body_cleaned removido (duplica√ß√£o desnecess√°ria)")
            
            # === VALIDAR FEATURES CONTRA BODY ===
            corrections_made = 0
            
            # Validar URL
            if 'url' in df.columns and 'body' in df.columns:
                corrections_made += self._validate_feature_against_body(df, 'url', 'body', [r'https?://\S+', r'www\.\S+'])
            
            # Validar Hashtags
            if 'hashtag' in df.columns and 'body' in df.columns:
                corrections_made += self._validate_feature_against_body(df, 'hashtag', 'body', [r'#\w+'])
            
            # Validar Mentions
            if 'mentions' in df.columns and 'body' in df.columns:
                corrections_made += self._validate_feature_against_body(df, 'mentions', 'body', [r'@\w+'])
            
            self.logger.info(f"‚úÖ Valida√ß√£o conclu√≠da: {corrections_made} corre√ß√µes aplicadas")
            
        else:
            # === DATASET SEM ESTRUTURA PADR√ÉO - EXTRAIR TUDO ===
            self.logger.info("‚ö†Ô∏è Dataset sem estrutura padr√£o - extraindo features do texto principal")
            df = self._extract_features_from_text(df, main_text_col)
        
        return df
    
    def _validate_feature_against_body(self, df: pd.DataFrame, feature_col: str, body_col: str, patterns: list) -> int:
        """Validar feature espec√≠fica contra body."""
        corrections = 0
        
        for idx, row in df.iterrows():
            body_text = str(row[body_col]) if pd.notna(row[body_col]) else ""
            existing_feature = row[feature_col] if pd.notna(row[feature_col]) else ""
            
            # Extrair feature do body
            extracted_features = []
            for pattern in patterns:
                matches = re.findall(pattern, body_text, re.IGNORECASE)
                extracted_features.extend(matches)
            
            # Se encontrou features no body mas coluna est√° vazia, corrigir
            if extracted_features and not existing_feature:
                if len(extracted_features) == 1:
                    df.at[idx, feature_col] = extracted_features[0]
                else:
                    df.at[idx, feature_col] = ';'.join(extracted_features)  # M√∫ltiplas features
                corrections += 1
        
        if corrections > 0:
            self.logger.info(f"üîß {feature_col}: {corrections} corre√ß√µes aplicadas")
        
        return corrections
    
    def _clean_body_text(self, df: pd.DataFrame):
        """
        REMOVIDO: body_cleaned n√£o √© mais necess√°rio.
        O texto limpo √© gerado como 'normalized_text' no Stage 02.
        """
        # N√£o fazer nada - body_cleaned removido para evitar duplica√ß√£o
        self.logger.info("üóëÔ∏è body_cleaned removido (duplica√ß√£o desnecess√°ria)")
        pass    
    def _extract_features_from_text(self, df: pd.DataFrame, text_col: str) -> pd.DataFrame:
        """Extrair features de dataset sem estrutura padr√£o (fallback)."""
        
        # Features essenciais para extrair
        feature_patterns = {
            'urls': [r'https?://\S+', r'www\.\S+'],
            'hashtags': [r'#\w+'],
            'mentions': [r'@\w+'],
            'channel_name': []  # Usar valor padr√£o
        }
        
        for feature_name, patterns in feature_patterns.items():
            if patterns:
                def extract_feature(text):
                    if pd.isna(text):
                        return []
                    
                    extracted = []
                    for pattern in patterns:
                        matches = re.findall(pattern, str(text), re.IGNORECASE)
                        extracted.extend(matches)
                    return list(set(extracted)) if extracted else []
                
                df[feature_name] = df[text_col].apply(extract_feature)
            else:
                df[feature_name] = "unknown_channel"
        
        self.logger.info("üÜï Features extra√≠das de dataset sem estrutura padr√£o")
        return df
    
    def _find_existing_feature_column(self, df: pd.DataFrame, possible_names: list) -> str:
        """Encontrar coluna existente para uma feature."""
        for col_name in possible_names:
            if col_name in df.columns:
                return col_name
        return None
    
    def _validate_and_correct_feature(self, df: pd.DataFrame, existing_col: str, feature_name: str, patterns: list, text_col: str) -> pd.DataFrame:
        """Validar e corrigir feature existente."""
        if not patterns:  # Channel name n√£o tem padr√£o regex
            self.logger.info(f"‚úÖ Feature {existing_col} mantida (sem valida√ß√£o regex)")
            return df
        
        # Extrair valores corretos do texto
        def extract_correct_values(text):
            if pd.isna(text):
                return []
            
            text = str(text)
            extracted = []
            for pattern in patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                extracted.extend(matches)
            return list(set(extracted))  # Remover duplicatas
        
        # Validar contra texto original
        df['_temp_extracted'] = df[text_col].apply(extract_correct_values)
        
        # Comparar e corrigir se necess√°rio
        corrections_made = 0
        
        def validate_and_fix(row):
            nonlocal corrections_made
            existing_value = row[existing_col]
            correct_value = row['_temp_extracted']
            
            # Se valor existente est√° vazio ou incorreto, corrigir
            if pd.isna(existing_value) or existing_value == [] or existing_value == '':
                if correct_value:
                    corrections_made += 1
                    return correct_value
            
            return existing_value
        
        df[existing_col] = df.apply(validate_and_fix, axis=1)
        df = df.drop('_temp_extracted', axis=1)
        
        if corrections_made > 0:
            self.logger.info(f"üîß Feature {existing_col}: {corrections_made} corre√ß√µes aplicadas")
        else:
            self.logger.info(f"‚úÖ Feature {existing_col}: valida√ß√£o OK, sem corre√ß√µes necess√°rias")
        
        return df
    
    def _extract_new_feature(self, df: pd.DataFrame, feature_name: str, patterns: list, text_col: str) -> pd.DataFrame:
        """Extrair nova feature do texto."""
        def extract_feature(text):
            if pd.isna(text):
                return []
            
            text = str(text)
            extracted = []
            
            if patterns:  # Features com padr√£o regex
                for pattern in patterns:
                    matches = re.findall(pattern, text, re.IGNORECASE)
                    extracted.extend(matches)
            else:  # Channel name - tentar extrair de metadados ou usar valor padr√£o
                return "unknown_channel"
            
            return list(set(extracted)) if extracted else []
        
        # Extrair feature
        df[feature_name] = df[text_col].apply(extract_feature)
        
        # Estat√≠sticas
        non_empty = df[feature_name].apply(lambda x: len(x) > 0 if isinstance(x, list) else bool(x)).sum()
        total = len(df)
        
        self.logger.info(f"üìä Feature {feature_name}: {non_empty}/{total} registros ({non_empty/total*100:.1f}%)")
        
        return df

    def _stage_07_linguistic_processing(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 07: Processamento lingu√≠stico com spaCy.

        USA: normalized_text do Stage 02
        GERA: tokens, lemmas, POS tags, entidades nomeadas
        """
        self.logger.info("üî§ Stage 07: Linguistic Processing (spaCy)")

        if not SPACY_AVAILABLE:
            self.logger.warning("‚ö†Ô∏è spaCy n√£o dispon√≠vel - usando processamento b√°sico")
            return self._linguistic_fallback(df)

        def process_text_with_spacy(text):
            """Processar texto com spaCy otimizado."""
            if pd.isna(text) or len(str(text).strip()) == 0:
                return {
                    'tokens': [],
                    'lemmas': [],
                    'pos_tags': [],
                    'entities': [],
                    'tokens_count': 0,
                    'entities_count': 0
                }

            try:
                # Processar texto (limitando para performance)
                doc = nlp(str(text)[:1000])  # Limitar a 1000 chars para performance

                tokens = [token.text for token in doc if not token.is_space]
                lemmas = [token.lemma_ for token in doc if not token.is_space and token.lemma_ != '-PRON-']
                entities = [(ent.text, ent.label_) for ent in doc.ents]

                return {
                    'tokens': tokens,
                    'lemmas': lemmas,
                    'tokens_count': len(tokens),
                    'entities_count': len(entities)
                }
            except Exception as e:
                self.logger.warning(f"Erro spaCy: {e}")
                return {
                    'tokens': [],
                    'lemmas': [],
                    'tokens_count': 0,
                    'entities_count': 0
                }

        # FIX: spaCy deve receber 'body' (texto cru) ‚Äî normalized_text √© lowercase
        # e sem pontua√ß√£o, o que degrada NER, POS tagging e sentence splitting
        spacy_input_col = 'body' if 'body' in df.columns else 'normalized_text'
        spacy_results = df[spacy_input_col].apply(process_text_with_spacy)

        # Extrair dados do spaCy (removidos pos_tags e entities - n√£o utilizados)
        df['spacy_tokens'] = spacy_results.apply(lambda x: x['tokens'])
        df['spacy_lemmas'] = spacy_results.apply(lambda x: x['lemmas'])
        df['spacy_tokens_count'] = spacy_results.apply(lambda x: x['tokens_count'])
        df['spacy_entities_count'] = spacy_results.apply(lambda x: x['entities_count'])

        # Criar texto processado com lemmas para stages posteriores
        df['lemmatized_text'] = df['spacy_lemmas'].apply(lambda x: ' '.join(x) if x else '')

        self.stats['stages_completed'] += 1
        self.stats['features_extracted'] += 4

        avg_tokens = df['spacy_tokens_count'].mean()
        avg_entities = df['spacy_entities_count'].mean()
        self.logger.info(f"‚úÖ spaCy processado: {avg_tokens:.1f} tokens, {avg_entities:.1f} entidades m√©dia")
        return df

    def _linguistic_fallback(self, df: pd.DataFrame) -> pd.DataFrame:
        """Fallback b√°sico quando spaCy n√£o est√° dispon√≠vel."""
        # FIX: usar body como input (consistente com spaCy path)
        fallback_col = 'body' if 'body' in df.columns else 'normalized_text'
        df['spacy_tokens'] = df[fallback_col].str.split()
        df['spacy_tokens_count'] = df['spacy_tokens'].str.len()
        df['spacy_lemmas'] = df['spacy_tokens']  # sem spaCy, lemmas = tokens
        df['spacy_entities_count'] = 0
        df['lemmatized_text'] = df[fallback_col].str.lower()  # fallback: lowercase do body

        self.stats['stages_completed'] += 1
        self.stats['features_extracted'] += 3
        return df

    def _stage_03_cross_dataset_deduplication(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        STAGE 03: Cross-Dataset Deduplication
        
        Elimina√ß√£o de duplicatas entre TODOS os datasets com contador de frequ√™ncia.
        Algoritmo: Agrupar por texto id√™ntico, manter registro mais antigo, 
        contar duplicatas com dupli_freq.
        
        Redu√ß√£o esperada: 40-50% (300k ‚Üí 180k)
        """
        try:
            self.logger.info("üîÑ STAGE 03: Cross-Dataset Deduplication")
            
            text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
            datetime_column = 'datetime' if 'datetime' in df.columns else df.columns[df.columns.str.contains('date|time', case=False)].tolist()[0] if any(df.columns.str.contains('date|time', case=False)) else None
            
            initial_count = len(df)
            self.logger.info(f"üìä Registros iniciais: {initial_count:,}")
            
            # Agrupar por texto id√™ntico
            grouping_columns = [text_column]
            
            # Preparar dados para agrupamento
            dedup_data = []
            
            for text, group in df.groupby(text_column):
                if pd.isna(text) or text.strip() == '':
                    continue
                    
                # Manter registro mais antigo (primeiro datetime)
                if datetime_column and datetime_column in group.columns:
                    # Converter datetime para ordena√ß√£o
                    group_sorted = group.copy()
                    if group_sorted[datetime_column].dtype == 'object':
                        try:
                            group_sorted['datetime_parsed'] = pd.to_datetime(group_sorted[datetime_column], 
                                                                           format='%d/%m/%Y %H:%M:%S', errors='coerce')
                        except:
                            group_sorted['datetime_parsed'] = pd.to_datetime(group_sorted[datetime_column], errors='coerce')
                        
                        # Ordenar por datetime e pegar o mais antigo
                        oldest_record = group_sorted.sort_values('datetime_parsed').iloc[0]
                    else:
                        oldest_record = group.iloc[0]
                else:
                    oldest_record = group.iloc[0]
                
                # Contador de duplicatas
                dupli_freq = len(group)
                
                # Metadados de dispers√£o
                channels_found = []
                if 'channel' in group.columns:
                    channels_found = group['channel'].dropna().unique().tolist()
                elif 'sender_id' in group.columns:
                    channels_found = group['sender_id'].dropna().unique().tolist()
                
                # Per√≠odo de ocorr√™ncia
                date_span_days = 0
                if datetime_column and datetime_column in group.columns:
                    try:
                        dates = pd.to_datetime(group[datetime_column], errors='coerce').dropna()
                        if len(dates) > 1:
                            date_span_days = (dates.max() - dates.min()).days
                    except:
                        pass
                
                # Criar registro deduplificado
                dedup_record = oldest_record.copy()
                dedup_record['dupli_freq'] = dupli_freq
                dedup_record['channels_found'] = len(channels_found)
                dedup_record['date_span_days'] = date_span_days
                
                dedup_data.append(dedup_record)
            
            # Criar DataFrame deduplificado
            if dedup_data:
                df_deduplicated = pd.DataFrame(dedup_data)
                df_deduplicated = df_deduplicated.reset_index(drop=True)
            else:
                df_deduplicated = df.copy()
                df_deduplicated['dupli_freq'] = 1
                df_deduplicated['channels_found'] = 0
                df_deduplicated['date_span_days'] = 0
            
            final_count = len(df_deduplicated)
            reduction_pct = ((initial_count - final_count) / initial_count * 100) if initial_count > 0 else 0
            
            # Estat√≠sticas de deduplica√ß√£o
            unique_texts = df_deduplicated['dupli_freq'].value_counts().sort_index()
            total_duplicates = df_deduplicated[df_deduplicated['dupli_freq'] > 1]['dupli_freq'].sum()
            
            self.logger.info(f"‚úÖ Deduplica√ß√£o conclu√≠da:")
            self.logger.info(f"   üìâ {initial_count:,} ‚Üí {final_count:,} registros")
            self.logger.info(f"   üìä Redu√ß√£o: {reduction_pct:.1f}%")
            self.logger.info(f"   üîÑ Duplicatas processadas: {total_duplicates:,}")
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 3
            
            return df_deduplicated
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 03: {e}")
            self.stats['processing_errors'] += 1
            # Em caso de erro, adicionar colunas padr√£o
            df['dupli_freq'] = 1
            df['channels_found'] = 0
            df['date_span_days'] = 0
            return df

    def _stage_04_statistical_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        STAGE 04: Statistical Analysis
        
        Comparar in√≠cio do dataset com o dataset reduzido.
        Gerar estat√≠sticas para classifica√ß√£o e gr√°ficos.
        
        Processamentos:
        - Contagem de dados antes e depois
        - Propor√ß√£o de duplicadas
        - Propor√ß√£o de hashtags
        - Detec√ß√£o de repeti√ß√µes excessivas para tabela com 10 principais casos
        """
        try:
            self.logger.info("üìä STAGE 04: Statistical Analysis")
            
            text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
            
            # === AN√ÅLISE DE DUPLICA√á√ÉO ===
            total_registros = len(df)
            registros_unicos = len(df[df['dupli_freq'] == 1])
            registros_duplicados = total_registros - registros_unicos
            
            duplicacao_pct = (registros_duplicados / total_registros * 100) if total_registros > 0 else 0
            
            # === AN√ÅLISE DE HASHTAGS ===
            # FIX: usar coluna 'hashtags_extracted' (Stage 01) ou 'body' (# removido de normalized_text)
            has_hashtags = 0
            if 'hashtags_extracted' in df.columns:
                has_hashtags = df['hashtags_extracted'].apply(
                    lambda x: len(x) > 0 if isinstance(x, list) else bool(x)
                ).sum()
            elif 'body' in df.columns:
                has_hashtags = df['body'].str.contains('#', na=False).sum()
            elif text_column in df.columns:
                has_hashtags = df[text_column].str.contains('#', na=False).sum()
            
            hashtag_pct = (has_hashtags / total_registros * 100) if total_registros > 0 else 0
            
            # === TOP 10 REPETI√á√ïES EXCESSIVAS ===
            top_duplicates = df[df['dupli_freq'] > 1].nlargest(10, 'dupli_freq')[
                [text_column, 'dupli_freq', 'channels_found', 'date_span_days']
            ].to_dict('records')
            
            # === ESTAT√çSTICAS B√ÅSICAS DE TEXTO ===
            if text_column in df.columns:
                char_counts = df[text_column].str.len().fillna(0)
                word_counts = df[text_column].str.split().str.len().fillna(0)
                
                df['char_count'] = char_counts
                df['word_count'] = word_counts
                
                avg_chars = char_counts.mean()
                avg_words = word_counts.mean()
            else:
                avg_chars = 0
                avg_words = 0
                df['char_count'] = 0
                df['word_count'] = 0
            
            # === PROPOR√á√ïES DE QUALIDADE ===
            # FIX: emoji_ratio e caps_ratio devem usar 'body' (texto cru) ‚Äî normalized_text
            # √© lowercase e sem emojis, o que faz essas m√©tricas retornarem sempre 0.0
            raw_col = 'body' if 'body' in df.columns else text_column
            if raw_col in df.columns:
                df['emoji_ratio'] = df[raw_col].apply(self._calculate_emoji_ratio)
                df['caps_ratio'] = df[raw_col].apply(self._calculate_caps_ratio)
                df['repetition_ratio'] = df[raw_col].apply(self._calculate_repetition_ratio)

                # Detec√ß√£o de idioma b√°sica (pode usar normalized_text ‚Äî lowercase ok)
                df['likely_portuguese'] = df[text_column].apply(self._detect_portuguese) if text_column in df.columns else True
            else:
                df['emoji_ratio'] = 0.0
                df['caps_ratio'] = 0.0
                df['repetition_ratio'] = 0.0
                df['likely_portuguese'] = True
            
            # === CONSOLIDA√á√ÉO DE ESTAT√çSTICAS ===
            # Consolidar estat√≠sticas globais em objeto summary
            summary_stats = {
                'total_dataset_size': total_registros,
                'unique_texts_count': registros_unicos,
                'duplication_percentage': round(duplicacao_pct, 2),
                'hashtag_percentage': round(hashtag_pct, 2),
                'avg_chars_per_text': round(avg_chars, 1),
                'avg_words_per_text': round(avg_words, 1)
            }

            # Salvar no contexto para acesso posterior
            self.global_stats = summary_stats
            
            # Log das estat√≠sticas
            self.logger.info(f"‚úÖ An√°lise estat√≠stica conclu√≠da:")
            self.logger.info(f"   üìä Total de registros: {total_registros:,}")
            self.logger.info(f"   üîÑ Duplica√ß√£o: {duplicacao_pct:.1f}%")
            self.logger.info(f"   # Hashtags: {hashtag_pct:.1f}%")
            self.logger.info(f"   üìù M√©dia: {avg_words:.1f} palavras, {avg_chars:.0f} chars")
            
            if top_duplicates:
                self.logger.info(f"   üîù Maior repeti√ß√£o: {top_duplicates[0]['dupli_freq']} ocorr√™ncias")
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 11
            
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 04: {e}")
            self.stats['processing_errors'] += 1
            return df

    def _stage_05_content_quality_filter(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        STAGE 05: Content Quality Filter
        
        Filtrar conte√∫do por qualidade e completude.
        Input: Dados deduplificados
        Output: Apenas conte√∫do de qualidade
        
        Filtros:
        - Comprimento: < 10 chars ou > 2000 chars
        - Qualidade: emoji_ratio > 70%, caps_ratio > 80%, repetition_ratio > 50%
        - Idioma: Manter apenas likely_portuguese = True
        
        Redu√ß√£o esperada: 15-25% (180k ‚Üí 135k)
        """
        try:
            self.logger.info("üéØ STAGE 05: Content Quality Filter")
            
            text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
            initial_count = len(df)
            
            # === FILTROS DE COMPRIMENTO ===
            # Muito curto: < 10 chars (s√≥ emoji/URL)
            length_filter = (df['char_count'] >= 10) & (df['char_count'] <= 2000)
            
            # === FILTROS DE QUALIDADE ===
            # emoji_ratio > 70% = ru√≠do
            emoji_filter = df['emoji_ratio'] <= 0.70
            
            # caps_ratio > 80% = spam  
            caps_filter = df['caps_ratio'] <= 0.80
            
            # repetition_ratio > 50% = baixa qualidade
            repetition_filter = df['repetition_ratio'] <= 0.50
            
            # === FILTROS DE IDIOMA ===
            # Manter apenas likely_portuguese = True
            language_filter = df['likely_portuguese'] == True
            
            # === APLICAR TODOS OS FILTROS ===
            quality_mask = length_filter & emoji_filter & caps_filter & repetition_filter & language_filter
            
            # === GERAR COLUNAS DE QUALIDADE ===
            # Contar problemas por tipo para logging
            problems = {
                'length_issue': (~length_filter).sum(),
                'excessive_emojis': (~emoji_filter).sum(),
                'excessive_caps': (~caps_filter).sum(),
                'excessive_repetition': (~repetition_filter).sum(),
                'non_portuguese': (~language_filter).sum()
            }
            
            # Content quality score (0-100)
            quality_components = [
                length_filter.astype(int) * 20,  # 20 pontos para comprimento adequado
                emoji_filter.astype(int) * 20,   # 20 pontos para emojis adequados
                caps_filter.astype(int) * 20,    # 20 pontos para caps adequados
                repetition_filter.astype(int) * 20, # 20 pontos para repeti√ß√£o adequada
                language_filter.astype(int) * 20    # 20 pontos para portugu√™s
            ]
            
            df['content_quality_score'] = sum(quality_components)
            
            # === APLICAR FILTRO ===
            df_filtered = df[quality_mask].copy().reset_index(drop=True)
            
            final_count = len(df_filtered)
            reduction_pct = ((initial_count - final_count) / initial_count * 100) if initial_count > 0 else 0
            
            # === ESTAT√çSTICAS DOS FILTROS ===
            avg_quality_score = df_filtered['content_quality_score'].mean()

            self.logger.info(f"‚úÖ Filtro de qualidade aplicado:")
            self.logger.info(f"   üìâ {initial_count:,} ‚Üí {final_count:,} registros")
            self.logger.info(f"   üìä Redu√ß√£o: {reduction_pct:.1f}%")
            self.logger.info(f"   üéØ Score qualidade m√©dio: {avg_quality_score:.1f}/100")
            self.logger.info(f"   ‚ùå Rejeitados: comprimento={problems['length_issue']}, emojis={problems['excessive_emojis']}")
            self.logger.info(f"      caps={problems['excessive_caps']}, repeti√ß√£o={problems['excessive_repetition']}, idioma={problems['non_portuguese']}")

            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 1
            
            return df_filtered
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 05: {e}")
            self.stats['processing_errors'] += 1
            # Em caso de erro, retornar dados originais com colunas padr√£o
            df['content_quality_score'] = 80
            return df

    def _stage_06_affordances_classification(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        STAGE 06: Affordances Classification (H√≠brido: Heur√≠stica + API)

        Estrat√©gia otimizada em 3 fases:
        1. Heur√≠stica expandida classifica todas as mensagens com scoring
        2. Mensagens de alta confian√ßa (>=0.6) ficam com resultado heur√≠stico
        3. Mensagens de baixa confian√ßa (<0.6) s√£o enviadas √† API em batches de 10

        Categorias:
        - noticia, midia_social, video_audio_gif, opiniao,
        - mobilizacao, ataque, interacao, is_forwarded
        """
        try:
            self.logger.info("üéØ STAGE 06: Affordances Classification (H√≠brido)")

            import os
            import requests
            import json
            import time
            from typing import List, Dict, Any

            text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
            initial_count = len(df)

            # === FASE 1: Heur√≠stica expandida em todas as mensagens ===
            self.logger.info(f"   üìã Fase 1: Heur√≠stica expandida em {initial_count} mensagens...")
            df = self._stage_06_affordances_heuristic_fallback(df)

            # Contar mensagens por n√≠vel de confian√ßa
            high_conf_mask = df['affordance_confidence'] >= 0.6
            low_conf_mask = df['affordance_confidence'] < 0.6
            high_conf_count = high_conf_mask.sum()
            low_conf_count = low_conf_mask.sum()

            self.logger.info(f"   üü¢ Alta confian√ßa heur√≠stica: {high_conf_count} ({high_conf_count/initial_count*100:.1f}%)")
            self.logger.info(f"   üü° Baixa confian√ßa (candidatas API): {low_conf_count} ({low_conf_count/initial_count*100:.1f}%)")

            # === FASE 2: Verificar se API est√° dispon√≠vel ===
            api_key = os.getenv('ANTHROPIC_API_KEY')
            if not api_key:
                self.logger.warning("‚ö†Ô∏è ANTHROPIC_API_KEY n√£o encontrada. Usando apenas heur√≠stica.")
                # Limpar coluna tempor√°ria
                if '_heuristic_scores' in df.columns:
                    df = df.drop(columns=['_heuristic_scores'])
                self.stats['stages_completed'] += 1
                self.stats['features_extracted'] += 10
                return df

            if low_conf_count == 0:
                self.logger.info("   ‚úÖ Todas as mensagens classificadas com alta confian√ßa. API n√£o necess√°ria.")
                if '_heuristic_scores' in df.columns:
                    df = df.drop(columns=['_heuristic_scores'])
                self.stats['stages_completed'] += 1
                self.stats['features_extracted'] += 10
                return df

            # === FASE 3: Classificar mensagens de baixa confian√ßa via API (batches de 10) ===
            self.logger.info(f"   ü§ñ Fase 3: Enviando {low_conf_count} mensagens √† API em batches de 10...")

            # Modelo configur√°vel via .env (default: Haiku 3.5)
            configured_model = os.getenv('ANTHROPIC_MODEL', 'claude-sonnet-4-20250514')
            self.logger.info(f"   üîß Modelo API: {configured_model}")

            api_config = {
                'model': configured_model,
                'max_tokens': 800,
                'temperature': 0.1,
                'system_prompt': """Voc√™ √© um classificador de conte√∫do especializado em discurso pol√≠tico brasileiro em redes sociais.

Classifique CADA mensagem numerada de acordo com as categorias de affordances (m√∫ltiplas poss√≠veis):

1. noticia: Conte√∫do informativo, reportagem, fatos
2. midia_social: Posts de redes sociais, compartilhamentos
3. video_audio_gif: Refer√™ncias a conte√∫do multim√≠dia
4. opiniao: Opini√µes pessoais, coment√°rios subjetivos
5. mobilizacao: Chamadas para a√ß√£o, mobiliza√ß√£o pol√≠tica
6. ataque: Ataques pessoais, insultos, agress√µes verbais
7. interacao: Respostas, men√ß√µes, conversa√ß√µes diretas
8. is_forwarded: Conte√∫do encaminhado/repassado

Responda APENAS com um JSON array v√°lido. Exemplo para 3 mensagens:
[{"id":1,"categorias":["opiniao","ataque"],"confianca":0.9},{"id":2,"categorias":["noticia"],"confianca":0.85},{"id":3,"categorias":["mobilizacao"],"confianca":0.8}]"""
            }

            def classify_batch_with_anthropic(texts: List[str]) -> List[Dict[str, Any]]:
                """Classificar batch de textos (at√© 10) em uma √∫nica chamada API."""
                # Montar mensagem com textos numerados
                numbered_texts = []
                for i, text in enumerate(texts, 1):
                    text_sample = str(text)[:400] if not pd.isna(text) else ''
                    if len(text_sample.strip()) < 10:
                        text_sample = '(mensagem vazia ou muito curta)'
                    numbered_texts.append(f"[{i}] {text_sample}")

                user_content = "Classifique estas mensagens:\n\n" + "\n\n".join(numbered_texts)

                headers = {
                    'Content-Type': 'application/json',
                    'x-api-key': api_key,
                    'anthropic-version': '2023-06-01',
                    'anthropic-beta': 'prompt-caching-2024-07-31'
                }

                payload = {
                    'model': api_config['model'],
                    'max_tokens': api_config['max_tokens'],
                    'temperature': api_config['temperature'],
                    'system': [
                        {
                            'type': 'text',
                            'text': api_config['system_prompt'],
                            'cache_control': {'type': 'ephemeral'}
                        }
                    ],
                    'messages': [{'role': 'user', 'content': user_content}]
                }

                try:
                    response = requests.post(
                        'https://api.anthropic.com/v1/messages',
                        headers=headers,
                        json=payload,
                        timeout=60
                    )

                    if response.status_code == 200:
                        result = response.json()
                        content = result['content'][0]['text'].strip()

                        # Parse JSON array
                        try:
                            classifications = json.loads(content)
                            if isinstance(classifications, list):
                                return classifications
                        except json.JSONDecodeError:
                            # Tentar extrair JSON de resposta
                            if '[' in content and ']' in content:
                                json_start = content.find('[')
                                json_end = content.rfind(']') + 1
                                try:
                                    classifications = json.loads(content[json_start:json_end])
                                    if isinstance(classifications, list):
                                        return classifications
                                except Exception:
                                    pass

                    elif response.status_code == 429:
                        self.logger.warning("‚ö†Ô∏è Rate limit atingido, aguardando 5s...")
                        time.sleep(5)

                    else:
                        self.logger.warning(f"‚ö†Ô∏è API error: {response.status_code}")

                except requests.RequestException as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro de conex√£o: {e}")

                # Retorno vazio em caso de erro
                return []

            # Processar mensagens de baixa confian√ßa
            low_conf_indices = df.index[low_conf_mask].tolist()

            # Verificar se deve usar Batch API (ass√≠ncrona) ou chamadas individuais
            use_batch_api = os.getenv('USE_BATCH_API', 'false').lower() in ('true', '1', 'yes')

            if use_batch_api and low_conf_count > 100:
                # === BATCH API (ass√≠ncrona, 50% desconto, at√© 24h) ===
                self.logger.info(f"   üì¶ Usando Batch API ass√≠ncrona para {low_conf_count} mensagens...")
                df = self._stage_06_submit_batch_api(
                    df, low_conf_indices, text_column, api_key, api_config
                )
            else:
                # === CHAMADAS INDIVIDUAIS (s√≠ncrono, batches de 10) ===
                if use_batch_api and low_conf_count <= 100:
                    self.logger.info(f"   ‚ÑπÔ∏è Batch API n√£o eficiente para {low_conf_count} mensagens. Usando chamadas diretas.")

                batch_size = 10
                api_calls_made = 0
                api_successes = 0
                api_failures = 0

                for i in range(0, len(low_conf_indices), batch_size):
                    batch_indices = low_conf_indices[i:i+batch_size]
                    batch_texts = [df.loc[idx, text_column] for idx in batch_indices]

                    classifications = classify_batch_with_anthropic(batch_texts)
                    api_calls_made += 1

                    if classifications:
                        for j, idx in enumerate(batch_indices):
                            if j < len(classifications):
                                cls = classifications[j]
                                if isinstance(cls, dict) and 'categorias' in cls:
                                    df.at[idx, 'affordance_categories'] = cls['categorias']
                                    df.at[idx, 'affordance_confidence'] = cls.get('confianca', 0.8)
                                    for aff_type in ['noticia', 'midia_social', 'video_audio_gif', 'opiniao',
                                                    'mobilizacao', 'ataque', 'interacao', 'is_forwarded']:
                                        df.at[idx, f'aff_{aff_type}'] = 1 if aff_type in cls['categorias'] else 0
                                    api_successes += 1
                                else:
                                    api_failures += 1
                            else:
                                api_failures += 1
                    else:
                        api_failures += len(batch_indices)

                    time.sleep(0.2)

                    if api_calls_made % 100 == 0:
                        progress = min(100, (i / len(low_conf_indices)) * 100)
                        self.logger.info(f"   üîÑ API Progresso: {progress:.1f}% ({api_calls_made} calls, {api_successes} sucessos)")

            # === ESTAT√çSTICAS FINAIS ===
            avg_confidence = df['affordance_confidence'].mean()
            classified_count = len(df[df['affordance_confidence'] > 0.1])

            affordance_types = ['noticia', 'midia_social', 'video_audio_gif', 'opiniao',
                              'mobilizacao', 'ataque', 'interacao', 'is_forwarded']
            category_counts = {}
            for affordance_type in affordance_types:
                count = df[f'aff_{affordance_type}'].sum()
                category_counts[affordance_type] = count

            # Limpar coluna tempor√°ria
            if '_heuristic_scores' in df.columns:
                df = df.drop(columns=['_heuristic_scores'])

            self.logger.info(f"‚úÖ Classifica√ß√£o H√≠brida de Affordances conclu√≠da:")
            self.logger.info(f"   üìä Heur√≠stica: {high_conf_count} mensagens ({high_conf_count/initial_count*100:.1f}%)")
            api_mode = "Batch API" if (use_batch_api and low_conf_count > 100) else "chamadas diretas"
            self.logger.info(f"   ü§ñ API ({api_mode}): {low_conf_count} mensagens processadas")
            self.logger.info(f"   ‚úÖ Total classificadas: {classified_count}/{initial_count}")
            self.logger.info(f"   üéØ Confian√ßa m√©dia: {avg_confidence:.3f}")

            top_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:5]
            self.logger.info(f"   üîù Top categorias: {dict(top_categories)}")

            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += len(affordance_types) + 2

            return df

        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 06: {e}")
            self.stats['processing_errors'] += 1
            # Fallback heur√≠stico
            return self._stage_06_affordances_heuristic_fallback(df)

    def _stage_06_submit_batch_api(self, df: pd.DataFrame, low_conf_indices: list,
                                    text_column: str, api_key: str, api_config: dict) -> pd.DataFrame:
        """
        Submeter mensagens de baixa confian√ßa √† Anthropic Batch API.
        Processa at√© 10.000 requests por batch com 50% de desconto.

        Args:
            df: DataFrame com dados
            low_conf_indices: √çndices de mensagens de baixa confian√ßa
            text_column: Nome da coluna de texto
            api_key: Chave da API Anthropic
            api_config: Configura√ß√µes do modelo (model, system_prompt, etc.)

        Returns:
            DataFrame atualizado com classifica√ß√µes da Batch API
        """
        import requests
        import json
        import time
        import tempfile
        import os

        batch_size = 10  # Mensagens por request individual dentro do batch
        max_requests_per_batch = 10000  # Limite da Batch API

        self.logger.info(f"   üì¶ Preparando Batch API: {len(low_conf_indices)} mensagens em batches de {batch_size}")

        # === FASE 1: Gerar requests para a Batch API ===
        batch_requests = []
        request_mapping = {}  # custom_id -> lista de √≠ndices do DataFrame

        for i in range(0, len(low_conf_indices), batch_size):
            batch_indices = low_conf_indices[i:i+batch_size]
            batch_texts = []
            for idx in batch_indices:
                text = df.loc[idx, text_column]
                text_sample = str(text)[:400] if not pd.isna(text) else ''
                if len(text_sample.strip()) < 10:
                    text_sample = '(mensagem vazia ou muito curta)'
                batch_texts.append(text_sample)

            # Montar mensagem com textos numerados
            numbered_texts = [f"[{j+1}] {t}" for j, t in enumerate(batch_texts)]
            user_content = "Classifique estas mensagens:\n\n" + "\n\n".join(numbered_texts)

            custom_id = f"batch_{i//batch_size:06d}"
            request_mapping[custom_id] = batch_indices

            request = {
                "custom_id": custom_id,
                "params": {
                    "model": api_config['model'],
                    "max_tokens": api_config['max_tokens'],
                    "temperature": api_config['temperature'],
                    "system": [
                        {
                            "type": "text",
                            "text": api_config['system_prompt'],
                            "cache_control": {"type": "ephemeral"}
                        }
                    ],
                    "messages": [{"role": "user", "content": user_content}]
                }
            }
            batch_requests.append(request)

        total_requests = len(batch_requests)
        self.logger.info(f"   üìù {total_requests} requests gerados para Batch API")

        # === FASE 2: Submeter batches (at√© 10.000 por vez) ===
        all_results = {}

        for batch_start in range(0, total_requests, max_requests_per_batch):
            batch_chunk = batch_requests[batch_start:batch_start + max_requests_per_batch]
            chunk_num = batch_start // max_requests_per_batch + 1
            total_chunks = (total_requests + max_requests_per_batch - 1) // max_requests_per_batch

            self.logger.info(f"   üöÄ Submetendo batch {chunk_num}/{total_chunks} ({len(batch_chunk)} requests)...")

            headers = {
                'Content-Type': 'application/json',
                'x-api-key': api_key,
                'anthropic-version': '2023-06-01',
                'anthropic-beta': 'prompt-caching-2024-07-31'
            }

            payload = {"requests": batch_chunk}

            try:
                response = requests.post(
                    'https://api.anthropic.com/v1/messages/batches',
                    headers=headers,
                    json=payload,
                    timeout=120
                )

                if response.status_code != 200:
                    self.logger.error(f"   ‚ùå Batch API erro: {response.status_code} - {response.text[:200]}")
                    continue

                batch_response = response.json()
                batch_id = batch_response['id']
                self.logger.info(f"   ‚úÖ Batch {batch_id} criado. Status: {batch_response['processing_status']}")

                # === FASE 3: Polling para resultados ===
                results = self._stage_06_poll_batch_results(batch_id, api_key, headers)
                all_results.update(results)

            except requests.RequestException as e:
                self.logger.error(f"   ‚ùå Erro ao submeter batch: {e}")
                continue

        # === FASE 4: Aplicar resultados ao DataFrame ===
        api_successes = 0
        api_failures = 0

        for custom_id, classifications in all_results.items():
            if custom_id not in request_mapping:
                continue

            batch_indices = request_mapping[custom_id]

            if classifications:
                for j, idx in enumerate(batch_indices):
                    if j < len(classifications):
                        cls = classifications[j]
                        if isinstance(cls, dict) and 'categorias' in cls:
                            df.at[idx, 'affordance_categories'] = cls['categorias']
                            df.at[idx, 'affordance_confidence'] = cls.get('confianca', 0.8)
                            for aff_type in ['noticia', 'midia_social', 'video_audio_gif', 'opiniao',
                                            'mobilizacao', 'ataque', 'interacao', 'is_forwarded']:
                                df.at[idx, f'aff_{aff_type}'] = 1 if aff_type in cls['categorias'] else 0
                            api_successes += 1
                        else:
                            api_failures += 1
                    else:
                        api_failures += 1
            else:
                api_failures += len(batch_indices)

        self.logger.info(f"   üìä Batch API conclu√≠da: {api_successes} sucessos, {api_failures} falhas")
        return df

    def _stage_06_poll_batch_results(self, batch_id: str, api_key: str, headers: dict,
                                      max_wait_seconds: int = 86400, poll_interval: int = 30) -> dict:
        """
        Polling para resultados da Batch API.

        Args:
            batch_id: ID do batch submetido
            api_key: Chave da API
            headers: Headers HTTP
            max_wait_seconds: Tempo m√°ximo de espera (default: 24h)
            poll_interval: Intervalo entre polls (default: 30s)

        Returns:
            Dict de custom_id -> lista de classifica√ß√µes
        """
        import requests
        import json
        import time

        results = {}
        start_time = time.time()

        self.logger.info(f"   ‚è≥ Aguardando resultados do batch {batch_id}...")

        while time.time() - start_time < max_wait_seconds:
            try:
                # Verificar status do batch
                status_response = requests.get(
                    f'https://api.anthropic.com/v1/messages/batches/{batch_id}',
                    headers={
                        'x-api-key': api_key,
                        'anthropic-version': '2023-06-01'
                    },
                    timeout=30
                )

                if status_response.status_code != 200:
                    self.logger.warning(f"   ‚ö†Ô∏è Erro ao verificar status: {status_response.status_code}")
                    time.sleep(poll_interval)
                    continue

                batch_status = status_response.json()
                processing_status = batch_status['processing_status']
                request_counts = batch_status.get('request_counts', {})

                processing = request_counts.get('processing', 0)
                succeeded = request_counts.get('succeeded', 0)
                errored = request_counts.get('errored', 0)
                total = processing + succeeded + errored

                elapsed = int(time.time() - start_time)
                self.logger.info(
                    f"   ‚è≥ Batch {batch_id}: {processing_status} "
                    f"({succeeded}/{total} ok, {errored} erros, {elapsed}s decorridos)"
                )

                if processing_status == 'ended':
                    # Coletar resultados
                    results_url = batch_status.get('results_url')
                    if results_url:
                        results = self._stage_06_fetch_batch_results(results_url, api_key)
                    else:
                        # Usar endpoint direto
                        results = self._stage_06_fetch_batch_results(
                            f'https://api.anthropic.com/v1/messages/batches/{batch_id}/results',
                            api_key
                        )

                    self.logger.info(f"   ‚úÖ Batch {batch_id} finalizado: {len(results)} resultados coletados")
                    return results

            except requests.RequestException as e:
                self.logger.warning(f"   ‚ö†Ô∏è Erro no polling: {e}")

            time.sleep(poll_interval)

        self.logger.warning(f"   ‚ö†Ô∏è Timeout: batch {batch_id} n√£o concluiu em {max_wait_seconds}s")
        return results

    def _stage_06_fetch_batch_results(self, results_url: str, api_key: str) -> dict:
        """
        Buscar e parsear resultados da Batch API (JSONL).

        Args:
            results_url: URL dos resultados
            api_key: Chave da API

        Returns:
            Dict de custom_id -> lista de classifica√ß√µes
        """
        import requests
        import json

        results = {}

        try:
            response = requests.get(
                results_url,
                headers={
                    'x-api-key': api_key,
                    'anthropic-version': '2023-06-01'
                },
                timeout=120,
                stream=True
            )

            if response.status_code != 200:
                self.logger.error(f"   ‚ùå Erro ao buscar resultados: {response.status_code}")
                return results

            # Parsear JSONL
            for line in response.iter_lines():
                if not line:
                    continue
                try:
                    entry = json.loads(line)
                    custom_id = entry.get('custom_id', '')
                    result = entry.get('result', {})

                    if result.get('type') == 'succeeded':
                        message = result.get('message', {})
                        content_blocks = message.get('content', [])

                        # Extrair texto da resposta
                        text_content = ''
                        for block in content_blocks:
                            if block.get('type') == 'text':
                                text_content = block.get('text', '').strip()
                                break

                        # Parsear JSON de classifica√ß√µes
                        classifications = self._stage_06_parse_batch_json(text_content)
                        results[custom_id] = classifications

                    elif result.get('type') in ('errored', 'canceled', 'expired'):
                        results[custom_id] = []
                        self.logger.debug(f"   ‚ö†Ô∏è Request {custom_id}: {result.get('type')}")

                except json.JSONDecodeError:
                    continue

        except requests.RequestException as e:
            self.logger.error(f"   ‚ùå Erro ao buscar resultados: {e}")

        return results

    def _stage_06_parse_batch_json(self, text: str) -> list:
        """
        Parsear JSON de classifica√ß√µes da resposta da API.
        Tenta m√∫ltiplas estrat√©gias de parsing.

        Args:
            text: Texto da resposta da API

        Returns:
            Lista de dicts com categorias e confian√ßa
        """
        import json

        if not text:
            return []

        # Tentativa 1: JSON direto
        try:
            result = json.loads(text)
            if isinstance(result, list):
                return result
        except json.JSONDecodeError:
            pass

        # Tentativa 2: Extrair array JSON do texto
        if '[' in text and ']' in text:
            json_start = text.find('[')
            json_end = text.rfind(']') + 1
            try:
                result = json.loads(text[json_start:json_end])
                if isinstance(result, list):
                    return result
            except json.JSONDecodeError:
                pass

        # Tentativa 3: Extrair objetos JSON individuais
        try:
            objects = []
            import re
            for match in re.finditer(r'\{[^{}]+\}', text):
                try:
                    obj = json.loads(match.group())
                    if 'categorias' in obj:
                        objects.append(obj)
                except json.JSONDecodeError:
                    continue
            if objects:
                return objects
        except Exception:
            pass

        return []

    def _stage_06_affordances_heuristic_fallback(self, df: pd.DataFrame) -> pd.DataFrame:
        """Fallback heur√≠stico para classifica√ß√£o de affordances sem API."""
        self.logger.info("üîÑ Aplicando classifica√ß√£o heur√≠stica de affordances...")

        text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'

        # Padr√µes heur√≠sticos expandidos (15-20 keywords por categoria)
        patterns = {
            'noticia': [
                'aconteceu', 'not√≠cia', 'informa√ß√£o', 'fato', 'governo', 'brasil',
                'reportagem', 'jornal', 'imprensa', 'publicou', 'divulgou', 'segundo',
                'fonte', 'comunicado', 'nota oficial', 'decreto', 'lei', 'medida',
                'aprovado', 'anunciou', 'declarou', 'dados', 'pesquisa', 'estudo'
            ],
            'midia_social': [
                'compartilhem', 'rt', 'retweet', 'curtir', 'like', 'seguir',
                'inscreva', 'canal', 'grupo', 'telegram', 'whatsapp', 'twitter',
                'instagram', 'youtube', 'facebook', 'tiktok', 'sigam', 'divulguem',
                'espalhem', 'repassem'
            ],
            'video_audio_gif': [
                'v√≠deo', 'video', 'audio', '√°udio', 'gif', 'assista', 'ou√ßa',
                'podcast', 'live', 'ao vivo', 'transmiss√£o', 'grava√ß√£o', 'filmou',
                'imagem', 'foto', 'print', 'screenshot', 'clipe', 'document√°rio'
            ],
            'opiniao': [
                'acho', 'penso', 'na minha opini√£o', 'acredito', 'creio',
                'considero', 'entendo', 'parece', 'me parece', 'na verdade',
                'sinceramente', 'francamente', 'obviamente', 'claramente',
                'infelizmente', 'felizmente', 'absurdo', 'rid√≠culo', 'inaceit√°vel'
            ],
            'mobilizacao': [
                'vamos', 'precisamos', 'juntos', 'a√ß√£o', 'mobilizar',
                'protesto', 'manifesta√ß√£o', 'marcha', 'ato', 'convoca√ß√£o',
                'compare√ßam', 'participem', 'lutar', 'resistir', 'unir',
                'levantar', 'defender', 'cobrar', 'exigir', 'pressionar'
            ],
            'ataque': [
                'idiota', 'burro', 'canalha', 'corrupto', 'mentiroso',
                'ladr√£o', 'bandido', 'vagabundo', 'safado', 'lixo',
                'vergonha', 'nojo', 'traidor', 'covarde', 'hip√≥crita',
                'incompetente', 'criminoso', 'fascista', 'comunista', 'genocida'
            ],
            'interacao': [
                '@', 'resposta', 'pergunta', 'd√∫vida', 'respondendo',
                'concordo', 'discordo', 'exatamente', 'isso mesmo',
                'verdade', 'falso', 'correto', 'errado', 'complementando'
            ],
            'is_forwarded': [
                'encaminhado', 'forward', 'repasse', 'compartilhe',
                'repassando', 'recebi', 'me mandaram', 'vejam isso',
                'olha isso', 'leiam', 'importante', 'urgente', 'aten√ß√£o'
            ]
        }

        import re

        def classify_text_heuristic(text):
            """Classifica texto por heur√≠stica com scoring de confian√ßa."""
            text_lower = str(text).lower() if not pd.isna(text) else ''
            if len(text_lower) < 5:
                return [], {}, 0.0

            categories = []
            scores = {}
            total_matches = 0

            for affordance_type, keywords in patterns.items():
                matches = sum(1 for kw in keywords if kw in text_lower)
                scores[affordance_type] = matches
                if matches >= 1:
                    categories.append(affordance_type)
                    total_matches += matches

            # FIX: regex em normalized_text nunca matcheia (: e // s√£o removidos na normaliza√ß√£o)
            # A detec√ß√£o de URL agora √© feita fora do loop, usando 'urls_extracted' do Stage 01
            if re.search(r'https?://', text_lower):
                if 'noticia' not in categories:
                    categories.append('noticia')
                    scores['noticia'] = scores.get('noticia', 0) + 1
                    total_matches += 1

            if text_lower.count('@') >= 2:
                if 'interacao' not in categories:
                    categories.append('interacao')
                    scores['interacao'] = scores.get('interacao', 0) + 2
                    total_matches += 2

            # Confian√ßa baseada no total de matches
            if total_matches == 0:
                confidence = 0.1
            elif total_matches <= 2:
                confidence = 0.4
            elif total_matches <= 4:
                confidence = 0.6
            elif total_matches <= 7:
                confidence = 0.75
            else:
                confidence = 0.85

            return categories, scores, confidence

        # Aplicar classifica√ß√£o vetorizada
        self.logger.info(f"   üìä Classificando {len(df)} mensagens por heur√≠stica expandida...")
        results = df[text_column].apply(classify_text_heuristic)

        # Extrair resultados
        df['affordance_categories'] = results.apply(lambda x: x[0])
        df['_heuristic_scores'] = results.apply(lambda x: x[1])
        df['affordance_confidence'] = results.apply(lambda x: x[2])

        # Colunas bin√°rias por categoria
        affordance_types = ['noticia', 'midia_social', 'video_audio_gif', 'opiniao',
                          'mobilizacao', 'ataque', 'interacao', 'is_forwarded']
        for affordance_type in affordance_types:
            df[f'aff_{affordance_type}'] = df['affordance_categories'].apply(
                lambda cats: 1 if affordance_type in cats else 0
            )

        # FIX: URL detection ‚Äî regex em normalized_text nunca matcheia (://  removido)
        # Usar 'urls_extracted' do Stage 01 (preserva URLs reais do body)
        if 'urls_extracted' in df.columns:
            has_url = df['urls_extracted'].apply(
                lambda x: len(x) > 0 if isinstance(x, list) else bool(x) if x else False
            )
            # Marcar textos com URL como 'noticia' se n√£o classificados
            mask_url_not_noticia = has_url & (df['aff_noticia'] == 0)
            df.loc[mask_url_not_noticia, 'aff_noticia'] = 1
            url_boost_count = mask_url_not_noticia.sum()
            if url_boost_count > 0:
                self.logger.info(f"   üîó URL detection via urls_extracted: +{url_boost_count} classifica√ß√µes 'noticia'")

        # Estat√≠sticas
        classified = len(df[df['affordance_confidence'] > 0.1])
        high_conf = len(df[df['affordance_confidence'] >= 0.6])
        low_conf = len(df[(df['affordance_confidence'] > 0.1) & (df['affordance_confidence'] < 0.6)])

        self.logger.info(f"‚úÖ Classifica√ß√£o heur√≠stica expandida conclu√≠da:")
        self.logger.info(f"   üìä Total classificadas: {classified}/{len(df)} ({classified/len(df)*100:.1f}%)")
        self.logger.info(f"   üü¢ Alta confian√ßa (>=0.6): {high_conf} ({high_conf/len(df)*100:.1f}%)")
        self.logger.info(f"   üü° Baixa confian√ßa (<0.6): {low_conf} ({low_conf/len(df)*100:.1f}%)")

        # Contagem por categoria
        for aff_type in affordance_types:
            count = df[f'aff_{aff_type}'].sum()
            if count > 0:
                self.logger.info(f"   üìå {aff_type}: {count} ({count/len(df)*100:.1f}%)")

        return df

    # ===============================================
    # M√âTODOS HELPER PARA AN√ÅLISE DE QUALIDADE
    # ===============================================
    
    def _calculate_emoji_ratio(self, text: str) -> float:
        """Calcular propor√ß√£o de emojis no texto."""
        if pd.isna(text) or len(text) == 0:
            return 0.0
        
        import re
        # Regex para detectar emojis Unicode
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F1E0-\U0001F1FF"  # flags (iOS)
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "]+",
            flags=re.UNICODE
        )
        
        emojis = emoji_pattern.findall(str(text))
        emoji_count = sum(len(emoji) for emoji in emojis)
        
        return min(1.0, emoji_count / len(str(text)))
    
    def _calculate_caps_ratio(self, text: str) -> float:
        """Calcular propor√ß√£o de letras mai√∫sculas."""
        if pd.isna(text) or len(text) == 0:
            return 0.0
        
        text_str = str(text)
        letters = [c for c in text_str if c.isalpha()]
        
        if len(letters) == 0:
            return 0.0
        
        caps_count = sum(1 for c in letters if c.isupper())
        return caps_count / len(letters)
    
    def _calculate_repetition_ratio(self, text: str) -> float:
        """Calcular propor√ß√£o de caracteres repetitivos."""
        if pd.isna(text) or len(text) <= 1:
            return 0.0
        
        text_str = str(text).lower()
        
        # Contar sequ√™ncias repetitivas (3+ caracteres iguais)
        repetition_count = 0
        current_char = ''
        current_count = 1
        
        for char in text_str:
            if char == current_char:
                current_count += 1
                if current_count >= 3:
                    repetition_count += 1
            else:
                current_char = char
                current_count = 1
        
        return min(1.0, repetition_count / len(text_str))
    
    def _detect_portuguese(self, text: str) -> bool:
        """Detec√ß√£o b√°sica de idioma portugu√™s."""
        if pd.isna(text) or len(text) < 10:
            return True  # Assumir portugu√™s para textos muito curtos
        
        text_lower = str(text).lower()
        
        # Palavras comuns em portugu√™s
        portuguese_indicators = [
            'que', 'n√£o', 'com', 'uma', 'para', 's√£o', 'por', 'mais', 'das', 'dos',
            'mas', 'foi', 'pela', 'at√©', 'isso', 'ela', 'entre', 'depois', 'sem',
            'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'voc√™',
            'j√°', 'eu', 'tamb√©m', 's√≥', 'pelo', 'nos', '√©', 'o', 'a', 'de', 'do',
            'da', 'em', 'um', 'para', '√©', 'com', 'n√£o', 'uma', 'os', 'no', 'se',
            'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'foi', 'ao', 'ele',
            'das', 'tem', '√†', 'seu', 'sua', 'ou', 'ser', 'quando', 'muito', 'h√°',
            'nos', 'j√°', 'est√°', 'eu', 'tamb√©m', 's√≥', 'pelo', 'pela', 'at√©'
        ]
        
        # Contar palavras portuguesas encontradas
        words = text_lower.split()
        portuguese_count = sum(1 for word in words if word in portuguese_indicators)
        
        # Considerar portugu√™s se >= 20% das palavras s√£o indicadores
        if len(words) > 0:
            portuguese_ratio = portuguese_count / len(words)
            return portuguese_ratio >= 0.2
        
        return True  # Default para portugu√™s

    @validate_stage_dependencies(required_columns=['normalized_text'], required_attrs=['political_lexicon'])
    def _stage_08_political_classification(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 08: Classifica√ß√£o pol√≠tica brasileira.

        REFORMULADO: usa spacy_lemmas (Stage 07) com token matching via set lookup.
        Aplica l√©xico unificado (914+ termos, 11 macrotemas) para classificar textos.
        Escopo: discurso bolsonarista/direita brasileira (2019-2023).
        Orienta√ß√µes retornadas: extrema-direita, direita, centro-direita, neutral.
        Nota: l√©xico n√£o inclui termos de esquerda (fora do escopo do projeto).
        """
        try:
            self.logger.info("üîÑ Stage 08: Classifica√ß√£o pol√≠tica brasileira (token matching via spaCy lemmas)")

            # Determinar coluna de input: preferir spacy_lemmas > lemmatized_text > normalized_text
            if 'spacy_lemmas' in df.columns:
                input_col = 'spacy_lemmas'
                self.logger.info("   üì• Input: spacy_lemmas (token-level matching)")
            elif 'lemmatized_text' in df.columns:
                input_col = 'lemmatized_text'
                self.logger.info("   üì• Input: lemmatized_text (string fallback)")
            else:
                input_col = 'normalized_text' if 'normalized_text' in df.columns else 'body'
                self.logger.warning(f"   ‚ö†Ô∏è spaCy output n√£o dispon√≠vel, fallback: {input_col}")

            # Classifica√ß√£o pol√≠tica usando l√©xico unificado
            df['political_orientation'] = df[input_col].apply(self._classify_political_orientation)
            df['political_keywords'] = df[input_col].apply(self._extract_political_keywords)
            df['political_intensity'] = df[input_col].apply(self._calculate_political_intensity)

            # Classifica√ß√£o tem√°tica - 12 categorias (political_keywords_dict.py)
            try:
                from src.core.political_keywords_dict import POLITICAL_KEYWORDS
                import re as _re

                for cat_name, cat_terms in POLITICAL_KEYWORDS.items():
                    col_name = 'cat_' + _re.sub(r'^cat\d+_', '', cat_name)
                    # Token matching: set intersection para single-word, substring para multi-word
                    cat_single = set(t for t in cat_terms if ' ' not in t)
                    cat_multi = [t for t in cat_terms if ' ' in t]

                    def _count_cat_matches(lemmas_or_text, s=cat_single, m=cat_multi):
                        if lemmas_or_text is None or (isinstance(lemmas_or_text, float) and pd.isna(lemmas_or_text)):
                            return 0
                        if isinstance(lemmas_or_text, list):
                            tset = set(t.lower() for t in lemmas_or_text if t)
                        else:
                            tset = set(str(lemmas_or_text).lower().split())
                        count = len(tset & s)
                        if m:
                            joined = ' '.join(sorted(tset))
                            count += sum(1 for t in m if t in joined)
                        return count

                    df[col_name] = df[input_col].apply(_count_cat_matches)

                self.logger.info(f"üìä Classifica√ß√£o tem√°tica: {len(POLITICAL_KEYWORDS)} categorias aplicadas (token matching)")
            except ImportError:
                self.logger.warning("‚ö†Ô∏è political_keywords_dict.py n√£o encontrado, pulando categorias tem√°ticas")

            # === CODIFICA√á√ÉO TCW (Tabela-Categoria-Palavra) ===
            # Integrado do classificador TCW: adiciona c√≥digos num√©ricos 3-d√≠gitos
            # e grau de concord√¢ncia entre as 3 tabelas LLM
            try:
                import json as _json
                from pathlib import Path as _Path

                tcw_path = _Path(__file__).parent / 'core' / 'tcw_codes.json'
                if tcw_path.exists():
                    with open(tcw_path, 'r', encoding='utf-8') as f:
                        tcw_codes = _json.load(f)

                    # Construir lookup: word ‚Üí list of codes
                    word_to_codes = {}
                    for code, info in tcw_codes.items():
                        word = info['word'].lower()
                        if word not in word_to_codes:
                            word_to_codes[word] = []
                        word_to_codes[word].append({
                            'code': code,
                            'table': info['table'],
                            'category': info['category'],
                            'category_name': info['category_name']
                        })

                    # Single-word e multi-word lookup sets
                    tcw_single = set(w for w in word_to_codes if ' ' not in w)
                    tcw_multi = [w for w in word_to_codes if ' ' in w]

                    def _tcw_classify(lemmas_or_text):
                        """Classificar texto usando codifica√ß√£o TCW."""
                        if lemmas_or_text is None or (isinstance(lemmas_or_text, float) and pd.isna(lemmas_or_text)):
                            return [], [], 0.0
                        if isinstance(lemmas_or_text, list):
                            tset = set(t.lower() for t in lemmas_or_text if t)
                        else:
                            tset = set(str(lemmas_or_text).lower().split())

                        codes_found = []
                        categories_found = set()

                        # Single-word matches
                        for word in tset & tcw_single:
                            for entry in word_to_codes[word]:
                                codes_found.append(entry['code'])
                                categories_found.add(entry['category_name'])

                        # Multi-word matches
                        if tcw_multi:
                            joined = ' '.join(sorted(tset))
                            for mw in tcw_multi:
                                if mw in joined:
                                    for entry in word_to_codes[mw]:
                                        codes_found.append(entry['code'])
                                        categories_found.add(entry['category_name'])

                        # Concord√¢ncia: quantas tabelas (1-3) concordam nos c√≥digos encontrados
                        if codes_found:
                            tables_seen = set()
                            for code in codes_found:
                                if code in tcw_codes:
                                    tables_seen.add(tcw_codes[code]['table'])
                            agreement = len(tables_seen) / 3.0  # 0.33, 0.67, 1.0
                        else:
                            agreement = 0.0

                        return codes_found, list(categories_found), agreement

                    tcw_results = df[input_col].apply(_tcw_classify)
                    df['tcw_codes'] = tcw_results.apply(lambda x: x[0])
                    df['tcw_categories'] = tcw_results.apply(lambda x: x[1])
                    df['tcw_agreement'] = tcw_results.apply(lambda x: x[2])
                    df['tcw_code_count'] = df['tcw_codes'].apply(len)

                    tcw_classified = (df['tcw_code_count'] > 0).sum()
                    self.logger.info(f"üî¢ TCW: {tcw_classified}/{len(df)} textos classificados ({tcw_classified/len(df)*100:.1f}%)")
                else:
                    self.logger.warning("‚ö†Ô∏è tcw_codes.json n√£o encontrado em src/core/, pulando TCW")
            except Exception as tcw_err:
                self.logger.warning(f"‚ö†Ô∏è Erro TCW: {tcw_err}")

            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 19  # 15 base + 4 TCW

            self.logger.info(f"‚úÖ Stage 08 conclu√≠do: {len(df)} registros processados")
            return df

        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 08: {e}")
            self.stats['processing_errors'] += 1
            return df

    def _stage_09_tfidf_vectorization(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 09: Vetoriza√ß√£o TF-IDF com tokens spaCy.
        
        Calcula TF-IDF usando tokens processados pelo spaCy.
        Trata casos de vocabul√°rio vazio em chunks pequenos.
        """
        try:
            self.logger.info("üîÑ Stage 09: Vetoriza√ß√£o TF-IDF")
            
            # Verificar se h√° dados suficientes
            if len(df) < 2:
                self.logger.warning(f"‚ö†Ô∏è Dados insuficientes para TF-IDF ({len(df)} documentos), preenchendo com padr√µes")
                df['tfidf_score_mean'] = 0.0
                df['tfidf_score_max'] = 0.0
                df['tfidf_top_terms'] = [[] for _ in range(len(df))]
                self.stats['features_extracted'] += 3
                return df
            
            # FIX: usar 'lemmatized_text' (output do Stage 07 spaCy) em vez de 'tokens' (inexistente)
            if 'lemmatized_text' in df.columns:
                text_data = df['lemmatized_text'].fillna('').tolist()
            elif 'spacy_tokens' in df.columns:
                text_data = df['spacy_tokens'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x)).fillna('').tolist()
            else:
                self.logger.warning("‚ö†Ô∏è lemmatized_text/spacy_tokens n√£o encontrados, usando normalized_text")
                text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
                text_data = df[text_column].fillna('').tolist()
            
            # Verificar se h√° texto n√£o-vazio
            non_empty_texts = [text for text in text_data if text.strip()]
            if len(non_empty_texts) < 2:
                self.logger.warning(f"‚ö†Ô∏è Textos vazios demais para TF-IDF ({len(non_empty_texts)} v√°lidos), usando fallback")
                df['tfidf_score_mean'] = 0.1
                df['tfidf_score_max'] = 0.2
                df['tfidf_top_terms'] = [['texto', 'palavra'] for _ in range(len(df))]
                self.stats['features_extracted'] += 3
                return df
            
            # TF-IDF com configura√ß√£o adaptativa para chunks pequenos
            from sklearn.feature_extraction.text import TfidfVectorizer
            import numpy as np
            
            # Ajustar max_features baseado no tamanho do chunk
            chunk_size = len(df)
            if chunk_size < 50:
                max_features = min(20, chunk_size * 2)  # Muito conservador
            elif chunk_size < 200:
                max_features = min(50, chunk_size)  # Conservador
            else:
                max_features = 100  # Padr√£o
            
            vectorizer = TfidfVectorizer(
                max_features=max_features,
                min_df=1,  # Aceitar termos que aparecem pelo menos 1 vez
                stop_words=None,  # J√° removemos stopwords no spaCy
                lowercase=False,   # J√° normalizado
                token_pattern=r'\S+',  # Aceitar qualquer token n√£o-espa√ßo
                ngram_range=(1, 1)  # Apenas unigramas para chunks pequenos
            )
            
            try:
                tfidf_matrix = vectorizer.fit_transform(text_data)
                feature_names = vectorizer.get_feature_names_out()
                
                # Verificar se conseguiu gerar features
                if tfidf_matrix.shape[1] == 0:
                    raise ValueError("Vocabul√°rio vazio ap√≥s vectoriza√ß√£o")
                
                # Converter para array denso para c√°lculos
                tfidf_dense = tfidf_matrix.toarray()
                
                # Scores m√©dios por documento
                df['tfidf_score_mean'] = np.mean(tfidf_dense, axis=1)
                df['tfidf_score_max'] = np.max(tfidf_dense, axis=1)
                
                # Top terms por documento 
                top_terms_count = min(5, len(feature_names))  # Adaptar ao vocabul√°rio dispon√≠vel
                df['tfidf_top_terms'] = [
                    [feature_names[i] for i in row.argsort()[::-1][:top_terms_count] if row[i] > 0]
                    for row in tfidf_dense
                ]
                
                self.logger.info(f"‚úÖ TF-IDF: {len(feature_names)} features, max_features={max_features}")
                
            except (ValueError, Exception) as ve:
                self.logger.warning(f"‚ö†Ô∏è Erro na vectoriza√ß√£o TF-IDF: {ve}, usando fallback simples")
                
                # Fallback: an√°lise simples baseada em frequ√™ncia de palavras
                import re
                from collections import Counter
                
                all_words = []
                for text in text_data:
                    if text and text.strip():
                        # Extrair palavras simples
                        words = re.findall(r'\w+', text.lower())
                        words = [w for w in words if len(w) > 2]  # Filtrar palavras muito curtas
                        all_words.extend(words)
                
                if all_words:
                    word_freq = Counter(all_words)
                    common_words = [word for word, _ in word_freq.most_common(10)]
                    
                    # Scores baseados em presen√ßa de palavras comuns
                    df['tfidf_score_mean'] = [
                        len([w for w in re.findall(r'\w+', str(text).lower()) if w in common_words]) / max(1, len(common_words)) * 0.5
                        for text in text_data
                    ]
                    df['tfidf_score_max'] = df['tfidf_score_mean'] * 1.5
                    df['tfidf_top_terms'] = [
                        [w for w in re.findall(r'\w+', str(text).lower()) if w in common_words][:5]
                        for text in text_data
                    ]
                else:
                    # √öltima op√ß√£o: valores padr√£o
                    df['tfidf_score_mean'] = 0.1
                    df['tfidf_score_max'] = 0.2
                    df['tfidf_top_terms'] = [[] for _ in range(len(df))]
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 3
            
            self.logger.info(f"‚úÖ Stage 09 conclu√≠do: {len(df)} registros processados")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 09: {e}")
            self.stats['processing_errors'] += 1
            
            # Valores padr√£o em caso de erro
            df['tfidf_score_mean'] = 0.0
            df['tfidf_score_max'] = 0.0
            df['tfidf_top_terms'] = [[] for _ in range(len(df))]
            self.stats['features_extracted'] += 3
            return df

    @validate_stage_dependencies(required_columns=['tfidf_score_mean'], required_attrs=['tfidf_matrix'])
    def _stage_10_clustering_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 10: An√°lise de clustering baseado em features lingu√≠sticas.

        Agrupa documentos similares usando caracter√≠sticas extra√≠das.
        """
        try:
            self.logger.info("üîÑ Stage 10: An√°lise de clustering")
            
            # Features num√©ricas para clustering
            numeric_features = []
            # FIX: 'text_length' n√£o existe ‚Äî Stage 04 gera 'char_count'
            for col in ['word_count', 'char_count', 'tfidf_score_mean', 'political_intensity']:
                if col in df.columns:
                    numeric_features.append(col)
            
            if len(numeric_features) < 2:
                self.logger.warning("‚ö†Ô∏è Features insuficientes para clustering")
                df['cluster_id'] = 0
                df['cluster_distance'] = 0.0
                df['cluster_size'] = len(df)
            else:
                from sklearn.preprocessing import StandardScaler

                # Preparar dados
                feature_data = df[numeric_features].fillna(0)
                scaler = StandardScaler()
                scaled_data = scaler.fit_transform(feature_data)

                # Tentar HDBSCAN (instalado no pyproject.toml, auto-detec√ß√£o de k)
                try:
                    import hdbscan
                    clusterer = hdbscan.HDBSCAN(
                        min_cluster_size=max(5, len(df) // 50),
                        min_samples=3,
                        metric='euclidean'
                    )
                    clusters = clusterer.fit_predict(scaled_data)
                    # HDBSCAN retorna -1 para noise
                    df['cluster_id'] = clusters
                    df['cluster_distance'] = 1.0 - clusterer.probabilities_  # prob ‚Üí dist√¢ncia
                    n_found = len(set(clusters) - {-1})
                    n_noise = (clusters == -1).sum()
                    self.logger.info(f"üìä HDBSCAN: {n_found} clusters, {n_noise} noise points")

                except (ImportError, Exception) as e:
                    # Fallback para K-Means
                    self.logger.warning(f"‚ö†Ô∏è HDBSCAN indispon√≠vel ({e}), usando KMeans fallback")
                    from sklearn.cluster import KMeans
                    n_clusters = min(5, len(df) // 10 + 1)
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                    clusters = kmeans.fit_predict(scaled_data)
                    df['cluster_id'] = clusters
                    df['cluster_distance'] = [
                        min(((scaled_data[i] - center) ** 2).sum() for center in kmeans.cluster_centers_)
                        for i in range(len(scaled_data))
                    ]

                # Tamanho dos clusters
                cluster_sizes = pd.Series(df['cluster_id']).value_counts().to_dict()
                df['cluster_size'] = df['cluster_id'].map(cluster_sizes)
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 3
            
            self.logger.info(f"‚úÖ Stage 10 conclu√≠do: {len(df)} registros processados")
            return df

        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 10: {e}")
            self.stats['processing_errors'] += 1
            return df

    def _stage_11_topic_modeling(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 11: Topic modeling com LDA.

        Descoberta autom√°tica de t√≥picos nos textos.
        """
        try:
            self.logger.info("üîÑ Stage 11: Topic modeling")
            
            # FIX: usar 'lemmatized_text' (output do Stage 07 spaCy) em vez de 'tokens' (inexistente)
            if 'lemmatized_text' in df.columns:
                text_data = df['lemmatized_text'].fillna('').tolist()
            elif 'spacy_tokens' in df.columns:
                text_data = df['spacy_tokens'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x)).fillna('').tolist()
            else:
                self.logger.warning("‚ö†Ô∏è lemmatized_text/spacy_tokens n√£o encontrados, usando normalized_text")
                text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
                text_data = df[text_column].fillna('').tolist()
            
            # Topic modeling b√°sico com LDA
            from sklearn.feature_extraction.text import CountVectorizer
            from sklearn.decomposition import LatentDirichletAllocation
            
            # Stopwords PT (termos funcionais que poluem LDA)
            pt_stopwords = [
                'de', 'da', 'do', 'das', 'dos', 'em', 'no', 'na', 'nos', 'nas',
                'um', 'uma', 'uns', 'umas', 'por', 'para', 'com', 'sem', 'sob',
                'que', 'se', 'n√£o', 'mais', 'muito', 'como', 'mas', 'ou', 'j√°',
                'tamb√©m', 's√≥', 'seu', 'sua', 'seus', 'suas', 'ele', 'ela', 'eles',
                'elas', 'isso', 'isto', 'esse', 'essa', 'este', 'esta', 'aqui',
                'ali', 'l√°', 'ao', 'aos', '√†', '√†s', 'pelo', 'pela', 'pelos', 'pelas',
                'entre', 'sobre', 'ap√≥s', 'at√©', 'quando', 'onde', 'quem', 'qual',
                'foi', 'ser', 'ter', 'est√°', 's√£o', 'tem', 'era', 'vai', 'pode',
                'nos', 'me', 'te', 'lhe', 'o', 'a', 'os', 'as', 'e', '√©',
                'eu', 'tu', 'n√≥s', 'v√≥s', 'meu', 'minha', 'teu', 'tua',
                'nosso', 'nossa', 'nossos', 'nossas', 'todo', 'toda', 'todos', 'todas',
                'outro', 'outra', 'outros', 'outras', 'mesmo', 'mesma', 'cada',
                'ainda', 'ent√£o', 'depois', 'antes', 'bem', 'agora', 'sempre',
                'nunca', 'nada', 'tudo', 'algo', 'assim', 'aquele', 'aquela',
                'http', 'https', 'www', 'com', 'org', 'br', 'the', 'and', 'for'
            ]

            # Preparar dados (com remo√ß√£o de stopwords PT)
            vectorizer = CountVectorizer(max_features=50, stop_words=pt_stopwords)
            doc_term_matrix = vectorizer.fit_transform(text_data)
            
            # LDA simples
            n_topics = min(5, len(df) // 20 + 1)
            lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
            doc_topic_matrix = lda.fit_transform(doc_term_matrix)
            
            # T√≥pico dominante para cada documento
            df['dominant_topic'] = doc_topic_matrix.argmax(axis=1)
            df['topic_probability'] = doc_topic_matrix.max(axis=1)
            
            # Palavras-chave dos t√≥picos
            feature_names = vectorizer.get_feature_names_out()
            topic_keywords = []
            for topic_idx, topic in enumerate(lda.components_):
                top_words = [feature_names[i] for i in topic.argsort()[::-1][:3]]
                topic_keywords.append(top_words)
            
            df['topic_keywords'] = df['dominant_topic'].apply(lambda x: topic_keywords[x] if x < len(topic_keywords) else [])
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 3
            
            self.logger.info(f"‚úÖ Stage 11 conclu√≠do: {len(df)} registros processados")
            return df

        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 11: {e}")
            self.stats['processing_errors'] += 1
            return df

    @validate_stage_dependencies(required_columns=['normalized_text'])
    def _stage_13_temporal_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 13: An√°lise temporal.

        Extrai padr√µes temporais dos timestamps.
        """
        try:
            self.logger.info("üîÑ Stage 13: An√°lise temporal")
            
            if 'datetime' not in df.columns:
                self.logger.warning("‚ö†Ô∏è datetime n√£o encontrado")
                df['hour'] = 12
                df['day_of_week'] = 1
                df['month'] = 1
            else:
                # Converter datetime para an√°lise temporal
                try:
                    datetime_series = pd.to_datetime(df['datetime'], format='%d/%m/%Y %H:%M:%S', errors='coerce')
                    
                    df['hour'] = datetime_series.dt.hour
                    df['day_of_week'] = datetime_series.dt.dayofweek
                    df['month'] = datetime_series.dt.month
                    df['year'] = datetime_series.dt.year
                    df['day_of_year'] = datetime_series.dt.dayofyear
                    
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro convers√£o datetime: {e}")
                    df['hour'] = 12
                    df['day_of_week'] = 1
                    df['month'] = 1
                    df['year'] = 2020
                    df['day_of_year'] = 1
            
            # Burst Detection - Kleinberg (2003), KDD
            # Detecta dias com volume anormal de mensagens
            df['is_burst_day'] = False
            if 'datetime' in df.columns:
                try:
                    dt_series = pd.to_datetime(df['datetime'], format='%d/%m/%Y %H:%M:%S', errors='coerce')
                    dates = dt_series.dt.date
                    daily_counts = dates.value_counts()
                    if len(daily_counts) >= 3:
                        mean_count = daily_counts.mean()
                        std_count = daily_counts.std()
                        burst_threshold = mean_count + 2 * std_count  # 2 desvios padr√£o
                        burst_dates = daily_counts[daily_counts > burst_threshold].index.tolist()
                        df['is_burst_day'] = dates.isin(burst_dates)
                        if burst_dates:
                            self.logger.info(f"üìà Burst detection: {len(burst_dates)} dias com volume anormal")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Burst detection: {e}")

            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 6

            self.logger.info(f"‚úÖ Stage 13 conclu√≠do: {len(df)} registros processados")
            return df

        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 13: {e}")
            self.stats['processing_errors'] += 1
            return df

    def _stage_14_network_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 14: An√°lise de rede (coordena√ß√£o e padr√µes).

        Detecta padr√µes de coordena√ß√£o e comportamento de rede.
        """
        try:
            self.logger.info("üîÑ Stage 14: An√°lise de rede")
            
            # An√°lise de coordena√ß√£o b√°sica
            if 'sender' in df.columns:
                sender_counts = df['sender'].value_counts()
                df['sender_frequency'] = df['sender'].map(sender_counts)
                df['is_frequent_sender'] = df['sender_frequency'] > df['sender_frequency'].median()
            else:
                df['sender_frequency'] = 1
                df['is_frequent_sender'] = False
            
            # An√°lise de URLs compartilhadas
            if 'urls_extracted' in df.columns:
                # URLs mais compartilhadas
                all_urls = []
                for urls in df['urls_extracted'].fillna('[]'):
                    if isinstance(urls, str):
                        try:
                            url_list = eval(urls) if urls.startswith('[') else [urls]
                            all_urls.extend(url_list)
                        except:
                            pass
                
                url_counts = pd.Series(all_urls).value_counts()
                df['shared_url_frequency'] = df['urls_extracted'].apply(
                    lambda x: max([url_counts.get(url, 0) for url in (eval(x) if isinstance(x, str) and x.startswith('[') else [])], default=0)
                )
            else:
                df['shared_url_frequency'] = 0
            
            # Coordena√ß√£o temporal (mensagens em hor√°rios similares)
            if 'hour' in df.columns:
                hour_counts = df['hour'].value_counts()
                df['temporal_coordination'] = df['hour'].map(hour_counts) / len(df)
            else:
                df['temporal_coordination'] = 0.0
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 4
            
            self.logger.info(f"‚úÖ Stage 14 conclu√≠do: {len(df)} registros processados")
            return df

        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 14: {e}")
            self.stats['processing_errors'] += 1
            return df

    def _stage_15_domain_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 15: An√°lise de dom√≠nios.

        Analisa dom√≠nios e URLs para identificar padr√µes de m√≠dia.
        """
        try:
            self.logger.info("üîÑ Stage 15: An√°lise de dom√≠nios")
            
            # An√°lise de dom√≠nios com trust score (Page et al. 1999, adaptado)
            if 'domain' in df.columns:
                df['domain_type'] = df['domain'].apply(self._classify_domain_type)
                df['domain_trust_score'] = df['domain'].apply(self._calculate_domain_trust_score)

                domain_counts = df['domain'].value_counts()
                df['domain_frequency'] = df['domain'].map(domain_counts)

                # M√≠dia mainstream vs alternativa (baseado em domain_type classificado)
                mainstream_types = ['mainstream_news', 'government']
                df['is_mainstream_media'] = df['domain_type'].isin(mainstream_types)
            else:
                df['domain_type'] = 'unknown'
                df['domain_trust_score'] = 0.0
                df['domain_frequency'] = 0
                df['is_mainstream_media'] = False
            
            # An√°lise de URLs
            if 'urls_extracted' in df.columns:
                df['url_count'] = df['urls_extracted'].apply(
                    lambda x: len(eval(x)) if isinstance(x, str) and x.startswith('[') else (1 if x else 0)
                )
                df['has_external_links'] = df['url_count'] > 0
            else:
                df['url_count'] = 0
                df['has_external_links'] = False
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 5
            
            self.logger.info(f"‚úÖ Stage 15 conclu√≠do: {len(df)} registros processados")
            return df

        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 15: {e}")
            self.stats['processing_errors'] += 1
            return df


    def _stage_12_semantic_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 12: An√°lise sem√¢ntica.
        
        An√°lise sem√¢ntica e de sentimento dos textos.
        """
        try:
            self.logger.info("üîÑ Stage 12: An√°lise sem√¢ntica")
            
            text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
            
            # An√°lise de sentimento b√°sica
            df['sentiment_polarity'] = df[text_column].apply(self._calculate_sentiment_polarity)
            df['sentiment_label'] = df['sentiment_polarity'].apply(
                lambda x: 'positive' if x > 0.1 else ('negative' if x < -0.1 else 'neutral')
            )
            
            # An√°lise de emo√ß√µes b√°sicas (usar body original para detectar !, ?, CAPS)
            raw_col = 'body' if 'body' in df.columns else text_column
            df['emotion_intensity'] = df.apply(
                lambda row: self._calculate_emotion_intensity(
                    str(row.get(text_column, '')),
                    raw_text=str(row.get(raw_col, ''))
                ), axis=1
            )
            df['has_aggressive_language'] = df[text_column].apply(self._detect_aggressive_language)
            
            # Complexidade sem√¢ntica
            # FIX: usar 'spacy_tokens' (output real do Stage 07) em vez de 'tokens' (inexistente)
            if 'spacy_tokens' in df.columns:
                df['semantic_diversity'] = df['spacy_tokens'].apply(
                    lambda x: len(set(x)) / len(x) if isinstance(x, list) and len(x) > 0 else 0
                )
            else:
                df['semantic_diversity'] = df[text_column].apply(
                    lambda x: len(set(str(x).split())) / len(str(x).split()) if len(str(x).split()) > 0 else 0
                )
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 5
            
            self.logger.info(f"‚úÖ Stage 12 conclu√≠do: {len(df)} registros processados")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 12: {e}")
            self.stats['processing_errors'] += 1
            return df

    def _stage_16_event_context(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 16: An√°lise de contexto de eventos.

        Detecta contextos pol√≠ticos e eventos relevantes.
        """
        try:
            self.logger.info("üîÑ Stage 16: An√°lise de contexto de eventos")

            # FIX: preferir lemmatized_text (output do spaCy) para melhor matching de contextos
            if 'lemmatized_text' in df.columns:
                text_column = 'lemmatized_text'
            elif 'normalized_text' in df.columns:
                text_column = 'normalized_text'
            else:
                text_column = 'body'
            
            # Contextos pol√≠ticos brasileiros
            df['political_context'] = df[text_column].apply(self._detect_political_context)
            df['mentions_government'] = df[text_column].apply(self._mentions_government)
            df['mentions_opposition'] = df[text_column].apply(self._mentions_opposition)
            
            # Eventos espec√≠ficos (elei√ß√µes, manifesta√ß√µes, etc.)
            df['election_context'] = df[text_column].apply(self._detect_election_context)
            df['protest_context'] = df[text_column].apply(self._detect_protest_context)
            
            # Frame Analysis - Entman (1993), J Communication 43(4): 51-58
            frame_results = df[text_column].apply(self._analyze_political_frames)
            df['frame_conflito'] = frame_results.apply(lambda x: x.get('conflito', 0.0))
            df['frame_responsabilizacao'] = frame_results.apply(lambda x: x.get('responsabilizacao', 0.0))
            df['frame_moralista'] = frame_results.apply(lambda x: x.get('moralista', 0.0))
            df['frame_economico'] = frame_results.apply(lambda x: x.get('economico', 0.0))

            # An√°lise temporal de eventos
            if 'datetime' in df.columns:
                df['is_weekend'] = df['day_of_week'].isin([5, 6]) if 'day_of_week' in df.columns else False
                df['is_business_hours'] = df['hour'].between(9, 17) if 'hour' in df.columns else False
            else:
                df['is_weekend'] = False
                df['is_business_hours'] = True

            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 11

            self.logger.info(f"‚úÖ Stage 16 conclu√≠do: {len(df)} registros, 4 frames Entman extra√≠dos")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 16: {e}")
            self.stats['processing_errors'] += 1
            return df

    def _stage_17_channel_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 17: An√°lise de canais/fontes.

        Classifica canais e fontes de informa√ß√£o.
        """
        try:
            self.logger.info("üîÑ Stage 17: An√°lise de canais")
            
            # An√°lise de canais
            if 'channel' in df.columns:
                df['channel_type'] = df['channel'].apply(self._classify_channel_type)
                
                channel_counts = df['channel'].value_counts()
                df['channel_activity'] = df['channel'].map(channel_counts)
                df['is_active_channel'] = df['channel_activity'] > df['channel_activity'].median()
            else:
                df['channel_type'] = 'unknown'
                df['channel_activity'] = 1
                df['is_active_channel'] = False
            
            # An√°lise de m√≠dia
            if 'media_type' in df.columns:
                df['content_type'] = df['media_type'].fillna('text')
                df['has_media'] = df['media_type'].notna()
            else:
                df['content_type'] = 'text'
                df['has_media'] = False
            
            # Padr√µes de forwarding
            if 'is_fwrd' in df.columns:
                df['is_forwarded'] = df['is_fwrd'].fillna(False)
                forwarded_ratio = df['is_forwarded'].mean()
                df['forwarding_context'] = forwarded_ratio
            else:
                df['is_forwarded'] = False
                df['forwarding_context'] = 0.0
            
            # Influ√™ncia do canal
            if 'sender' in df.columns and 'channel' in df.columns:
                sender_channel_counts = df.groupby(['sender', 'channel']).size()
                df['sender_channel_influence'] = df.apply(
                    lambda row: sender_channel_counts.get((row['sender'], row['channel']), 0), axis=1
                )
            else:
                df['sender_channel_influence'] = 1
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 7
            
            self.logger.info(f"‚úÖ Stage 17 conclu√≠do: {len(df)} registros processados")
            return df

        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 17: {e}")
            self.stats['processing_errors'] += 1
            return df

    # ==========================================
    # HELPER METHODS FOR ANALYSIS STAGES
    # (Integrado com lexico_unified_system.json: 956 termos, 9 macrotemas)
    # ==========================================

    def _classify_political_orientation(self, lemmas_or_text) -> str:
        """
        Classifica orienta√ß√£o pol√≠tica usando l√©xico unificado.
        REFORMULADO: aceita lista de lemmas (spaCy) ou string (fallback).
        Token matching via set lookup ‚Äî O(1) por token, zero falsos positivos.
        """
        if lemmas_or_text is None or (isinstance(lemmas_or_text, float) and pd.isna(lemmas_or_text)):
            return 'neutral'

        # Converter input para set de tokens (lemmas ou palavras)
        if isinstance(lemmas_or_text, list):
            token_set = set(t.lower() for t in lemmas_or_text if t)
        else:
            token_set = set(str(lemmas_or_text).lower().split())

        if not token_set:
            return 'neutral'

        terms_map = self._political_terms_map

        # Macrotemas de direita/extrema-direita
        direita_categories = [
            'identidade_patriotica', 'inimigos_ideologicos', 'teorias_conspiracao',
            'negacionismo', 'autoritarismo_violencia', 'mobilizacao_acao',
            'desinformacao_verdade', 'estrategias_discursivas', 'eventos_simbolicos',
            'corrupcao_transparencia', 'politica_externa'
        ]

        # Contar matches por macrotema via set intersection
        scores = {}
        for cat in direita_categories:
            terms = terms_map.get(cat, [])
            # Para termos compostos (multi-word), verificar no texto concatenado
            single_word_terms = set(t for t in terms if ' ' not in t)
            multi_word_terms = [t for t in terms if ' ' in t]

            count = len(token_set & single_word_terms)
            if multi_word_terms:
                text_joined = ' '.join(sorted(token_set))
                count += sum(1 for t in multi_word_terms if t in text_joined)
            scores[cat] = count

        total_matches = sum(scores.values())
        if total_matches == 0:
            return 'neutral'

        radical_score = scores.get('autoritarismo_violencia', 0) + scores.get('mobilizacao_acao', 0)
        conspiracao_score = scores.get('teorias_conspiracao', 0) + scores.get('negacionismo', 0)
        identidade_score = scores.get('identidade_patriotica', 0) + scores.get('eventos_simbolicos', 0)
        adversario_score = scores.get('inimigos_ideologicos', 0)

        if radical_score >= 2 or (conspiracao_score >= 2 and adversario_score >= 1):
            return 'extrema-direita'
        elif adversario_score >= 2 or conspiracao_score >= 2:
            return 'direita'
        elif identidade_score >= 2:
            return 'centro-direita'
        elif total_matches >= 1:
            return 'direita'
        return 'neutral'

    def _extract_political_keywords(self, lemmas_or_text) -> list:
        """
        Extrai palavras-chave pol√≠ticas usando l√©xico unificado.
        REFORMULADO: token matching via set intersection.
        """
        if lemmas_or_text is None or (isinstance(lemmas_or_text, float) and pd.isna(lemmas_or_text)):
            return []

        if isinstance(lemmas_or_text, list):
            token_set = set(t.lower() for t in lemmas_or_text if t)
        else:
            token_set = set(str(lemmas_or_text).lower().split())

        if not token_set:
            return []

        terms_map = self._political_terms_map
        found = []
        for cat, terms in terms_map.items():
            single_word_terms = set(t for t in terms if ' ' not in t)
            matches = token_set & single_word_terms
            for m in matches:
                if m not in found:
                    found.append(m)
                    if len(found) >= 10:
                        return found
            # Multi-word terms: fallback substring
            multi_word_terms = [t for t in terms if ' ' in t]
            if multi_word_terms:
                text_joined = ' '.join(sorted(token_set))
                for t in multi_word_terms:
                    if t in text_joined and t not in found:
                        found.append(t)
                        if len(found) >= 10:
                            return found
        return found

    def _calculate_political_intensity(self, lemmas_or_text) -> float:
        """
        Calcula intensidade usando termos de mobiliza√ß√£o e autoritarismo.
        REFORMULADO: token matching via set intersection.
        """
        if lemmas_or_text is None or (isinstance(lemmas_or_text, float) and pd.isna(lemmas_or_text)):
            return 0.0

        if isinstance(lemmas_or_text, list):
            token_set = set(t.lower() for t in lemmas_or_text if t)
        else:
            token_set = set(str(lemmas_or_text).lower().split())

        if not token_set:
            return 0.0

        terms_map = self._political_terms_map

        # Termos de alta intensidade
        intensity_terms = (
            terms_map.get('mobilizacao_acao', []) +
            terms_map.get('autoritarismo_violencia', []) +
            terms_map.get('desinformacao_verdade', [])
        )

        if not intensity_terms:
            return 0.0

        single_word = set(t for t in intensity_terms if ' ' not in t)
        match_count = len(token_set & single_word)

        # Multi-word fallback
        multi_word = [t for t in intensity_terms if ' ' in t]
        if multi_word:
            text_joined = ' '.join(sorted(token_set))
            match_count += sum(1 for t in multi_word if t in text_joined)

        return min(match_count * 0.15, 1.0)

    def _classify_domain_type(self, domain: str) -> str:
        """Classifica tipo de dom√≠nio com categorias expandidas."""
        if not domain or pd.isna(domain):
            return 'unknown'

        domain_lower = str(domain).lower()

        # Categorias expandidas (baseado em domain_authority_analysis do archive)
        trusted_news = ['folha.uol.com.br', 'g1.globo.com', 'estadao.com.br',
                       'oglobo.globo.com', 'uol.com.br', 'bbc.com', 'reuters.com',
                       'globo.com', 'folha.com', 'r7.com', 'terra.com.br']
        government = ['.gov.br', '.leg.br', '.jus.br', '.mil.br']
        video = ['youtube.com', 'youtu.be', 'vimeo.com', 'rumble.com', 'odysee.com']
        social = ['twitter.com', 'x.com', 'facebook.com', 'instagram.com',
                 't.me', 'telegram.me', 'whatsapp.com', 'tiktok.com']
        blog = ['blog', 'wordpress', 'medium.com', 'substack.com', 'blogspot']

        if any(trusted in domain_lower for trusted in trusted_news):
            return 'mainstream_news'
        elif any(gov in domain_lower for gov in government):
            return 'government'
        elif any(v in domain_lower for v in video):
            return 'video'
        elif any(s in domain_lower for s in social):
            return 'social'
        elif any(b in domain_lower for b in blog):
            return 'blog'
        else:
            return 'alternative'

    def _calculate_domain_trust_score(self, domain: str) -> float:
        """Calcula score de confian√ßa do dom√≠nio (Page et al. 1999, adaptado)."""
        if not domain or pd.isna(domain):
            return 0.0
        dtype = self._classify_domain_type(domain)
        trust_map = {
            'government': 0.9, 'mainstream_news': 0.8, 'video': 0.5,
            'social': 0.4, 'blog': 0.3, 'alternative': 0.2, 'unknown': 0.0
        }
        return trust_map.get(dtype, 0.0)

    def _calculate_sentiment_polarity(self, text: str) -> float:
        """Calcula polaridade com dicion√°rio LIWC expandido (Balage Filho et al. 2013)."""
        if not text or pd.isna(text):
            return 0.0

        # Dicion√°rio LIWC-PT expandido (baseado em sci_validated_methods_implementation.py)
        positive_words = [
            'bom', 'boa', 'bons', 'boas', '√≥timo', '√≥tima', 'excelente',
            'maravilhoso', 'maravilhosa', 'perfeito', 'perfeita', 'amor',
            'feliz', 'felicidade', 'alegria', 'alegre', 'vit√≥ria', 'sucesso',
            'conquista', 'esperan√ßa', 'orgulho', 'admira√ß√£o', 'respeito',
            'liberdade', 'paz', 'progresso', 'avan√ßo', 'melhoria',
            'lindo', 'linda', 'beleza', 'incr√≠vel', 'fant√°stico', 'fant√°stica',
            'parab√©ns', 'obrigado', 'obrigada', 'gratid√£o', 'aben√ßoado',
            'honra', 'gl√≥ria', 'ben√ß√£o', 'f√©', 'for√ßa', 'coragem'
        ]
        negative_words = [
            'ruim', 'p√©ssimo', 'p√©ssima', 'terr√≠vel', 'horr√≠vel', '√≥dio',
            'raiva', 'triste', 'tristeza', 'infeliz', 'medo', 'fracasso',
            'derrota', 'vergonha', 'nojo', 'desgra√ßa', 'desastre',
            'culpa', 'mis√©ria', 'sofrimento', 'dor', 'ang√∫stia',
            'decep√ß√£o', 'frustra√ß√£o', 'absurdo', 'rid√≠culo', 'rid√≠cula',
            'lament√°vel', 'deplor√°vel', 'covarde', 'covardia', 'mentira',
            'mentiroso', 'mentirosa', 'trai√ß√£o', 'traidor', 'traidora',
            'destrui√ß√£o', 'morte', 'desespero', 'p√¢nico', 'terror',
            'criminoso', 'criminosa', 'bandido', 'bandida', 'corrupto', 'corrup√ß√£o'
        ]

        text_lower = str(text).lower()
        words = text_lower.split()
        total_words = len(words)
        if total_words == 0:
            return 0.0

        positive_count = sum(1 for word in words if word in positive_words)
        negative_count = sum(1 for word in words if word in negative_words)

        return (positive_count - negative_count) / total_words

    def _calculate_emotion_intensity(self, text: str, raw_text: str = None) -> float:
        """Calcula intensidade emocional usando texto original (com pontua√ß√£o)."""
        # Usar raw_text (body original) se dispon√≠vel, pois normalized_text remove pontua√ß√£o
        source = raw_text if raw_text else text
        if not source or pd.isna(source):
            return 0.0

        source_str = str(source)
        emotion_markers = source_str.count('!') + source_str.count('?') + source_str.count('...')
        caps_words = sum(1 for word in source_str.split() if word.isupper() and len(word) > 2)

        return min((emotion_markers + caps_words) / 10.0, 1.0)

    def _detect_aggressive_language(self, text: str) -> bool:
        """Detecta linguagem agressiva usando l√©xico (autoritarismo_violencia + inimigos)."""
        if not text or pd.isna(text):
            return False

        text_lower = str(text).lower()
        terms_map = self._political_terms_map

        # Combinar termos de viol√™ncia e agress√£o do l√©xico
        aggressive_terms = terms_map.get('autoritarismo_violencia', [])
        # Adicionar termos cl√°ssicos de agress√£o pessoal
        extra_aggressive = [
            '√≥dio', 'matar', 'destruir', 'eliminar', 'acabar',
            'burro', 'idiota', 'imbecil', 'est√∫pido', 'canalha',
            'vagabundo', 'lixo', 'verme', 'parasita', 'bandido',
            'safado', 'nojento', 'covarde', 'traidor', 'criminoso'
        ]

        all_aggressive = set(aggressive_terms + extra_aggressive)
        return any(term in text_lower for term in all_aggressive)

    def _detect_political_context(self, text: str) -> str:
        """Detecta contexto pol√≠tico."""
        if not text or pd.isna(text):
            return 'none'

        text_lower = str(text).lower()

        if any(word in text_lower for word in ['elei√ß√£o', 'voto', 'urna', 'candidato', 'campanha', 'debate']):
            return 'electoral'
        elif any(word in text_lower for word in ['governo', 'ministro', 'presidente', 'planalto', 'congresso']):
            return 'government'
        elif any(word in text_lower for word in ['manifesta√ß√£o', 'protesto', 'greve', 'ato', 'marcha']):
            return 'protest'
        elif any(word in text_lower for word in ['economia', 'infla√ß√£o', 'desemprego', 'pib', 'd√≥lar']):
            return 'economic'
        elif any(word in text_lower for word in ['pandemia', 'covid', 'vacina', 'lockdown', 'quarentena']):
            return 'pandemic'
        else:
            return 'general'

    def _mentions_government(self, text: str) -> bool:
        """Verifica se menciona governo."""
        if not text or pd.isna(text):
            return False

        government_terms = [
            'governo', 'presidente', 'ministro', 'secret√°rio', 'federal',
            'planalto', 'congresso', 'senado', 'c√¢mara', 'deputado',
            'senador', 'governador', 'prefeito', 'bolsonaro', 'lula'
        ]
        text_lower = str(text).lower()
        return any(term in text_lower for term in government_terms)

    def _mentions_opposition(self, text: str) -> bool:
        """Verifica se menciona oposi√ß√£o."""
        if not text or pd.isna(text):
            return False

        terms_map = self._political_terms_map
        opposition_terms = terms_map.get('inimigos_ideologicos', [])
        extra = ['oposi√ß√£o', 'contra', 'resist√™ncia', 'impeachment', 'fora']
        all_terms = set(opposition_terms + extra)

        text_lower = str(text).lower()
        return any(term in text_lower for term in all_terms)

    def _detect_election_context(self, text: str) -> bool:
        """Detecta contexto eleitoral."""
        if not text or pd.isna(text):
            return False

        election_terms = [
            'elei√ß√£o', 'elei√ß√µes', 'voto', 'votos', 'urna', 'urnas',
            'candidato', 'candidata', 'campanha', 'debate', 'apura√ß√£o',
            'segundo turno', 'primeiro turno', 'tse', 'propaganda eleitoral'
        ]
        text_lower = str(text).lower()
        return any(term in text_lower for term in election_terms)

    def _detect_protest_context(self, text: str) -> bool:
        """Detecta contexto de protesto."""
        if not text or pd.isna(text):
            return False

        terms_map = self._political_terms_map
        mobilizacao = terms_map.get('mobilizacao_acao', [])
        extra = ['manifesta√ß√£o', 'protesto', 'greve', 'ocupa√ß√£o', 'ato', 'marcha']
        all_terms = set(mobilizacao + extra)

        text_lower = str(text).lower()
        return any(term in text_lower for term in all_terms)

    def _classify_channel_type(self, channel: str) -> str:
        """Classifica tipo de canal."""
        if not channel or pd.isna(channel):
            return 'unknown'

        channel_lower = str(channel).lower()

        if any(word in channel_lower for word in ['news', 'not√≠cia', 'jornal', 'imprensa']):
            return 'news'
        elif any(word in channel_lower for word in ['brasil', 'patriota', 'conservador', 'direita', 'bolso']):
            return 'political'
        elif any(word in channel_lower for word in ['humor', 'meme', 'engra√ßado', 'zueira']):
            return 'entertainment'
        elif any(word in channel_lower for word in ['gospel', 'igreja', 'cristo', 'deus']):
            return 'religious'
        else:
            return 'general'

    # ==========================================
    # FRAME ANALYSIS - Entman (1993)
    # ==========================================

    def _analyze_political_frames(self, text: str) -> dict:
        """Identifica frames pol√≠ticos (Entman 1993, J Communication 43(4): 51-58)."""
        if not text or pd.isna(text):
            return {'conflito': 0.0, 'responsabilizacao': 0.0, 'moralista': 0.0, 'economico': 0.0}

        frames = {
            'conflito': ['contra', 'ataque', 'briga', 'guerra', 'batalha', 'confronto',
                        'disputa', 'embate', 'oposi√ß√£o', 'advers√°rio', 'inimigo'],
            'responsabilizacao': ['culpa', 'respons√°vel', 'causou', 'provocou', 'deve',
                                 'culpado', 'responsabilidade', 'neglig√™ncia', 'omiss√£o'],
            'moralista': ['certo', 'errado', 'justo', 'moral', '√©tica', 'valores',
                         'pecado', 'virtude', 'honra', 'vergonha', 'dever', 'dignidade'],
            'economico': ['economia', 'dinheiro', 'custo', 'gasto', 'investimento', 'pib',
                         'infla√ß√£o', 'desemprego', 'sal√°rio', 'imposto', 'd√≠vida', 'mercado']
        }

        text_lower = str(text).lower()
        result = {}
        for frame, keywords in frames.items():
            score = sum(1 for word in keywords if word in text_lower)
            result[frame] = score / len(keywords)
        return result

    # ==========================================
    # MANN-KENDALL TREND TEST - Mann (1945); Kendall (1975)
    # ==========================================

    def _mann_kendall_trend_test(self, time_series) -> dict:
        """Teste n√£o-param√©trico para tend√™ncia (Mann 1945, Kendall 1975)."""
        try:
            from scipy import stats as scipy_stats
        except ImportError:
            return {'statistic': 0, 'p_value': 1.0, 'trend': 'unavailable'}

        n = len(time_series)
        if n < 4:
            return {'statistic': 0, 'p_value': 1.0, 'trend': 'insufficient_data'}

        s = 0
        for i in range(n - 1):
            for j in range(i + 1, n):
                s += np.sign(time_series[j] - time_series[i])

        var_s = n * (n - 1) * (2 * n + 5) / 18
        if s > 0:
            z = (s - 1) / np.sqrt(var_s)
        elif s < 0:
            z = (s + 1) / np.sqrt(var_s)
        else:
            z = 0

        p_value = 2 * (1 - scipy_stats.norm.cdf(abs(z)))

        if p_value < 0.05:
            trend = 'increasing' if z > 0 else 'decreasing'
        else:
            trend = 'no_trend'

        return {'statistic': float(s), 'p_value': float(p_value), 'trend': trend}

    # ==========================================
    # INFORMATION CASCADE DETECTION - Leskovec et al. (2007)
    # ==========================================

    def _detect_information_cascades(self, df) -> pd.DataFrame:
        """Detecta cascatas de informa√ß√£o (Leskovec et al. 2007, ACM Trans Web)."""
        cascades = []
        if 'is_fwrd' not in df.columns:
            return pd.DataFrame(cascades)

        forwarded = df[df['is_fwrd'] == True].copy()
        if len(forwarded) < 3:
            return pd.DataFrame(cascades)

        forwarded['cascade_id'] = forwarded.groupby('body').ngroup()
        for cascade_id in forwarded['cascade_id'].unique():
            cascade = forwarded[forwarded['cascade_id'] == cascade_id]
            if len(cascade) > 2:
                cascades.append({
                    'cascade_id': cascade_id,
                    'size': len(cascade),
                    'channels': cascade['channel'].nunique() if 'channel' in cascade.columns else 0,
                    'content_preview': str(cascade['body'].iloc[0])[:100]
                })

        return pd.DataFrame(cascades)


def main():
    """Teste do analyzer limpo."""
    logging.basicConfig(level=logging.INFO)

    # Teste com dados de exemplo
    test_data = pd.DataFrame({
        'body': [
            'Este √© um texto sobre pol√≠tica brasileira com bolsonaro',
            'Discuss√£o sobre economia e mercado financeiro liberal',
            'An√°lise social progressista sobre direitos humanos',
            'Texto neutro sobre tecnologia e ci√™ncia',
            'Debate pol√≠tico conservador sobre tradi√ß√µes'
        ],
        'timestamp': [
            '2023-01-01 10:00:00',
            '2023-01-01 11:00:00',
            '2023-01-01 10:30:00',
            '2023-01-02 15:00:00',
            '2023-01-01 10:15:00'
        ]
    })

    analyzer = CleanScientificAnalyzer()
    result = analyzer.analyze_dataset(test_data)

    print(f"\n‚úÖ An√°lise conclu√≠da:")
    print(f"üìä Colunas geradas: {result['columns_generated']}")
    print(f"üéØ Stages completados: {result['stats']['stages_completed']}")
    print(f"üîß Features extra√≠das: {result['stats']['features_extracted']}")

    return result


if __name__ == "__main__":
    main()