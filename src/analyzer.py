#!/usr/bin/env python3
"""
digiNEV Analyzer v.final
=======================

Sistema consolidado √∫nico de an√°lise de discurso pol√≠tico brasileiro.
Pipeline com 17 est√°gios interligados gerando 102+ colunas de an√°lise.

ARQUITETURA CONSOLIDADA:
- Sistema √∫nico centralizado (elimina estruturas paralelas)
- 17 est√°gios cient√≠ficos sequenciais
- Dados reais processados (sem m√©tricas inventadas)
- Configura√ß√£o unificada via config/settings.yaml

Author: digiNEV Academic Research Team
Version: v.final (Consolida√ß√£o Final)
"""

import pandas as pd
import numpy as np
import logging
import re
import json
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Machine Learning imports (sempre dispon√≠veis)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics.pairwise import cosine_similarity

# Standard library imports
import unicodedata
import time
from collections import Counter
from urllib.parse import urlparse

# Memory monitoring (optional)
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# Dependency validation decorator
def validate_stage_dependencies(required_columns=None, required_attrs=None):
    """
    Decorator para validar depend√™ncias entre stages.

    Args:
        required_columns: Lista de colunas obrigat√≥rias no DataFrame
        required_attrs: Lista de atributos obrigat√≥rios na inst√¢ncia
    """
    def decorator(func):
        def wrapper(self, df, *args, **kwargs):
            # Skip validation if processing chunks
            if getattr(self, '_skip_validation', False):
                return func(self, df, *args, **kwargs)

            stage_name = func.__name__.replace('_stage_', 'Stage ').replace('_', ' ').title()

            # Validar colunas obrigat√≥rias
            if required_columns:
                missing_cols = [col for col in required_columns if col not in df.columns]
                if missing_cols:
                    self.logger.error(f"‚ùå {stage_name}: Colunas obrigat√≥rias ausentes: {missing_cols}")
                    raise ValueError(f"Colunas obrigat√≥rias ausentes para {stage_name}: {missing_cols}")

            # Validar atributos obrigat√≥rios na inst√¢ncia
            if required_attrs:
                missing_attrs = [attr for attr in required_attrs if not hasattr(self, attr)]
                if missing_attrs:
                    self.logger.error(f"‚ùå {stage_name}: Atributos obrigat√≥rios ausentes: {missing_attrs}")
                    raise ValueError(f"Atributos obrigat√≥rios ausentes para {stage_name}: {missing_attrs}")

            # Executar fun√ß√£o se valida√ß√µes passaram
            return func(self, df, *args, **kwargs)
        return wrapper
    return decorator

# spaCy para processamento lingu√≠stico em portugu√™s
try:
    import spacy
    # Tentar carregar modelo portugu√™s
    try:
        nlp = spacy.load("pt_core_news_lg")
        SPACY_AVAILABLE = True
    except OSError:
        try:
            nlp = spacy.load("pt_core_news_sm")
            SPACY_AVAILABLE = True
        except OSError:
            nlp = None
            SPACY_AVAILABLE = False
except ImportError:
    nlp = None
    SPACY_AVAILABLE = False


class Analyzer:
    """
    Analisador consolidado com 14 stages sequenciais interligados.

    STAGES IMPLEMENTADOS (sem fallbacks confusos):
    01. feature_extraction (Python puro) - Detec√ß√£o autom√°tica de colunas e features
    02. text_preprocessing (Python puro) - Limpeza b√°sica de texto em portugu√™s
    03. linguistic_processing (spaCy) - Processamento lingu√≠stico avan√ßado LOGO AP√ìS limpeza
    04. statistical_analysis (Python puro) - An√°lise estat√≠stica com dados spaCy
    05. political_classification (Lexicon real) - Classifica√ß√£o pol√≠tica brasileira
    06. tfidf_vectorization (scikit-learn) - TF-IDF com tokens spaCy
    07. clustering_analysis (scikit-learn) - Clustering baseado em features lingu√≠sticas
    08. topic_modeling (scikit-learn) - Topic modeling com embeddings
    09. temporal_analysis (Python puro) - An√°lise temporal
    10. network_analysis (Python puro) - Coordena√ß√£o e padr√µes de rede
    """

    def __init__(self, chunk_size: int = 10000, memory_limit_gb: float = 2.0, auto_chunk: bool = True,
                 political_relevance_threshold: float = 0.02):
        """
        Inicializar analyzer com capacidades de auto-chunking.

        Args:
            chunk_size: Tamanho do chunk quando auto-chunking √© necess√°rio
            memory_limit_gb: Limite de mem√≥ria para trigger de chunking
            auto_chunk: Se True, detecta automaticamente quando usar chunks
            political_relevance_threshold: Threshold m√≠nimo para relev√¢ncia pol√≠tica (padr√£o: 0.02)
        """
        self.logger = logging.getLogger(self.__class__.__name__)

        # Configura√ß√µes de chunking autom√°tico
        self.chunk_size = chunk_size
        self.memory_limit_gb = memory_limit_gb
        self.auto_chunk = auto_chunk

        # Configura√ß√µes de filtros
        self.political_relevance_threshold = political_relevance_threshold

        # Load political lexicon if available
        self.political_lexicon = self._load_political_lexicon()

        # Initialize ML components
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.kmeans_model = None
        self.lda_model = None

        # Stats tracking
        self.stats = {
            'stages_completed': 0,
            'features_extracted': 0,
            'processing_errors': 0,
            'chunked_processing': False,
            'total_chunks': 0
        }

        # Global statistics container
        self.global_stats = {}

        self.logger.info("‚úÖ Analyzer v.final inicializado (auto-chunking habilitado)")

    def _load_political_lexicon(self) -> Dict:
        """Carregar lexicon pol√≠tico brasileiro correto."""
        lexicon_path = Path("src/core/lexico_politico_hierarquizado.json")

        if lexicon_path.exists():
            try:
                with open(lexicon_path, 'r', encoding='utf-8') as f:
                    lexicon = json.load(f)
                self.logger.info(f"‚úÖ Lexicon pol√≠tico carregado: {len(lexicon)} categorias")
                return lexicon
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro ao carregar lexicon: {e}")

        # Lexicon pol√≠tico brasileiro correto (conforme political_visualization_enhanced.py)
        return {
            "bolsonarista": ["bolsonaro", "mito", "capit√£o", "messias", "brasil acima de tudo"],
            "lulista": ["lula", "squid", "ex-presidente", "pt", "petista"],
            "anti-bolsonaro": ["fora bolsonaro", "impeachment", "golpista", "fascista"],
            "neutro": ["governo", "pol√≠tica", "brasil", "pa√≠s"],
            "geral": ["elei√ß√£o", "voto", "democracia", "constitui√ß√£o"],
            "indefinido": ["moderado", "centrista"]
        }

    def _should_use_chunking(self, data_input):
        """
        Determinar automaticamente se deve usar processamento em chunks.
        
        Args:
            data_input: DataFrame ou caminho do arquivo
            
        Returns:
            Tuple[usar_chunks, estimativa_registros, motivo]
        """
        try:
            # Se for DataFrame, verificar tamanho direto
            if isinstance(data_input, pd.DataFrame):
                n_records = len(data_input)
                memory_usage_mb = data_input.memory_usage(deep=True).sum() / (1024**2)
                
                if n_records > 10000:
                    return True, n_records, f"Dataset grande: {n_records:,} registros"
                elif memory_usage_mb > self.memory_limit_gb * 1024:
                    return True, n_records, f"Uso de mem√≥ria alto: {memory_usage_mb:.1f}MB"
                else:
                    return False, n_records, f"Dataset pequeno: {n_records:,} registros"
            
            # Se for caminho de arquivo, estimar tamanho
            elif isinstance(data_input, (str, Path)):
                file_path = Path(data_input)
                if not file_path.exists():
                    return False, 0, "Arquivo n√£o encontrado"
                
                # Estimar n√∫mero de registros pelo tamanho do arquivo
                file_size_mb = file_path.stat().st_size / (1024**2)
                estimated_records = int(file_size_mb * 100)  # Estimativa: ~100 registros por MB
                
                if file_size_mb > 50:  # Arquivos > 50MB
                    return True, estimated_records, f"Arquivo grande: {file_size_mb:.1f}MB"
                elif estimated_records > 10000:
                    return True, estimated_records, f"Muitos registros estimados: {estimated_records:,}"
                else:
                    return False, estimated_records, f"Arquivo pequeno: {file_size_mb:.1f}MB"
            
            return False, 0, "Tipo de entrada n√£o reconhecido"
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao determinar chunking: {e}")
            return False, 0, "Erro na detec√ß√£o"

    def _check_memory_usage(self) -> bool:
        """Verificar se mem√≥ria est√° pr√≥xima do limite."""
        if not PSUTIL_AVAILABLE:
            self.logger.warning("‚ö†Ô∏è psutil n√£o dispon√≠vel para monitoramento de mem√≥ria")
            return False

        try:
            memory_gb = psutil.Process().memory_info().rss / (1024**3)
            if memory_gb > self.memory_limit_gb:
                self.logger.warning(f"üö® Mem√≥ria alta: {memory_gb:.1f}GB > {self.memory_limit_gb}GB")
                return True
            return False
        except Exception as e:
            self.logger.error(f"‚ùå Erro no monitoramento de mem√≥ria: {e}")
            return False

    def _clean_memory(self):
        """Limpar mem√≥ria for√ßadamente."""
        import gc
        gc.collect()
        self.logger.info("üßπ Mem√≥ria limpa")

    def analyze(self, data_input, **kwargs) -> Dict[str, Any]:
        """
        M√©todo principal unificado que detecta automaticamente o modo de processamento.
        
        Args:
            data_input: DataFrame ou caminho para arquivo CSV
            **kwargs: Argumentos adicionais (max_records, output_file, etc.)
            
        Returns:
            Resultado da an√°lise (formato unificado)
        """
        # Auto-detec√ß√£o do modo de processamento
        should_chunk, estimated_records, reason = self._should_use_chunking(data_input)
        
        self.logger.info(f"ü§ñ Auto-detec√ß√£o: {reason}")
        
        if self.auto_chunk and should_chunk:
            self.logger.info(f"‚ö° Modo CHUNKED ativado automaticamente")
            self.stats['chunked_processing'] = True
            return self._analyze_chunked(data_input, **kwargs)
        else:
            self.logger.info(f"üî¨ Modo NORMAL (in-memory)")
            self.stats['chunked_processing'] = False
            
            # Se for arquivo, carregar como DataFrame
            if isinstance(data_input, (str, Path)):
                df = self._load_dataframe(data_input, kwargs.get('max_records'))
            else:
                df = data_input.copy()
                
            return self.analyze_dataset(df)

    def _load_dataframe(self, file_path: str, max_records: Optional[int] = None) -> pd.DataFrame:
        """Carregar DataFrame com detec√ß√£o autom√°tica de separador e tratamento robusto."""
        file_path = Path(file_path)
        
        # Detectar separador lendo primeira linha
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                first_line = f.readline().strip()
                
            # Contar separadores na primeira linha
            comma_count = first_line.count(',')
            semicolon_count = first_line.count(';')
            
            # Escolher separador baseado em contagem
            if semicolon_count > comma_count:
                separator = ';'
            elif comma_count > 0:
                separator = ','
            else:
                # Fallback: tentar ambos e ver qual funciona melhor
                try:
                    test_df = pd.read_csv(file_path, sep=';', nrows=1)
                    if len(test_df.columns) > 1:
                        separator = ';'
                    else:
                        separator = ','
                except:
                    separator = ','
            
            self.logger.info(f"üîç Separador detectado: '{separator}' (v√≠rgulas: {comma_count}, ponto-v√≠rgulas: {semicolon_count})")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro na detec√ß√£o de separador: {e}, usando v√≠rgula por padr√£o")
            separator = ','
        
        # Carregar com limite de registros se especificado
        try:
            # Usar configura√ß√µes mais robustas para CSV com conte√∫do complexo
            read_kwargs = {
                'sep': separator,
                'encoding': 'utf-8',
                'quotechar': '"',
                'quoting': 1,  # QUOTE_ALL
                'skipinitialspace': True,
                'on_bad_lines': 'skip'  # Pular linhas problem√°ticas
            }
            
            if max_records:
                read_kwargs['nrows'] = max_records
                
            df = pd.read_csv(file_path, **read_kwargs)
                
            # Verificar se carregamento foi bem-sucedido
            if len(df.columns) == 1 and separator == ';':
                # Tentar com v√≠rgula se ponto-v√≠rgula resultou em uma coluna s√≥
                self.logger.warning("‚ö†Ô∏è Tentando v√≠rgula ap√≥s falha com ponto-v√≠rgula")
                read_kwargs['sep'] = ','
                df = pd.read_csv(file_path, **read_kwargs)
                separator = ','
                
            self.logger.info(f"üìÇ Dataset carregado: {len(df):,} registros, {len(df.columns)} colunas (sep='{separator}')")
            return df
            
        except Exception as e:
            # Fallback: tentar com configura√ß√µes mais permissivas
            self.logger.warning(f"‚ö†Ô∏è Erro com configura√ß√µes padr√£o: {e}, tentando modo permissivo")
            try:
                fallback_kwargs = {
                    'sep': separator,
                    'encoding': 'utf-8',
                    'quotechar': '"',
                    'quoting': 0,  # QUOTE_MINIMAL
                    'skipinitialspace': True,
                    'on_bad_lines': 'skip',
                    'error_bad_lines': False,
                    'warn_bad_lines': True
                }
                
                if max_records:
                    fallback_kwargs['nrows'] = max_records
                    
                df = pd.read_csv(file_path, **fallback_kwargs)
                self.logger.info(f"üìÇ Dataset carregado (modo permissivo): {len(df):,} registros, {len(df.columns)} colunas")
                return df
                
            except Exception as e2:
                self.logger.error(f"‚ùå Erro ao carregar arquivo mesmo com fallback: {e2}")
                raise

    def _analyze_chunked(self, data_input, **kwargs) -> Dict[str, Any]:
        """
        Processamento em chunks integrado ao Analyzer principal.
        
        Args:
            data_input: Caminho do arquivo ou DataFrame  
            **kwargs: max_records, output_file, etc.
            
        Returns:
            Estat√≠sticas consolidadas no formato padr√£o
        """
        self.logger.info("‚ö° Iniciando an√°lise chunked integrada")
        
        # Se for DataFrame, converter para modo chunked simulado
        if isinstance(data_input, pd.DataFrame):
            return self._chunked_from_dataframe(data_input, **kwargs)
        
        # Processar arquivo em chunks
        file_path = Path(data_input)
        max_records = kwargs.get('max_records')
        output_file = kwargs.get('output_file')
        
        consolidated_results = []
        total_records = 0
        total_chunks = 0
        
        # Estat√≠sticas consolidadas
        consolidated_stats = {
            'political_distribution': {},
            'temporal_stats': {'valid_timestamps': 0, 'total_records': 0},
            'coordination_stats': {'coordinated': 0, 'total_records': 0},
            'domain_stats': {'with_links': 0, 'total_records': 0},
            'stages_completed': 0,
            'features_extracted': 0,
            'processing_errors': 0
        }
        
        try:
            # Processar cada chunk
            for chunk in self._load_dataset_chunks(file_path, max_records):
                chunk_start = time.time()
                
                # Analisar chunk usando m√©todo espec√≠fico para chunks
                chunk_data = self._analyze_single_chunk(chunk.copy())
                
                # Consolidar estat√≠sticas
                total_records += len(chunk_data)
                total_chunks += 1
                
                # Consolidar distribui√ß√£o pol√≠tica
                if 'political_spectrum' in chunk_data.columns:
                    political_dist = chunk_data['political_spectrum'].value_counts()
                    for category, count in political_dist.items():
                        consolidated_stats['political_distribution'][category] = \
                            consolidated_stats['political_distribution'].get(category, 0) + count
                
                # Consolidar outras estat√≠sticas
                if 'has_temporal_data' in chunk_data.columns:
                    valid_temporal = chunk_data['has_temporal_data'].sum()
                    consolidated_stats['temporal_stats']['valid_timestamps'] += valid_temporal
                    consolidated_stats['temporal_stats']['total_records'] += len(chunk_data)
                
                if 'potential_coordination' in chunk_data.columns:
                    coordinated = chunk_data['potential_coordination'].sum()
                    consolidated_stats['coordination_stats']['coordinated'] += coordinated
                    consolidated_stats['coordination_stats']['total_records'] += len(chunk_data)
                
                if 'has_external_links' in chunk_data.columns:
                    with_links = chunk_data['has_external_links'].sum()
                    consolidated_stats['domain_stats']['with_links'] += with_links
                    consolidated_stats['domain_stats']['total_records'] += len(chunk_data)
                
                # Atualizar stats consolidadas
                consolidated_stats['stages_completed'] = max(consolidated_stats['stages_completed'], self.stats.get('stages_completed', 0))
                consolidated_stats['features_extracted'] = max(consolidated_stats['features_extracted'], self.stats.get('features_extracted', 0))
                
                # Salvar chunk se solicitado
                if output_file:
                    chunk_output = f"{output_file.replace('.csv', '')}_chunk_{total_chunks}.csv"
                    chunk_data.to_csv(chunk_output, index=False, sep=';')
                    self.logger.info(f"üíæ Chunk salvo: {chunk_output}")
                
                chunk_time = time.time() - chunk_start
                chunk_performance = len(chunk_data) / chunk_time if chunk_time > 0 else 0
                
                self.logger.info(f"‚úÖ Chunk {total_chunks} processado: {len(chunk_data):,} registros em {chunk_time:.1f}s ({chunk_performance:.1f} reg/s)")
                
                # Limpar mem√≥ria entre chunks
                del chunk_data, chunk
                if self._check_memory_usage():
                    self._clean_memory()
                
        except Exception as e:
            self.logger.error(f"‚ùå Erro na an√°lise chunked: {e}")
            consolidated_stats['processing_errors'] += 1
            raise
        
        # Atualizar stats principais
        self.stats.update({
            'total_records_processed': total_records,
            'chunks_processed': total_chunks,
            'chunked_processing': True,
            'stages_completed': consolidated_stats['stages_completed'],
            'features_extracted': consolidated_stats['features_extracted']
        })
        
        self.logger.info(f"üéâ An√°lise chunked conclu√≠da: {total_records:,} registros em {total_chunks} chunks")
        
        # Retornar no formato padr√£o do Analyzer
        return {
            'data': pd.DataFrame({'processed_records': [total_records]}),  # DataFrame m√≠nimo para compatibilidade
            'stats': self.stats.copy(),
            'consolidated_stats': consolidated_stats,
            'columns_generated': 75,  # Estimativa baseada no pipeline
            'total_records': total_records,
            'stages_completed': consolidated_stats['stages_completed']
        }

    def _analyze_single_chunk(self, chunk_df: pd.DataFrame) -> pd.DataFrame:
        """
        Analisa um chunk espec√≠fico sem recurs√£o.
        Ajusta thresholds para chunks pequenos.
        """
        try:
            # Ajustar par√¢metros para chunks pequenos
            original_threshold = getattr(self, 'political_relevance_threshold', 0.02)
            chunk_size = len(chunk_df)
            
            # Threshold mais permissivo para chunks pequenos
            if chunk_size < 1000:
                self.political_relevance_threshold = 0.01  # Mais permissivo
                self.logger.info(f"üìä Chunk pequeno ({chunk_size}): threshold ajustado para {self.political_relevance_threshold}")
            elif chunk_size < 5000:
                self.political_relevance_threshold = 0.015  # Moderadamente permissivo
            
            # Ativar flag para pular valida√ß√µes durante processamento de chunk
            self._skip_validation = True
            
            self.logger.info(f"üîÑ Processando chunk: {len(chunk_df)} registros")
            
            # Processar atrav√©s do pipeline principal
            result_chunk = chunk_df.copy()
            
            # Stages 01-02: Preprocessing
            result_chunk = self._stage_01_feature_extraction(result_chunk)
            result_chunk = self._stage_02_text_preprocessing(result_chunk)
            
            # Stages 03-06: Volume reduction (cr√≠ticos para chunks)
            result_chunk = self._stage_03_cross_dataset_deduplication(result_chunk)
            result_chunk = self._stage_04_statistical_analysis(result_chunk)
            result_chunk = self._stage_05_content_quality_filter(result_chunk)
            result_chunk = self._stage_06_affordances_classification(result_chunk)
            
            # Verificar se ainda h√° dados suficientes ap√≥s filtros
            if len(result_chunk) == 0:
                self.logger.warning("‚ö†Ô∏è Chunk completamente filtrado, retornando dados parciais")
                # Restaurar threshold original
                self.political_relevance_threshold = original_threshold
                self._skip_validation = False
                return chunk_df.copy()  # Retornar dados originais com minimal processing
            
            # Stages 07-09: Linguistic processing (ajustados para chunks pequenos)
            try:
                result_chunk = self._stage_07_linguistic_processing(result_chunk)
                result_chunk = self._stage_08_political_classification(result_chunk)
                
                # Stage 09: TF-IDF com tratamento especial para chunks pequenos
                if len(result_chunk) >= 5:  # M√≠nimo vi√°vel para TF-IDF
                    result_chunk = self._stage_09_tfidf_vectorization(result_chunk)
                else:
                    self.logger.warning(f"üîç Chunk muito pequeno ({len(result_chunk)}) para TF-IDF, preenchendo com valores padr√£o")
                    result_chunk['tfidf_score_mean'] = 0.0
                    result_chunk['tfidf_score_max'] = 0.0 
                    result_chunk['tfidf_top_terms'] = [[] for _ in range(len(result_chunk))]
                    self.stats['features_extracted'] += 3
                
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro em processamento lingu√≠stico do chunk: {e}")
                # Continuar com dados parciais
            
            # Stages 10-17: Advanced analysis (opcionais para chunks pequenos)
            try:
                if len(result_chunk) >= 10:  # S√≥ executar se chunk tem tamanho vi√°vel
                    result_chunk = self._stage_10_clustering_analysis(result_chunk)
                    result_chunk = self._stage_11_topic_modeling(result_chunk)
                    result_chunk = self._stage_12_semantic_analysis(result_chunk)
                    result_chunk = self._stage_13_temporal_analysis(result_chunk)
                    result_chunk = self._stage_14_network_analysis(result_chunk)  # CORRIGIDO
                    result_chunk = self._stage_15_domain_analysis(result_chunk)   # CORRIGIDO
                    result_chunk = self._stage_16_event_context(result_chunk)     # CORRIGIDO
                    result_chunk = self._stage_17_channel_analysis(result_chunk) # CORRIGIDO
                else:
                    self.logger.info(f"üìä Chunk pequeno ({len(result_chunk)}): pulando an√°lises avan√ßadas")
                    # Adicionar colunas padr√£o para manter consist√™ncia
                    self._add_default_columns_for_small_chunks(result_chunk)
                    
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro em an√°lises avan√ßadas do chunk: {e}")
                # Continuar com dados dispon√≠veis
            
            # Restaurar configura√ß√µes originais
            self.political_relevance_threshold = original_threshold
            self._skip_validation = False
            
            self.logger.info(f"‚úÖ Chunk processado: {len(result_chunk)} registros finais")
            return result_chunk
            
        except Exception as e:
            # Restaurar configura√ß√µes em caso de erro
            if hasattr(self, 'political_relevance_threshold'):
                self.political_relevance_threshold = getattr(self, '_original_threshold', 0.02)
            self._skip_validation = False
            
            self.logger.error(f"‚ùå Erro ao processar chunk: {e}")
            # Retornar pelo menos os dados originais com colunas b√°sicas
            return self._add_minimal_columns(chunk_df.copy())

    def _add_default_columns_for_small_chunks(self, df: pd.DataFrame) -> pd.DataFrame:
        """Adiciona colunas padr√£o para chunks pequenos que pulam an√°lises avan√ßadas."""
        try:
            # Clustering columns
            if 'cluster_id' not in df.columns:
                df['cluster_id'] = 0
                df['cluster_size'] = len(df)
                df['cluster_confidence'] = 0.5
            
            # Topic modeling columns  
            if 'topic_distribution' not in df.columns:
                df['topic_distribution'] = [[] for _ in range(len(df))]
                df['dominant_topic'] = 0
                df['topic_confidence'] = 0.5
            
            # Semantic analysis columns
            if 'semantic_density' not in df.columns:
                df['semantic_density'] = 0.5
                df['semantic_complexity'] = 0.5
                df['semantic_coherence'] = 0.5
            
            # Temporal analysis columns
            if 'hour' not in df.columns and 'datetime' in df.columns:
                try:
                    df['hour'] = pd.to_datetime(df['datetime']).dt.hour
                    df['day_of_week'] = pd.to_datetime(df['datetime']).dt.dayofweek  
                    df['month'] = pd.to_datetime(df['datetime']).dt.month
                except:
                    df['hour'] = 12  # Default noon
                    df['day_of_week'] = 1  # Default Monday
                    df['month'] = 6  # Default June
            
            # Network coordination columns
            if 'coordination_score' not in df.columns:
                df['coordination_score'] = 0.0
                df['cascade_participation'] = False
                df['influence_score'] = 0.0
            
            # Domain analysis columns
            if 'domains_found' not in df.columns:
                df['domains_found'] = [[] for _ in range(len(df))]
                df['domain_authority_score'] = 0.0
                df['url_diversity'] = 0.0
            
            # Event context columns
            if 'political_events_detected' not in df.columns:
                df['political_events_detected'] = [[] for _ in range(len(df))]
                df['event_context_score'] = 0.0
                df['temporal_relevance'] = 0.5
            
            # Channel analysis columns
            if 'channel_classification' not in df.columns:
                df['channel_classification'] = 'unknown'
                df['channel_authority'] = 0.5
                df['source_type'] = 'social_media'
            
            self.logger.info(f"üìã Colunas padr√£o adicionadas para chunk pequeno")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao adicionar colunas padr√£o: {e}")
            return df
    
    def _add_minimal_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Adiciona colunas m√≠nimas em caso de erro no processamento."""
        try:
            # Colunas essenciais
            if 'political_relevance_score' not in df.columns:
                df['political_relevance_score'] = 0.5
            if 'content_quality_score' not in df.columns:
                df['content_quality_score'] = 50.0
            if 'normalized_text' not in df.columns:
                text_col = 'body' if 'body' in df.columns else df.columns[0]
                df['normalized_text'] = df[text_col].fillna('')
            
            self.logger.info(f"üìã Colunas m√≠nimas adicionadas para recupera√ß√£o de erro")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao adicionar colunas m√≠nimas: {e}")
            return df

    def _load_dataset_chunks(self, file_path: Path, max_records: Optional[int] = None):
        """Generator para carregar dataset em chunks."""
        if not file_path.exists():
            raise FileNotFoundError(f"Dataset n√£o encontrado: {file_path}")
        
        self.logger.info(f"üìÇ Carregando dataset em chunks: {file_path}")
        
        # Detectar separador e configura√ß√£o necess√°ria
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                first_line = f.readline().strip()
                
            # Determinar separador
            comma_count = first_line.count(',')
            semicolon_count = first_line.count(';')
            separator = ';' if semicolon_count > comma_count else ','
            
            # Configura√ß√µes robustas para CSV complexo
            read_kwargs = {
                'sep': separator,
                'encoding': 'utf-8',
                'engine': 'python',  # Usar engine Python para maior flexibilidade
                'quotechar': '"',
                'quoting': 1,  # QUOTE_ALL
                'skipinitialspace': True,
                'on_bad_lines': 'skip',
                'chunksize': self.chunk_size
            }
            
            if max_records:
                read_kwargs['nrows'] = max_records
                
            self.logger.info(f"üîç Configura√ß√£o: sep='{separator}', engine=python")
            
            chunk_number = 0
            total_processed = 0
            
            try:
                # Usar pandas com engine Python para maior robustez
                for chunk in pd.read_csv(file_path, **read_kwargs):
                    chunk_number += 1
                    total_processed += len(chunk)
                    
                    self.logger.info(f"üì¶ Chunk {chunk_number}: {len(chunk)} registros (total: {total_processed})")
                    
                    # Verificar se chunk tem dados v√°lidos
                    if len(chunk) > 0 and len(chunk.columns) > 1:
                        yield chunk
                    
                    if max_records and total_processed >= max_records:
                        break
                        
            except Exception as e:
                self.logger.error(f"‚ùå Erro ao carregar dataset: {e}")
                
                # Fallback: carregar linha por linha se necess√°rio
                self.logger.warning("‚ö†Ô∏è Tentando carregamento linha por linha...")
                try:
                    import csv
                    
                    with open(file_path, 'r', encoding='utf-8') as csvfile:
                        # Detectar dialect automaticamente
                        sample = csvfile.read(1024)
                        csvfile.seek(0)
                        sniffer = csv.Sniffer()
                        dialect = sniffer.sniff(sample)
                        
                        reader = csv.DictReader(csvfile, dialect=dialect)
                        
                        chunk_data = []
                        for i, row in enumerate(reader):
                            chunk_data.append(row)
                            
                            if len(chunk_data) >= self.chunk_size:
                                df_chunk = pd.DataFrame(chunk_data)
                                chunk_number += 1
                                self.logger.info(f"üì¶ Chunk CSV {chunk_number}: {len(df_chunk)} registros")
                                yield df_chunk
                                chunk_data = []
                            
                            if max_records and i >= max_records:
                                break
                        
                        # √öltimo chunk se houver dados
                        if chunk_data:
                            df_chunk = pd.DataFrame(chunk_data)
                            chunk_number += 1
                            self.logger.info(f"üì¶ Chunk CSV final {chunk_number}: {len(df_chunk)} registros")
                            yield df_chunk
                            
                except Exception as e2:
                    self.logger.error(f"‚ùå Fallback tamb√©m falhou: {e2}")
                    raise
                
        except Exception as e:
            self.logger.error(f"‚ùå Erro cr√≠tico no carregamento: {e}")
            raise

    def _chunked_from_dataframe(self, df: pd.DataFrame, **kwargs) -> Dict[str, Any]:
        """Simular processamento chunked para DataFrame que j√° est√° em mem√≥ria."""
        self.logger.info("üîÑ Simulando chunked para DataFrame em mem√≥ria")
        
        # Se DataFrame √© pequeno, processar normalmente
        if len(df) <= self.chunk_size:
            return self._analyze_single_chunk(df)

        # Dividir DataFrame em chunks
        chunks = [df[i:i+self.chunk_size] for i in range(0, len(df), self.chunk_size)]

        consolidated_results = []
        for i, chunk in enumerate(chunks, 1):
            self.logger.info(f"üì¶ Processando chunk {i}/{len(chunks)}: {len(chunk)} registros")
            result = self._analyze_single_chunk(chunk.copy())
            consolidated_results.append(result)
            
            if self._check_memory_usage():
                self._clean_memory()
        
        # Para simplificar, retornar resultado do √∫ltimo chunk com stats consolidados
        final_result = consolidated_results[-1]
        final_result['stats']['total_chunks'] = len(chunks)
        final_result['stats']['chunked_processing'] = True
        
        return final_result

    def analyze_dataset(self, data_input, max_records=None) -> Dict[str, Any]:
        """
        Analisar dataset com pipeline sequencial otimizado de 17 stages.

        NOVA SEQU√äNCIA OTIMIZADA (conforme PIPELINE_STAGES_ANALYSIS.md):
        Fase 1: Prepara√ß√£o (01-02) - estrutura b√°sica
        Fase 2: Redu√ß√£o de volume (03-06) - CR√çTICO para performance
        Fase 3: An√°lise lingu√≠stica (07-09) - volume reduzido
        Fase 4: An√°lises avan√ßadas (10-17) - dados otimizados

        Args:
            data_input: DataFrame ou caminho do arquivo para an√°lise
            max_records: Limite m√°ximo de registros para processar (opcional)

        Returns:
            Dict com resultado da an√°lise
        """
        # Verificar se deve usar chunking
        should_chunk, estimated_records, reason = self._should_use_chunking(data_input)

        # Se for caminho de arquivo e deve usar chunking, usar processamento chunked
        if self.auto_chunk and should_chunk:
            self.logger.info(f"‚ö° Chunking ativado: {reason}")
            self.stats['chunked_processing'] = True
            return self._analyze_chunked(data_input, max_records=max_records)

        # Sen√£o, carregar dados e processar normalmente
        self.stats['chunked_processing'] = False

        # Se for caminho de arquivo, carregar
        if isinstance(data_input, (str, Path)):
            df = self._load_dataframe(data_input, max_records)
        else:
            df = data_input
            if max_records and len(df) > max_records:
                df = df.head(max_records)
                self.logger.info(f"üìä Limitado a {max_records:,} registros")

        try:
            self.logger.info(f"üî¨ Iniciando an√°lise OTIMIZADA: {len(df)} registros")

            # Reset stats
            self.stats = {'stages_completed': 0, 'features_extracted': 0, 'processing_errors': 0}

            # ===========================================
            # FASE 1: PREPARA√á√ÉO E ESTRUTURA (01-02)
            # ===========================================
            
            # STAGE 01: Feature Extraction (estrutura b√°sica)
            df = self._stage_01_feature_extraction(df)

            # STAGE 02: Text Preprocessing (limpeza b√°sica)
            df = self._stage_02_text_preprocessing(df)

            # ===========================================
            # FASE 2: REDU√á√ÉO DE VOLUME (03-06) - CR√çTICO!
            # ===========================================
            
            # STAGE 03: Cross-Dataset Deduplication (40-50% redu√ß√£o)
            df = self._stage_03_cross_dataset_deduplication(df)

            # STAGE 04: Statistical Analysis (compara√ß√£o antes/depois)
            df = self._stage_04_statistical_analysis(df)

            # STAGE 05: Content Quality Filter (15-25% redu√ß√£o adicional)
            df = self._stage_05_content_quality_filter(df)

            # STAGE 06: Affordances Classification (AI-powered analysis)
            df = self._stage_06_affordances_classification(df)

            self.logger.info(f"üìä FASE 2 CONCLU√çDA: Volume reduzido para {len(df):,} registros")

            # ===========================================
            # FASE 3: AN√ÅLISE LINGU√çSTICA (07-09) - VOLUME REDUZIDO
            # ===========================================
            
            # STAGE 07: Linguistic Processing (spaCy - AGORA com volume otimizado)
            df = self._stage_07_linguistic_processing(df)  # Usar m√©todo existente

            # STAGE 08: Political Classification (usando tokens spaCy)
            df = self._stage_08_political_classification(df)  # Usar m√©todo existente

            # STAGE 09: TF-IDF Vectorization (usando lemmas spaCy)
            df = self._stage_09_tfidf_vectorization(df)  # Usar m√©todo existente

            # ===========================================
            # FASE 4: AN√ÅLISES AVAN√áADAS (10-17)
            # ===========================================
            
            # STAGE 10: Clustering Analysis
            df = self._stage_10_clustering_analysis(df)  # Usar m√©todo existente

            # STAGE 11: Topic Modeling
            df = self._stage_11_topic_modeling(df)  # Usar m√©todo existente

            # STAGE 12: Semantic Analysis
            df = self._stage_12_semantic_analysis(df)  # Usar m√©todo existente

            # STAGE 13: Temporal Analysis
            df = self._stage_13_temporal_analysis(df)  # Usar m√©todo existente

            # STAGE 14: Network Analysis
            df = self._stage_14_network_analysis(df)  # Usar m√©todo existente

            # STAGE 15: Domain Analysis
            df = self._stage_15_domain_analysis(df)  # Usar m√©todo existente

            # STAGE 16: Event Context Analysis
            df = self._stage_16_event_context(df)  # Usar m√©todo existente

            # STAGE 17: Channel Analysis
            df = self._stage_17_channel_analysis(df)  # Usar m√©todo existente

            # Final metadata
            df['processing_timestamp'] = datetime.now().isoformat()
            df['stages_completed'] = self.stats['stages_completed']
            df['features_extracted'] = self.stats['features_extracted']

            self.logger.info(f"‚úÖ An√°lise OTIMIZADA conclu√≠da: {len(df.columns)} colunas, {self.stats['stages_completed']} stages")
            self.logger.info(f"üéØ Performance: Processados {len(df):,} registros finais")

            return {
                'success': True,
                'data': df,
                'stats': self.stats.copy(),
                'columns_generated': len(df.columns),
                'total_records': len(df),
                'stages_completed': self.stats['stages_completed']
            }

        except Exception as e:
            self.logger.error(f"‚ùå Erro na an√°lise: {e}")
            raise

    def _stage_01_feature_extraction(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        STAGE 01: Extra√ß√£o e identifica√ß√£o autom√°tica de features (Python puro).

        SEMPRE O PRIMEIRO STAGE - identifica colunas dispon√≠veis e extrai features se necess√°rio.
        """
        self.logger.info("üîç STAGE 01: Feature Extraction")

        # Identificar coluna de texto principal
        text_columns = []
        for col in df.columns:
            if df[col].dtype == 'object':
                # Verificar se cont√©m texto substancial
                sample = df[col].dropna().head(10)
                if len(sample) > 0:
                    avg_length = sample.astype(str).str.len().mean()
                    if avg_length > 20:  # Textos com mais de 20 caracteres em m√©dia
                        text_columns.append(col)

        # Selecionar melhor coluna de texto
        if not text_columns:
            raise ValueError("‚ùå Nenhuma coluna de texto encontrada")

        # Priorizar colunas comuns
        priority_columns = ['text', 'body', 'message', 'content', 'texto', 'mensagem']
        main_text_column = None

        for priority in priority_columns:
            if priority in text_columns:
                main_text_column = priority
                break

        if not main_text_column:
            main_text_column = text_columns[0]

        # Identificar coluna de timestamp (se dispon√≠vel)
        timestamp_column = None
        for col in df.columns:
            if 'time' in col.lower() or 'date' in col.lower() or 'timestamp' in col.lower():
                timestamp_column = col
                break

        # === PADRONIZA√á√ÉO DE DATETIME ===
        if timestamp_column:
            df = self._standardize_datetime_column(df, timestamp_column)
            # Ap√≥s padroniza√ß√£o, a coluna se chama 'datetime'
            timestamp_column = 'datetime'

        # DETEC√á√ÉO AUTOM√ÅTICA DE FEATURES EXISTENTES
        features_detected = self._detect_existing_features(df)

        # EXTRA√á√ÉO AUTOM√ÅTICA DE FEATURES (se n√£o existem)
        df = self._extract_missing_features(df, main_text_column, features_detected)

        # === CONTAR COLUNAS DE METADADOS ===
        # Metadados = todas as colunas exceto texto principal e datetime padronizado
        metadata_columns = []
        for col in df.columns:
            if col not in [main_text_column, 'datetime'] and not col.startswith(('emojis_', 'hashtags_', 'urls_', 'mentions_')):
                metadata_columns.append(col)

        # Adicionar features identificadas
        df['main_text_column'] = main_text_column
        df['timestamp_column'] = timestamp_column if timestamp_column else 'none'
        df['metadata_columns_count'] = len(metadata_columns)
        df['has_timestamp'] = timestamp_column is not None

        self.stats['stages_completed'] += 1
        self.stats['features_extracted'] += 4 + len(features_detected['extracted'])

        self.logger.info(f"‚úÖ Features: text={main_text_column}, timestamp={timestamp_column}")
        self.logger.info(f"‚úÖ Features detectadas: {list(features_detected['existing'].keys())}")
        self.logger.info(f"‚úÖ Features extra√≠das: {features_detected['extracted']}")
        if timestamp_column:
            self.logger.info(f"üìÖ Datetime otimizado: coluna √∫nica 'datetime'")
        return df

    def _standardize_datetime_column(self, df: pd.DataFrame, timestamp_column: str) -> pd.DataFrame:
        """
        Padronizar coluna de datetime para formato √∫nico DD/MM/AAAA HH:MM:SS.
        Remove coluna original e substitui por vers√£o padronizada.
        
        Args:
            df: DataFrame com dados
            timestamp_column: Nome da coluna de timestamp identificada
            
        Returns:
            DataFrame com coluna datetime padronizada (substitui a original)
        """
        self.logger.info(f"üìÖ Padronizando datetime da coluna: {timestamp_column}")
        
        def parse_datetime(datetime_str):
            """Tentar m√∫ltiplos formatos de datetime."""
            if pd.isna(datetime_str):
                return None
                
            datetime_str = str(datetime_str).strip()
            
            # Formatos comuns para tentar
            formats_to_try = [
                '%Y-%m-%d %H:%M:%S',      # 2019-07-02 01:10:00
                '%d/%m/%Y %H:%M:%S',      # 02/07/2019 01:10:00
                '%Y-%m-%d',               # 2019-07-02
                '%d/%m/%Y',               # 02/07/2019
                '%Y-%m-%d %H:%M',         # 2019-07-02 01:10
                '%d/%m/%Y %H:%M',         # 02/07/2019 01:10
                '%Y/%m/%d %H:%M:%S',      # 2019/07/02 01:10:00
                '%m/%d/%Y %H:%M:%S',      # 07/02/2019 01:10:00 (formato americano)
            ]
            
            for fmt in formats_to_try:
                try:
                    parsed_dt = pd.to_datetime(datetime_str, format=fmt)
                    # Converter para formato padr√£o brasileiro DD/MM/AAAA HH:MM:SS
                    return parsed_dt.strftime('%d/%m/%Y %H:%M:%S')
                except (ValueError, TypeError):
                    continue
                    
            # Se nenhum formato funcionou, tentar parse gen√©rico do pandas
            try:
                parsed_dt = pd.to_datetime(datetime_str, infer_datetime_format=True)
                return parsed_dt.strftime('%d/%m/%Y %H:%M:%S')
            except:
                return None
        
        # Aplicar padroniza√ß√£o
        datetime_standardized = df[timestamp_column].apply(parse_datetime)
        
        # === SUBSTITUIR COLUNA ORIGINAL ===
        # Remover coluna original e usar nome 'datetime' para a vers√£o padronizada
        df = df.drop(columns=[timestamp_column])
        df['datetime'] = datetime_standardized
        
        # Estat√≠sticas de convers√£o
        valid_datetimes = df['datetime'].notna().sum()
        total_records = len(df)
        success_rate = (valid_datetimes / total_records) * 100
        
        self.logger.info(f"‚úÖ Datetime padronizado e substitu√≠do: {valid_datetimes}/{total_records} ({success_rate:.1f}%) v√°lidos")
        
        # Amostras do resultado
        sample_standardized = df['datetime'].dropna().head(3).tolist()
        
        self.logger.info(f"üìã Formato final:")
        for i, std in enumerate(sample_standardized):
            self.logger.info(f"   {i+1}. {std}")
        
        return df

    def _detect_existing_features(self, df: pd.DataFrame) -> Dict:
        """
        Detecta features que j√° existem como colunas no DataFrame.
        """
        existing_features = {}

        # Features de interesse para detectar
        feature_patterns = {
            'hashtags': ['hashtag', 'hashtags', 'tags'],
            'urls': ['url', 'urls', 'links', 'link'],
            'mentions': ['mention', 'mentions', 'user_mentions', 'usuarios'],
            'emojis': ['emoji', 'emojis', 'emoticon'],
            'reply_count': ['reply', 'replies', 'respostas'],
            'retweet_count': ['retweet', 'retweets', 'rt_count'],
            'like_count': ['like', 'likes', 'curtidas', 'fav'],
            'user_info': ['user', 'username', 'author', 'usuario']
        }

        for feature_name, patterns in feature_patterns.items():
            for pattern in patterns:
                matching_cols = [col for col in df.columns if pattern in col.lower()]
                if matching_cols:
                    existing_features[feature_name] = matching_cols[0]
                    break

        return {
            'existing': existing_features,
            'extracted': []
        }

    def _extract_missing_features(self, df: pd.DataFrame, text_column: str, features_info: Dict) -> pd.DataFrame:
        """
        Extrai apenas features essenciais do texto principal.
        """
        extracted_features = []

        # Verificar se a coluna de texto existe
        if text_column not in df.columns:
            self.logger.error(f"‚ùå Coluna de texto '{text_column}' n√£o encontrada no DataFrame")
            self.logger.error(f"Colunas dispon√≠veis: {list(df.columns)}")
            # Usar primeira coluna dispon√≠vel como fallback
            text_column = df.columns[0] if len(df.columns) > 0 else 'body'
            self.logger.warning(f"‚ö†Ô∏è Usando coluna '{text_column}' como fallback")

        # S√≥ extrair se n√£o existir coluna correspondente
        if 'hashtags' not in features_info['existing']:
            df['hashtags_extracted'] = df[text_column].astype(str).str.findall(r'#\w+')
            extracted_features.append('hashtags')

        if 'urls' not in features_info['existing']:
            url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
            df['urls_extracted'] = df[text_column].astype(str).str.findall(url_pattern)
            extracted_features.append('urls')

        if 'mentions' not in features_info['existing']:
            # Padr√£o para @mentions
            df['mentions_extracted'] = df[text_column].astype(str).str.findall(r'@\w+')
            extracted_features.append('mentions')

        if 'emojis' not in features_info['existing']:
            # Padr√£o b√°sico para emojis (Unicode)
            emoji_pattern = r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002600-\U000027BF]'
            df['emojis_extracted'] = df[text_column].astype(str).str.findall(emoji_pattern)
            extracted_features.append('emojis')

        # REMOVIDAS: has_interrogation, has_exclamation, has_caps_words, has_portuguese_words
        # Estas colunas n√£o s√£o necess√°rias para a an√°lise

        features_info['extracted'] = extracted_features
        return df

    def _stage_02_text_preprocessing(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        STAGE 02: Valida√ß√£o de Features + Limpeza de Texto.

        Para datasets com estrutura correta (datetime, body, url, hashtag, channel, etc):
        1. Validar features existentes contra coluna 'body' 
        2. Corrigir features incorretas/vazias
        3. Limpar body_cleaned (texto sem features)
        4. Aplicar normaliza√ß√£o de texto
        
        USA: Detec√ß√£o autom√°tica da estrutura do dataset
        """
        self.logger.info("üßπ STAGE 02: Feature Validation + Text Preprocessing")

        # === DETECTAR ESTRUTURA DO DATASET ===
        expected_columns = ['datetime', 'body', 'url', 'hashtag', 'channel', 'is_fwrd', 'mentions', 'sender', 'media_type', 'domain', 'body_cleaned']
        
        if all(col in df.columns for col in expected_columns[:5]):  # Verificar colunas essenciais
            self.logger.info("‚úÖ Dataset estruturado detectado - validando features existentes")
            
            # === FASE 1: VALIDA√á√ÉO DE FEATURES EXISTENTES ===
            df = self._extract_and_validate_features(df, 'body')
            
            # === FASE 2: LIMPEZA DE TEXTO (usar body como principal) ===
            main_text_col = 'body'
            
        else:
            self.logger.info("‚ö†Ô∏è Dataset n√£o estruturado - usando coluna principal")
            
            # Obter nome da coluna principal de texto (armazenado no Stage 01)
            if 'main_text_column' in df.columns and len(df) > 0:
                main_text_col = df['main_text_column'].iloc[0]
                self.logger.info(f"üîç Coluna principal identificada: {main_text_col}")
                
                # Verificar se a coluna existe realmente
                if main_text_col not in df.columns:
                    self.logger.warning(f"‚ö†Ô∏è Coluna '{main_text_col}' n√£o encontrada, buscando alternativa")
                    # Buscar coluna de texto v√°lida
                    text_columns = [col for col in df.columns if df[col].dtype == 'object' and col not in ['main_text_column', 'timestamp_column']]
                    if text_columns:
                        main_text_col = text_columns[0]
                        self.logger.info(f"‚úÖ Usando coluna alternativa: {main_text_col}")
                    else:
                        raise ValueError("‚ùå Nenhuma coluna de texto v√°lida encontrada")
            else:
                # Fallback: buscar coluna de texto
                text_columns = [col for col in df.columns if df[col].dtype == 'object']
                if text_columns:
                    main_text_col = text_columns[0]
                    self.logger.warning(f"‚ö†Ô∏è Usando primeira coluna de texto dispon√≠vel: {main_text_col}")
                else:
                    raise ValueError("‚ùå Nenhuma coluna de texto encontrada")
            
            # === FASE 1: EXTRA√á√ÉO DE FEATURES ===
            df = self._extract_and_validate_features(df, main_text_col)
        
        # === FASE 2: NORMALIZA√á√ÉO DE TEXTO ===
        def clean_text(text):
            """Limpar texto usando Python puro."""
            if pd.isna(text):
                return ""

            text = str(text)

            # Normalizar unicode
            text = unicodedata.normalize('NFKD', text)

            # Remover caracteres especiais mas preservar acentos
            text = re.sub(r'[^\w\s\u00C0-\u017F]', ' ', text)

            # Normalizar espa√ßos
            text = re.sub(r'\s+', ' ', text).strip()

            # Converter para lowercase
            text = text.lower()

            return text

        # Aplicar limpeza ao texto principal
        df['normalized_text'] = df[main_text_col].apply(clean_text)

        self.stats['stages_completed'] += 1
        self.stats['features_extracted'] += 2

        self.logger.info(f"‚úÖ Stage 02 conclu√≠do: {df['normalized_text'].str.len().mean():.1f} chars m√©dia")
        return df

    def _extract_and_validate_features(self, df: pd.DataFrame, main_text_col: str) -> pd.DataFrame:
        """
        Validar features existentes contra coluna 'body' e corrigir se necess√°rio.
        
        Dataset j√° tem: datetime, body, url, hashtag, channel, is_fwrd, mentions, sender, media_type, domain, body_cleaned
        """
        self.logger.info("üîç Validando features existentes contra coluna 'body'...")
        
        # === VERIFICAR SE DATASET TEM ESTRUTURA CORRETA ===
        expected_columns = ['datetime', 'body', 'url', 'hashtag', 'channel', 'is_fwrd', 'mentions', 'sender', 'media_type', 'domain', 'body_cleaned']
        
        # Se o dataset tem as colunas corretas, usar body como texto principal
        if all(col in df.columns for col in expected_columns[:5]):  # Verificar colunas essenciais
            self.logger.info("‚úÖ Dataset com estrutura correta detectado")
            
            # === REMOVER BODY_CLEANED (duplica√ß√£o desnecess√°ria) ===
            if 'body_cleaned' in df.columns:
                df = df.drop(columns=['body_cleaned'])
                self.logger.info("üóëÔ∏è body_cleaned removido (duplica√ß√£o desnecess√°ria)")
            
            # === VALIDAR FEATURES CONTRA BODY ===
            corrections_made = 0
            
            # Validar URL
            if 'url' in df.columns and 'body' in df.columns:
                corrections_made += self._validate_feature_against_body(df, 'url', 'body', [r'https?://\S+', r'www\.\S+'])
            
            # Validar Hashtags
            if 'hashtag' in df.columns and 'body' in df.columns:
                corrections_made += self._validate_feature_against_body(df, 'hashtag', 'body', [r'#\w+'])
            
            # Validar Mentions
            if 'mentions' in df.columns and 'body' in df.columns:
                corrections_made += self._validate_feature_against_body(df, 'mentions', 'body', [r'@\w+'])
            
            self.logger.info(f"‚úÖ Valida√ß√£o conclu√≠da: {corrections_made} corre√ß√µes aplicadas")
            
        else:
            # === DATASET SEM ESTRUTURA PADR√ÉO - EXTRAIR TUDO ===
            self.logger.info("‚ö†Ô∏è Dataset sem estrutura padr√£o - extraindo features do texto principal")
            df = self._extract_features_from_text(df, main_text_col)
        
        return df
    
    def _validate_feature_against_body(self, df: pd.DataFrame, feature_col: str, body_col: str, patterns: list) -> int:
        """Validar feature espec√≠fica contra body."""
        corrections = 0
        
        for idx, row in df.iterrows():
            body_text = str(row[body_col]) if pd.notna(row[body_col]) else ""
            existing_feature = row[feature_col] if pd.notna(row[feature_col]) else ""
            
            # Extrair feature do body
            extracted_features = []
            for pattern in patterns:
                matches = re.findall(pattern, body_text, re.IGNORECASE)
                extracted_features.extend(matches)
            
            # Se encontrou features no body mas coluna est√° vazia, corrigir
            if extracted_features and not existing_feature:
                if len(extracted_features) == 1:
                    df.at[idx, feature_col] = extracted_features[0]
                else:
                    df.at[idx, feature_col] = ';'.join(extracted_features)  # M√∫ltiplas features
                corrections += 1
        
        if corrections > 0:
            self.logger.info(f"üîß {feature_col}: {corrections} corre√ß√µes aplicadas")
        
        return corrections
    
    def _clean_body_text(self, df: pd.DataFrame):
        """
        REMOVIDO: body_cleaned n√£o √© mais necess√°rio.
        O texto limpo √© gerado como 'normalized_text' no Stage 02.
        """
        # N√£o fazer nada - body_cleaned removido para evitar duplica√ß√£o
        self.logger.info("üóëÔ∏è body_cleaned removido (duplica√ß√£o desnecess√°ria)")
        pass    
    def _extract_features_from_text(self, df: pd.DataFrame, text_col: str) -> pd.DataFrame:
        """Extrair features de dataset sem estrutura padr√£o (fallback)."""
        
        # Features essenciais para extrair
        feature_patterns = {
            'urls': [r'https?://\S+', r'www\.\S+'],
            'hashtags': [r'#\w+'],
            'mentions': [r'@\w+'],
            'channel_name': []  # Usar valor padr√£o
        }
        
        for feature_name, patterns in feature_patterns.items():
            if patterns:
                def extract_feature(text):
                    if pd.isna(text):
                        return []
                    
                    extracted = []
                    for pattern in patterns:
                        matches = re.findall(pattern, str(text), re.IGNORECASE)
                        extracted.extend(matches)
                    return list(set(extracted)) if extracted else []
                
                df[feature_name] = df[text_col].apply(extract_feature)
            else:
                df[feature_name] = "unknown_channel"
        
        self.logger.info("üÜï Features extra√≠das de dataset sem estrutura padr√£o")
        return df
    
    def _find_existing_feature_column(self, df: pd.DataFrame, possible_names: list) -> str:
        """Encontrar coluna existente para uma feature."""
        for col_name in possible_names:
            if col_name in df.columns:
                return col_name
        return None
    
    def _validate_and_correct_feature(self, df: pd.DataFrame, existing_col: str, feature_name: str, patterns: list, text_col: str) -> pd.DataFrame:
        """Validar e corrigir feature existente."""
        if not patterns:  # Channel name n√£o tem padr√£o regex
            self.logger.info(f"‚úÖ Feature {existing_col} mantida (sem valida√ß√£o regex)")
            return df
        
        # Extrair valores corretos do texto
        def extract_correct_values(text):
            if pd.isna(text):
                return []
            
            text = str(text)
            extracted = []
            for pattern in patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                extracted.extend(matches)
            return list(set(extracted))  # Remover duplicatas
        
        # Validar contra texto original
        df['_temp_extracted'] = df[text_col].apply(extract_correct_values)
        
        # Comparar e corrigir se necess√°rio
        corrections_made = 0
        
        def validate_and_fix(row):
            nonlocal corrections_made
            existing_value = row[existing_col]
            correct_value = row['_temp_extracted']
            
            # Se valor existente est√° vazio ou incorreto, corrigir
            if pd.isna(existing_value) or existing_value == [] or existing_value == '':
                if correct_value:
                    corrections_made += 1
                    return correct_value
            
            return existing_value
        
        df[existing_col] = df.apply(validate_and_fix, axis=1)
        df = df.drop('_temp_extracted', axis=1)
        
        if corrections_made > 0:
            self.logger.info(f"üîß Feature {existing_col}: {corrections_made} corre√ß√µes aplicadas")
        else:
            self.logger.info(f"‚úÖ Feature {existing_col}: valida√ß√£o OK, sem corre√ß√µes necess√°rias")
        
        return df
    
    def _extract_new_feature(self, df: pd.DataFrame, feature_name: str, patterns: list, text_col: str) -> pd.DataFrame:
        """Extrair nova feature do texto."""
        def extract_feature(text):
            if pd.isna(text):
                return []
            
            text = str(text)
            extracted = []
            
            if patterns:  # Features com padr√£o regex
                for pattern in patterns:
                    matches = re.findall(pattern, text, re.IGNORECASE)
                    extracted.extend(matches)
            else:  # Channel name - tentar extrair de metadados ou usar valor padr√£o
                return "unknown_channel"
            
            return list(set(extracted)) if extracted else []
        
        # Extrair feature
        df[feature_name] = df[text_col].apply(extract_feature)
        
        # Estat√≠sticas
        non_empty = df[feature_name].apply(lambda x: len(x) > 0 if isinstance(x, list) else bool(x)).sum()
        total = len(df)
        
        self.logger.info(f"üìä Feature {feature_name}: {non_empty}/{total} registros ({non_empty/total*100:.1f}%)")
        
        return df

    def _stage_07_linguistic_processing(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        STAGE 03: Processamento lingu√≠stico com spaCy (LOGO AP√ìS limpeza).

        USA: normalized_text do Stage 02
        GERA: tokens, lemmas, POS tags, entidades nomeadas
        """
        self.logger.info("üî§ STAGE 03: Linguistic Processing (spaCy)")

        if not SPACY_AVAILABLE:
            self.logger.warning("‚ö†Ô∏è spaCy n√£o dispon√≠vel - usando processamento b√°sico")
            return self._linguistic_fallback(df)

        def process_text_with_spacy(text):
            """Processar texto com spaCy otimizado."""
            if pd.isna(text) or len(str(text).strip()) == 0:
                return {
                    'tokens': [],
                    'lemmas': [],
                    'pos_tags': [],
                    'entities': [],
                    'tokens_count': 0,
                    'entities_count': 0
                }

            try:
                # Processar texto (limitando para performance)
                doc = nlp(str(text)[:1000])  # Limitar a 1000 chars para performance

                tokens = [token.text for token in doc if not token.is_space]
                lemmas = [token.lemma_ for token in doc if not token.is_space and token.lemma_ != '-PRON-']
                entities = [(ent.text, ent.label_) for ent in doc.ents]

                return {
                    'tokens': tokens,
                    'lemmas': lemmas,
                    'tokens_count': len(tokens),
                    'entities_count': len(entities)
                }
            except Exception as e:
                self.logger.warning(f"Erro spaCy: {e}")
                return {
                    'tokens': [],
                    'lemmas': [],
                    'tokens_count': 0,
                    'entities_count': 0
                }

        # Processar textos normalizados
        spacy_results = df['normalized_text'].apply(process_text_with_spacy)

        # Extrair dados do spaCy (removidos pos_tags e entities - n√£o utilizados)
        df['spacy_tokens'] = spacy_results.apply(lambda x: x['tokens'])
        df['spacy_lemmas'] = spacy_results.apply(lambda x: x['lemmas'])
        df['spacy_tokens_count'] = spacy_results.apply(lambda x: x['tokens_count'])
        df['spacy_entities_count'] = spacy_results.apply(lambda x: x['entities_count'])

        # Criar texto processado com lemmas para stages posteriores
        df['lemmatized_text'] = df['spacy_lemmas'].apply(lambda x: ' '.join(x) if x else '')

        self.stats['stages_completed'] += 1
        self.stats['features_extracted'] += 4

        avg_tokens = df['spacy_tokens_count'].mean()
        avg_entities = df['spacy_entities_count'].mean()
        self.logger.info(f"‚úÖ spaCy processado: {avg_tokens:.1f} tokens, {avg_entities:.1f} entidades m√©dia")
        return df

    def _linguistic_fallback(self, df: pd.DataFrame) -> pd.DataFrame:
        """Fallback b√°sico quando spaCy n√£o est√° dispon√≠vel."""
        # Tokeniza√ß√£o b√°sica
        df['spacy_tokens'] = df['normalized_text'].str.split()
        df['spacy_tokens_count'] = df['spacy_tokens'].str.len()
        df['spacy_entities_count'] = 0
        df['lemmatized_text'] = df['normalized_text']  # Usar texto normalizado

        self.stats['stages_completed'] += 1
        self.stats['features_extracted'] += 3
        return df

    def _stage_03_cross_dataset_deduplication(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        STAGE 03: Cross-Dataset Deduplication
        
        Elimina√ß√£o de duplicatas entre TODOS os datasets com contador de frequ√™ncia.
        Algoritmo: Agrupar por texto id√™ntico, manter registro mais antigo, 
        contar duplicatas com dupli_freq.
        
        Redu√ß√£o esperada: 40-50% (300k ‚Üí 180k)
        """
        try:
            self.logger.info("üîÑ STAGE 03: Cross-Dataset Deduplication")
            
            text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
            datetime_column = 'datetime' if 'datetime' in df.columns else df.columns[df.columns.str.contains('date|time', case=False)].tolist()[0] if any(df.columns.str.contains('date|time', case=False)) else None
            
            initial_count = len(df)
            self.logger.info(f"üìä Registros iniciais: {initial_count:,}")
            
            # Agrupar por texto id√™ntico
            grouping_columns = [text_column]
            
            # Preparar dados para agrupamento
            dedup_data = []
            
            for text, group in df.groupby(text_column):
                if pd.isna(text) or text.strip() == '':
                    continue
                    
                # Manter registro mais antigo (primeiro datetime)
                if datetime_column and datetime_column in group.columns:
                    # Converter datetime para ordena√ß√£o
                    group_sorted = group.copy()
                    if group_sorted[datetime_column].dtype == 'object':
                        try:
                            group_sorted['datetime_parsed'] = pd.to_datetime(group_sorted[datetime_column], 
                                                                           format='%d/%m/%Y %H:%M:%S', errors='coerce')
                        except:
                            group_sorted['datetime_parsed'] = pd.to_datetime(group_sorted[datetime_column], errors='coerce')
                        
                        # Ordenar por datetime e pegar o mais antigo
                        oldest_record = group_sorted.sort_values('datetime_parsed').iloc[0]
                    else:
                        oldest_record = group.iloc[0]
                else:
                    oldest_record = group.iloc[0]
                
                # Contador de duplicatas
                dupli_freq = len(group)
                
                # Metadados de dispers√£o
                channels_found = []
                if 'channel' in group.columns:
                    channels_found = group['channel'].dropna().unique().tolist()
                elif 'sender_id' in group.columns:
                    channels_found = group['sender_id'].dropna().unique().tolist()
                
                # Per√≠odo de ocorr√™ncia
                date_span_days = 0
                if datetime_column and datetime_column in group.columns:
                    try:
                        dates = pd.to_datetime(group[datetime_column], errors='coerce').dropna()
                        if len(dates) > 1:
                            date_span_days = (dates.max() - dates.min()).days
                    except:
                        pass
                
                # Criar registro deduplificado
                dedup_record = oldest_record.copy()
                dedup_record['dupli_freq'] = dupli_freq
                dedup_record['channels_found'] = len(channels_found)
                dedup_record['date_span_days'] = date_span_days
                
                dedup_data.append(dedup_record)
            
            # Criar DataFrame deduplificado
            if dedup_data:
                df_deduplicated = pd.DataFrame(dedup_data)
                df_deduplicated = df_deduplicated.reset_index(drop=True)
            else:
                df_deduplicated = df.copy()
                df_deduplicated['dupli_freq'] = 1
                df_deduplicated['channels_found'] = 0
                df_deduplicated['date_span_days'] = 0
            
            final_count = len(df_deduplicated)
            reduction_pct = ((initial_count - final_count) / initial_count * 100) if initial_count > 0 else 0
            
            # Estat√≠sticas de deduplica√ß√£o
            unique_texts = df_deduplicated['dupli_freq'].value_counts().sort_index()
            total_duplicates = df_deduplicated[df_deduplicated['dupli_freq'] > 1]['dupli_freq'].sum()
            
            self.logger.info(f"‚úÖ Deduplica√ß√£o conclu√≠da:")
            self.logger.info(f"   üìâ {initial_count:,} ‚Üí {final_count:,} registros")
            self.logger.info(f"   üìä Redu√ß√£o: {reduction_pct:.1f}%")
            self.logger.info(f"   üîÑ Duplicatas processadas: {total_duplicates:,}")
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 3
            
            return df_deduplicated
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 03: {e}")
            self.stats['processing_errors'] += 1
            # Em caso de erro, adicionar colunas padr√£o
            df['dupli_freq'] = 1
            df['channels_found'] = 0
            df['date_span_days'] = 0
            return df

    def _stage_04_statistical_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        STAGE 04: Statistical Analysis
        
        Comparar in√≠cio do dataset com o dataset reduzido.
        Gerar estat√≠sticas para classifica√ß√£o e gr√°ficos.
        
        Processamentos:
        - Contagem de dados antes e depois
        - Propor√ß√£o de duplicadas
        - Propor√ß√£o de hashtags
        - Detec√ß√£o de repeti√ß√µes excessivas para tabela com 10 principais casos
        """
        try:
            self.logger.info("üìä STAGE 04: Statistical Analysis")
            
            text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
            
            # === AN√ÅLISE DE DUPLICA√á√ÉO ===
            total_registros = len(df)
            registros_unicos = len(df[df['dupli_freq'] == 1])
            registros_duplicados = total_registros - registros_unicos
            
            duplicacao_pct = (registros_duplicados / total_registros * 100) if total_registros > 0 else 0
            
            # === AN√ÅLISE DE HASHTAGS ===
            has_hashtags = 0
            if 'has_hashtags' in df.columns:
                has_hashtags = df['has_hashtags'].sum()
            elif text_column in df.columns:
                has_hashtags = df[text_column].str.contains('#', na=False).sum()
            
            hashtag_pct = (has_hashtags / total_registros * 100) if total_registros > 0 else 0
            
            # === TOP 10 REPETI√á√ïES EXCESSIVAS ===
            top_duplicates = df[df['dupli_freq'] > 1].nlargest(10, 'dupli_freq')[
                [text_column, 'dupli_freq', 'channels_found', 'date_span_days']
            ].to_dict('records')
            
            # === ESTAT√çSTICAS B√ÅSICAS DE TEXTO ===
            if text_column in df.columns:
                char_counts = df[text_column].str.len().fillna(0)
                word_counts = df[text_column].str.split().str.len().fillna(0)
                
                df['char_count'] = char_counts
                df['word_count'] = word_counts
                
                avg_chars = char_counts.mean()
                avg_words = word_counts.mean()
            else:
                avg_chars = 0
                avg_words = 0
                df['char_count'] = 0
                df['word_count'] = 0
            
            # === PROPOR√á√ïES DE QUALIDADE ===
            if text_column in df.columns:
                df['emoji_ratio'] = df[text_column].apply(self._calculate_emoji_ratio)
                df['caps_ratio'] = df[text_column].apply(self._calculate_caps_ratio)
                df['repetition_ratio'] = df[text_column].apply(self._calculate_repetition_ratio)
                
                # Detec√ß√£o de idioma b√°sica
                df['likely_portuguese'] = df[text_column].apply(self._detect_portuguese)
            else:
                df['emoji_ratio'] = 0.0
                df['caps_ratio'] = 0.0
                df['repetition_ratio'] = 0.0
                df['likely_portuguese'] = True
            
            # === CONSOLIDA√á√ÉO DE ESTAT√çSTICAS ===
            # Consolidar estat√≠sticas globais em objeto summary
            summary_stats = {
                'total_dataset_size': total_registros,
                'unique_texts_count': registros_unicos,
                'duplication_percentage': round(duplicacao_pct, 2),
                'hashtag_percentage': round(hashtag_pct, 2),
                'avg_chars_per_text': round(avg_chars, 1),
                'avg_words_per_text': round(avg_words, 1)
            }

            # Salvar no contexto para acesso posterior
            self.global_stats = summary_stats
            
            # Log das estat√≠sticas
            self.logger.info(f"‚úÖ An√°lise estat√≠stica conclu√≠da:")
            self.logger.info(f"   üìä Total de registros: {total_registros:,}")
            self.logger.info(f"   üîÑ Duplica√ß√£o: {duplicacao_pct:.1f}%")
            self.logger.info(f"   # Hashtags: {hashtag_pct:.1f}%")
            self.logger.info(f"   üìù M√©dia: {avg_words:.1f} palavras, {avg_chars:.0f} chars")
            
            if top_duplicates:
                self.logger.info(f"   üîù Maior repeti√ß√£o: {top_duplicates[0]['dupli_freq']} ocorr√™ncias")
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 11
            
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 04: {e}")
            self.stats['processing_errors'] += 1
            return df

    def _stage_05_content_quality_filter(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        STAGE 05: Content Quality Filter
        
        Filtrar conte√∫do por qualidade e completude.
        Input: Dados deduplificados
        Output: Apenas conte√∫do de qualidade
        
        Filtros:
        - Comprimento: < 10 chars ou > 2000 chars
        - Qualidade: emoji_ratio > 70%, caps_ratio > 80%, repetition_ratio > 50%
        - Idioma: Manter apenas likely_portuguese = True
        
        Redu√ß√£o esperada: 15-25% (180k ‚Üí 135k)
        """
        try:
            self.logger.info("üéØ STAGE 05: Content Quality Filter")
            
            text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
            initial_count = len(df)
            
            # === FILTROS DE COMPRIMENTO ===
            # Muito curto: < 10 chars (s√≥ emoji/URL)
            length_filter = (df['char_count'] >= 10) & (df['char_count'] <= 2000)
            
            # === FILTROS DE QUALIDADE ===
            # emoji_ratio > 70% = ru√≠do
            emoji_filter = df['emoji_ratio'] <= 0.70
            
            # caps_ratio > 80% = spam  
            caps_filter = df['caps_ratio'] <= 0.80
            
            # repetition_ratio > 50% = baixa qualidade
            repetition_filter = df['repetition_ratio'] <= 0.50
            
            # === FILTROS DE IDIOMA ===
            # Manter apenas likely_portuguese = True
            language_filter = df['likely_portuguese'] == True
            
            # === APLICAR TODOS OS FILTROS ===
            quality_mask = length_filter & emoji_filter & caps_filter & repetition_filter & language_filter
            
            # === GERAR COLUNAS DE QUALIDADE ===
            # Contar problemas por tipo para logging
            problems = {
                'length_issue': (~length_filter).sum(),
                'excessive_emojis': (~emoji_filter).sum(),
                'excessive_caps': (~caps_filter).sum(),
                'excessive_repetition': (~repetition_filter).sum(),
                'non_portuguese': (~language_filter).sum()
            }
            
            # Content quality score (0-100)
            quality_components = [
                length_filter.astype(int) * 20,  # 20 pontos para comprimento adequado
                emoji_filter.astype(int) * 20,   # 20 pontos para emojis adequados
                caps_filter.astype(int) * 20,    # 20 pontos para caps adequados
                repetition_filter.astype(int) * 20, # 20 pontos para repeti√ß√£o adequada
                language_filter.astype(int) * 20    # 20 pontos para portugu√™s
            ]
            
            df['content_quality_score'] = sum(quality_components)
            
            # === APLICAR FILTRO ===
            df_filtered = df[quality_mask].copy().reset_index(drop=True)
            
            final_count = len(df_filtered)
            reduction_pct = ((initial_count - final_count) / initial_count * 100) if initial_count > 0 else 0
            
            # === ESTAT√çSTICAS DOS FILTROS ===
            avg_quality_score = df_filtered['content_quality_score'].mean()

            self.logger.info(f"‚úÖ Filtro de qualidade aplicado:")
            self.logger.info(f"   üìâ {initial_count:,} ‚Üí {final_count:,} registros")
            self.logger.info(f"   üìä Redu√ß√£o: {reduction_pct:.1f}%")
            self.logger.info(f"   üéØ Score qualidade m√©dio: {avg_quality_score:.1f}/100")
            self.logger.info(f"   ‚ùå Rejeitados: comprimento={problems['length_issue']}, emojis={problems['excessive_emojis']}")
            self.logger.info(f"      caps={problems['excessive_caps']}, repeti√ß√£o={problems['excessive_repetition']}, idioma={problems['non_portuguese']}")

            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 1
            
            return df_filtered
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 05: {e}")
            self.stats['processing_errors'] += 1
            # Em caso de erro, retornar dados originais com colunas padr√£o
            df['content_quality_score'] = 80
            return df

    def _stage_06_affordances_classification(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        STAGE 06: Affordances Classification

        Classificar mensagens usando API Anthropic em categorias de affordances:
        - noticia: Conte√∫do informativo, not√≠cias, fatos
        - midia_social: Posts, compartilhamentos, intera√ß√µes sociais
        - video_audio_gif: Conte√∫do multim√≠dia
        - opiniao: Opini√µes, an√°lises, coment√°rios pessoais
        - mobilizacao: Chamadas para a√ß√£o, mobiliza√ß√£o pol√≠tica
        - ataque: Ataques pessoais, insultos, agress√µes verbais
        - interacao: Respostas, men√ß√µes, conversa√ß√µes
        - is_forwarded: Conte√∫do encaminhado/repassado

        Usa Zero-Shot Analysis com Claude API.
        """
        try:
            self.logger.info("üéØ STAGE 06: Affordances Classification")

            # Importar bibliotecas necess√°rias
            import os
            import requests
            import json
            import time
            from typing import List, Dict, Any

            # Verificar configura√ß√£o da API
            api_key = os.getenv('ANTHROPIC_API_KEY')
            if not api_key:
                self.logger.error("‚ùå ANTHROPIC_API_KEY n√£o encontrada. Pulando classifica√ß√£o por IA.")
                # Aplicar classifica√ß√£o heur√≠stica como fallback
                return self._stage_06_affordances_heuristic_fallback(df)

            text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
            initial_count = len(df)

            # === CONFIGURA√á√ÉO DA API ===
            api_config = {
                'model': 'claude-3-5-haiku-20241022',  # Modelo mais econ√¥mico
                'max_tokens': 150,
                'temperature': 0.1,
                'system_prompt': """Voc√™ √© um classificador de conte√∫do especializado em discurso pol√≠tico brasileiro em redes sociais.

Classifique cada mensagem de acordo com as seguintes categorias de affordances (m√∫ltiplas categorias s√£o poss√≠veis):

1. noticia: Conte√∫do informativo, reportagem, fatos, acontecimentos
2. midia_social: Posts t√≠picos de redes sociais, compartilhamentos casuais
3. video_audio_gif: Refer√™ncias a conte√∫do multim√≠dia (v√≠deo, √°udio, gif)
4. opiniao: Opini√µes pessoais, an√°lises, coment√°rios subjetivos
5. mobilizacao: Chamadas para a√ß√£o, convoca√ß√µes, mobiliza√ß√£o pol√≠tica
6. ataque: Ataques pessoais, insultos, agress√µes verbais
7. interacao: Respostas, men√ß√µes, conversa√ß√µes diretas
8. is_forwarded: Indica se o conte√∫do parece ser encaminhado/repassado

Responda APENAS com um JSON v√°lido no formato:
{"categorias": ["categoria1", "categoria2"], "confianca": 0.85}

Onde confianca √© um valor entre 0.0 e 1.0."""
            }

            # === FUN√á√ÉO DE CLASSIFICA√á√ÉO POR API ===
            def classify_with_anthropic(text: str) -> Dict[str, Any]:
                """Classificar texto usando API Anthropic."""
                if pd.isna(text) or len(str(text).strip()) < 10:
                    return {"categorias": [], "confianca": 0.0}

                # Limitar texto para economizar tokens
                text_sample = str(text)[:500]

                headers = {
                    'Content-Type': 'application/json',
                    'x-api-key': api_key,
                    'anthropic-version': '2023-06-01'
                }

                payload = {
                    'model': api_config['model'],
                    'max_tokens': api_config['max_tokens'],
                    'temperature': api_config['temperature'],
                    'system': api_config['system_prompt'],
                    'messages': [
                        {
                            'role': 'user',
                            'content': f'Classifique esta mensagem: "{text_sample}"'
                        }
                    ]
                }

                try:
                    response = requests.post(
                        'https://api.anthropic.com/v1/messages',
                        headers=headers,
                        json=payload,
                        timeout=30
                    )

                    if response.status_code == 200:
                        result = response.json()
                        content = result['content'][0]['text'].strip()

                        # Parse do JSON retornado
                        try:
                            classification = json.loads(content)
                            if isinstance(classification, dict) and 'categorias' in classification:
                                return classification
                        except json.JSONDecodeError:
                            # Tentar extrair JSON de resposta n√£o estruturada
                            if '{' in content and '}' in content:
                                json_start = content.find('{')
                                json_end = content.rfind('}') + 1
                                try:
                                    classification = json.loads(content[json_start:json_end])
                                    if isinstance(classification, dict) and 'categorias' in classification:
                                        return classification
                                except:
                                    pass

                    # Rate limiting
                    elif response.status_code == 429:
                        self.logger.warning("‚ö†Ô∏è Rate limit atingido, aguardando...")
                        time.sleep(2)
                        return {"categorias": [], "confianca": 0.0}

                    else:
                        self.logger.warning(f"‚ö†Ô∏è API error: {response.status_code}")

                except requests.RequestException as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro de conex√£o: {e}")

                # Fallback em caso de erro
                return {"categorias": [], "confianca": 0.0}

            # === PROCESSAMENTO EM LOTES ===
            self.logger.info(f"ü§ñ Classificando {initial_count} mensagens com API Anthropic...")

            batch_size = 50  # Processar em lotes pequenos
            results = []
            api_calls_made = 0
            successful_classifications = 0

            for i in range(0, len(df), batch_size):
                batch = df.iloc[i:i+batch_size]
                batch_results = []

                for idx, row in batch.iterrows():
                    text = row[text_column]
                    classification = classify_with_anthropic(text)

                    batch_results.append(classification)
                    api_calls_made += 1

                    if classification['confianca'] > 0:
                        successful_classifications += 1

                    # Rate limiting
                    time.sleep(0.1)  # 100ms entre requests

                results.extend(batch_results)

                # Log de progresso
                progress = min(100, ((i + batch_size) / len(df)) * 100)
                self.logger.info(f"   üîÑ Progresso: {progress:.1f}% ({api_calls_made} calls, {successful_classifications} sucessos)")

            # === APLICAR RESULTADOS ===
            df['affordance_categories'] = [result['categorias'] for result in results]
            df['affordance_confidence'] = [result['confianca'] for result in results]

            # Adicionar colunas bin√°rias para cada categoria
            affordance_types = ['noticia', 'midia_social', 'video_audio_gif', 'opiniao',
                              'mobilizacao', 'ataque', 'interacao', 'is_forwarded']

            for affordance_type in affordance_types:
                df[f'aff_{affordance_type}'] = df['affordance_categories'].apply(
                    lambda cats: 1 if affordance_type in cats else 0
                )

            # === ESTAT√çSTICAS ===
            avg_confidence = df['affordance_confidence'].mean()
            classified_count = len(df[df['affordance_confidence'] > 0])

            # Contagem por categoria
            category_counts = {}
            for affordance_type in affordance_types:
                count = df[f'aff_{affordance_type}'].sum()
                category_counts[affordance_type] = count

            self.logger.info(f"‚úÖ Classifica√ß√£o de Affordances conclu√≠da:")
            self.logger.info(f"   üìä {api_calls_made} chamadas API realizadas")
            self.logger.info(f"   ‚úÖ {successful_classifications}/{initial_count} mensagens classificadas")
            self.logger.info(f"   üéØ Confian√ßa m√©dia: {avg_confidence:.3f}")
            self.logger.info(f"   üìà Taxa de sucesso: {(successful_classifications/initial_count)*100:.1f}%")

            # Top 5 categorias
            top_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:5]
            self.logger.info(f"   üîù Top categorias: {dict(top_categories)}")

            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += len(affordance_types) + 2

            return df

        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 06: {e}")
            self.stats['processing_errors'] += 1
            # Fallback heur√≠stico
            return self._stage_06_affordances_heuristic_fallback(df)

    def _stage_06_affordances_heuristic_fallback(self, df: pd.DataFrame) -> pd.DataFrame:
        """Fallback heur√≠stico para classifica√ß√£o de affordances sem API."""
        self.logger.info("üîÑ Aplicando classifica√ß√£o heur√≠stica de affordances...")

        text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'

        # Padr√µes heur√≠sticos baseados em palavras-chave
        patterns = {
            'noticia': ['aconteceu', 'not√≠cia', 'informa√ß√£o', 'fato', 'governo', 'brasil'],
            'midia_social': ['compartilhem', 'rt', 'retweet', 'curtir', 'like'],
            'video_audio_gif': ['v√≠deo', 'audio', 'gif', 'assista', 'ou√ßa'],
            'opiniao': ['acho', 'penso', 'na minha opini√£o', 'acredito', 'creio'],
            'mobilizacao': ['vamos', 'precisamos', 'juntos', 'a√ß√£o', 'mobilizar'],
            'ataque': ['idiota', 'burro', 'canalha', 'corrupto', 'mentiroso'],
            'interacao': ['@', 'resposta', 'pergunta', 'd√∫vida'],
            'is_forwarded': ['encaminhado', 'forward', 'repasse', 'compartilhe']
        }

        # Aplicar classifica√ß√£o heur√≠stica
        for affordance_type, keywords in patterns.items():
            df[f'aff_{affordance_type}'] = df[text_column].apply(
                lambda text: 1 if any(kw in str(text).lower() for kw in keywords) else 0
            )

        # Colunas de compatibilidade
        df['affordance_categories'] = [[] for _ in range(len(df))]
        df['affordance_confidence'] = 0.5  # Confian√ßa baixa para heur√≠stica

        self.logger.info("‚úÖ Classifica√ß√£o heur√≠stica aplicada como fallback")
        return df

    # ===============================================
    # M√âTODOS HELPER PARA AN√ÅLISE DE QUALIDADE
    # ===============================================
    
    def _calculate_emoji_ratio(self, text: str) -> float:
        """Calcular propor√ß√£o de emojis no texto."""
        if pd.isna(text) or len(text) == 0:
            return 0.0
        
        import re
        # Regex para detectar emojis Unicode
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F1E0-\U0001F1FF"  # flags (iOS)
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "]+",
            flags=re.UNICODE
        )
        
        emojis = emoji_pattern.findall(str(text))
        emoji_count = sum(len(emoji) for emoji in emojis)
        
        return min(1.0, emoji_count / len(str(text)))
    
    def _calculate_caps_ratio(self, text: str) -> float:
        """Calcular propor√ß√£o de letras mai√∫sculas."""
        if pd.isna(text) or len(text) == 0:
            return 0.0
        
        text_str = str(text)
        letters = [c for c in text_str if c.isalpha()]
        
        if len(letters) == 0:
            return 0.0
        
        caps_count = sum(1 for c in letters if c.isupper())
        return caps_count / len(letters)
    
    def _calculate_repetition_ratio(self, text: str) -> float:
        """Calcular propor√ß√£o de caracteres repetitivos."""
        if pd.isna(text) or len(text) <= 1:
            return 0.0
        
        text_str = str(text).lower()
        
        # Contar sequ√™ncias repetitivas (3+ caracteres iguais)
        repetition_count = 0
        current_char = ''
        current_count = 1
        
        for char in text_str:
            if char == current_char:
                current_count += 1
                if current_count >= 3:
                    repetition_count += 1
            else:
                current_char = char
                current_count = 1
        
        return min(1.0, repetition_count / len(text_str))
    
    def _detect_portuguese(self, text: str) -> bool:
        """Detec√ß√£o b√°sica de idioma portugu√™s."""
        if pd.isna(text) or len(text) < 10:
            return True  # Assumir portugu√™s para textos muito curtos
        
        text_lower = str(text).lower()
        
        # Palavras comuns em portugu√™s
        portuguese_indicators = [
            'que', 'n√£o', 'com', 'uma', 'para', 's√£o', 'por', 'mais', 'das', 'dos',
            'mas', 'foi', 'pela', 'at√©', 'isso', 'ela', 'entre', 'depois', 'sem',
            'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'voc√™',
            'j√°', 'eu', 'tamb√©m', 's√≥', 'pelo', 'nos', '√©', 'o', 'a', 'de', 'do',
            'da', 'em', 'um', 'para', '√©', 'com', 'n√£o', 'uma', 'os', 'no', 'se',
            'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'foi', 'ao', 'ele',
            'das', 'tem', '√†', 'seu', 'sua', 'ou', 'ser', 'quando', 'muito', 'h√°',
            'nos', 'j√°', 'est√°', 'eu', 'tamb√©m', 's√≥', 'pelo', 'pela', 'at√©'
        ]
        
        # Contar palavras portuguesas encontradas
        words = text_lower.split()
        portuguese_count = sum(1 for word in words if word in portuguese_indicators)
        
        # Considerar portugu√™s se >= 20% das palavras s√£o indicadores
        if len(words) > 0:
            portuguese_ratio = portuguese_count / len(words)
            return portuguese_ratio >= 0.2
        
        return True  # Default para portugu√™s

    @validate_stage_dependencies(required_columns=['normalized_text'], required_attrs=['political_lexicon'])
    def _stage_08_political_classification(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 05: Classifica√ß√£o pol√≠tica brasileira.
        
        Aplica l√©xico pol√≠tico brasileiro para classificar textos em:
        - extrema-direita, direita, centro-direita, centro, centro-esquerda, esquerda
        """
        try:
            self.logger.info("üîÑ Stage 05: Classifica√ß√£o pol√≠tica brasileira")
            
            if 'normalized_text' not in df.columns:
                self.logger.warning("‚ö†Ô∏è normalized_text n√£o encontrado, usando body")
                text_column = 'body'
            else:
                text_column = 'normalized_text'
            
            # Classifica√ß√£o pol√≠tica b√°sica
            df['political_orientation'] = df[text_column].apply(self._classify_political_orientation)
            df['political_keywords'] = df[text_column].apply(self._extract_political_keywords)
            df['political_intensity'] = df[text_column].apply(self._calculate_political_intensity)
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 3
            
            self.logger.info(f"‚úÖ Stage 05 conclu√≠do: {len(df)} registros processados")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 05: {e}")
            self.stats['processing_errors'] += 1
            return df

    def _stage_09_tfidf_vectorization(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 09: Vetoriza√ß√£o TF-IDF com tokens spaCy.
        
        Calcula TF-IDF usando tokens processados pelo spaCy.
        Trata casos de vocabul√°rio vazio em chunks pequenos.
        """
        try:
            self.logger.info("üîÑ Stage 09: Vetoriza√ß√£o TF-IDF")
            
            # Verificar se h√° dados suficientes
            if len(df) < 2:
                self.logger.warning(f"‚ö†Ô∏è Dados insuficientes para TF-IDF ({len(df)} documentos), preenchendo com padr√µes")
                df['tfidf_score_mean'] = 0.0
                df['tfidf_score_max'] = 0.0
                df['tfidf_top_terms'] = [[] for _ in range(len(df))]
                self.stats['features_extracted'] += 3
                return df
            
            if 'tokens' not in df.columns:
                self.logger.warning("‚ö†Ô∏è tokens n√£o encontrados, usando normalized_text")
                text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
                text_data = df[text_column].fillna('').tolist()
            else:
                # Usar tokens do spaCy
                text_data = df['tokens'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x)).fillna('').tolist()
            
            # Verificar se h√° texto n√£o-vazio
            non_empty_texts = [text for text in text_data if text.strip()]
            if len(non_empty_texts) < 2:
                self.logger.warning(f"‚ö†Ô∏è Textos vazios demais para TF-IDF ({len(non_empty_texts)} v√°lidos), usando fallback")
                df['tfidf_score_mean'] = 0.1
                df['tfidf_score_max'] = 0.2
                df['tfidf_top_terms'] = [['texto', 'palavra'] for _ in range(len(df))]
                self.stats['features_extracted'] += 3
                return df
            
            # TF-IDF com configura√ß√£o adaptativa para chunks pequenos
            from sklearn.feature_extraction.text import TfidfVectorizer
            import numpy as np
            
            # Ajustar max_features baseado no tamanho do chunk
            chunk_size = len(df)
            if chunk_size < 50:
                max_features = min(20, chunk_size * 2)  # Muito conservador
            elif chunk_size < 200:
                max_features = min(50, chunk_size)  # Conservador
            else:
                max_features = 100  # Padr√£o
            
            vectorizer = TfidfVectorizer(
                max_features=max_features,
                min_df=1,  # Aceitar termos que aparecem pelo menos 1 vez
                stop_words=None,  # J√° removemos stopwords no spaCy
                lowercase=False,   # J√° normalizado
                token_pattern=r'\S+',  # Aceitar qualquer token n√£o-espa√ßo
                ngram_range=(1, 1)  # Apenas unigramas para chunks pequenos
            )
            
            try:
                tfidf_matrix = vectorizer.fit_transform(text_data)
                feature_names = vectorizer.get_feature_names_out()
                
                # Verificar se conseguiu gerar features
                if tfidf_matrix.shape[1] == 0:
                    raise ValueError("Vocabul√°rio vazio ap√≥s vectoriza√ß√£o")
                
                # Converter para array denso para c√°lculos
                tfidf_dense = tfidf_matrix.toarray()
                
                # Scores m√©dios por documento
                df['tfidf_score_mean'] = np.mean(tfidf_dense, axis=1)
                df['tfidf_score_max'] = np.max(tfidf_dense, axis=1)
                
                # Top terms por documento 
                top_terms_count = min(5, len(feature_names))  # Adaptar ao vocabul√°rio dispon√≠vel
                df['tfidf_top_terms'] = [
                    [feature_names[i] for i in row.argsort()[::-1][:top_terms_count] if row[i] > 0]
                    for row in tfidf_dense
                ]
                
                self.logger.info(f"‚úÖ TF-IDF: {len(feature_names)} features, max_features={max_features}")
                
            except (ValueError, Exception) as ve:
                self.logger.warning(f"‚ö†Ô∏è Erro na vectoriza√ß√£o TF-IDF: {ve}, usando fallback simples")
                
                # Fallback: an√°lise simples baseada em frequ√™ncia de palavras
                import re
                from collections import Counter
                
                all_words = []
                for text in text_data:
                    if text and text.strip():
                        # Extrair palavras simples
                        words = re.findall(r'\w+', text.lower())
                        words = [w for w in words if len(w) > 2]  # Filtrar palavras muito curtas
                        all_words.extend(words)
                
                if all_words:
                    word_freq = Counter(all_words)
                    common_words = [word for word, _ in word_freq.most_common(10)]
                    
                    # Scores baseados em presen√ßa de palavras comuns
                    df['tfidf_score_mean'] = [
                        len([w for w in re.findall(r'\w+', str(text).lower()) if w in common_words]) / max(1, len(common_words)) * 0.5
                        for text in text_data
                    ]
                    df['tfidf_score_max'] = df['tfidf_score_mean'] * 1.5
                    df['tfidf_top_terms'] = [
                        [w for w in re.findall(r'\w+', str(text).lower()) if w in common_words][:5]
                        for text in text_data
                    ]
                else:
                    # √öltima op√ß√£o: valores padr√£o
                    df['tfidf_score_mean'] = 0.1
                    df['tfidf_score_max'] = 0.2
                    df['tfidf_top_terms'] = [[] for _ in range(len(df))]
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 3
            
            self.logger.info(f"‚úÖ Stage 09 conclu√≠do: {len(df)} registros processados")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 09: {e}")
            self.stats['processing_errors'] += 1
            
            # Valores padr√£o em caso de erro
            df['tfidf_score_mean'] = 0.0
            df['tfidf_score_max'] = 0.0
            df['tfidf_top_terms'] = [[] for _ in range(len(df))]
            self.stats['features_extracted'] += 3
            return df

    @validate_stage_dependencies(required_columns=['tfidf_score_mean'], required_attrs=['tfidf_matrix'])
    def _stage_10_clustering_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 07: An√°lise de clustering baseado em features lingu√≠sticas.
        
        Agrupa documentos similares usando caracter√≠sticas extra√≠das.
        """
        try:
            self.logger.info("üîÑ Stage 07: An√°lise de clustering")
            
            # Features num√©ricas para clustering
            numeric_features = []
            for col in ['word_count', 'text_length', 'tfidf_score_mean', 'political_intensity']:
                if col in df.columns:
                    numeric_features.append(col)
            
            if len(numeric_features) < 2:
                self.logger.warning("‚ö†Ô∏è Features insuficientes para clustering")
                df['cluster_id'] = 0
                df['cluster_distance'] = 0.0
                df['cluster_size'] = len(df)
            else:
                from sklearn.cluster import KMeans
                from sklearn.preprocessing import StandardScaler
                
                # Preparar dados
                feature_data = df[numeric_features].fillna(0)
                scaler = StandardScaler()
                scaled_data = scaler.fit_transform(feature_data)
                
                # Clustering simples
                n_clusters = min(5, len(df) // 10 + 1)
                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                clusters = kmeans.fit_predict(scaled_data)
                
                df['cluster_id'] = clusters
                df['cluster_distance'] = [
                    min(((scaled_data[i] - center) ** 2).sum() for center in kmeans.cluster_centers_)
                    for i in range(len(scaled_data))
                ]
                
                # Tamanho dos clusters
                cluster_sizes = pd.Series(clusters).value_counts().to_dict()
                df['cluster_size'] = df['cluster_id'].map(cluster_sizes)
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 3
            
            self.logger.info(f"‚úÖ Stage 07 conclu√≠do: {len(df)} registros processados")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 07: {e}")
            self.stats['processing_errors'] += 1
            return df

    def _stage_11_topic_modeling(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 08: Topic modeling com embeddings.
        
        Descoberta autom√°tica de t√≥picos nos textos.
        """
        try:
            self.logger.info("üîÑ Stage 08: Topic modeling")
            
            if 'tokens' not in df.columns:
                self.logger.warning("‚ö†Ô∏è tokens n√£o encontrados, usando normalized_text")
                text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
                text_data = df[text_column].fillna('').tolist()
            else:
                text_data = df['tokens'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x)).fillna('').tolist()
            
            # Topic modeling b√°sico com LDA
            from sklearn.feature_extraction.text import CountVectorizer
            from sklearn.decomposition import LatentDirichletAllocation
            
            # Preparar dados
            vectorizer = CountVectorizer(max_features=50, stop_words=None)
            doc_term_matrix = vectorizer.fit_transform(text_data)
            
            # LDA simples
            n_topics = min(5, len(df) // 20 + 1)
            lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
            doc_topic_matrix = lda.fit_transform(doc_term_matrix)
            
            # T√≥pico dominante para cada documento
            df['dominant_topic'] = doc_topic_matrix.argmax(axis=1)
            df['topic_probability'] = doc_topic_matrix.max(axis=1)
            
            # Palavras-chave dos t√≥picos
            feature_names = vectorizer.get_feature_names_out()
            topic_keywords = []
            for topic_idx, topic in enumerate(lda.components_):
                top_words = [feature_names[i] for i in topic.argsort()[::-1][:3]]
                topic_keywords.append(top_words)
            
            df['topic_keywords'] = df['dominant_topic'].apply(lambda x: topic_keywords[x] if x < len(topic_keywords) else [])
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 3
            
            self.logger.info(f"‚úÖ Stage 08 conclu√≠do: {len(df)} registros processados")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 08: {e}")
            self.stats['processing_errors'] += 1
            return df

    @validate_stage_dependencies(required_columns=['normalized_text'])
    def _stage_13_temporal_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 09: An√°lise temporal.
        
        Extrai padr√µes temporais dos timestamps.
        """
        try:
            self.logger.info("üîÑ Stage 09: An√°lise temporal")
            
            if 'datetime' not in df.columns:
                self.logger.warning("‚ö†Ô∏è datetime n√£o encontrado")
                df['hour'] = 12
                df['day_of_week'] = 1
                df['month'] = 1
            else:
                # Converter datetime para an√°lise temporal
                try:
                    datetime_series = pd.to_datetime(df['datetime'], format='%d/%m/%Y %H:%M:%S', errors='coerce')
                    
                    df['hour'] = datetime_series.dt.hour
                    df['day_of_week'] = datetime_series.dt.dayofweek
                    df['month'] = datetime_series.dt.month
                    df['year'] = datetime_series.dt.year
                    df['day_of_year'] = datetime_series.dt.dayofyear
                    
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro convers√£o datetime: {e}")
                    df['hour'] = 12
                    df['day_of_week'] = 1
                    df['month'] = 1
                    df['year'] = 2020
                    df['day_of_year'] = 1
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 5
            
            self.logger.info(f"‚úÖ Stage 09 conclu√≠do: {len(df)} registros processados")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 09: {e}")
            self.stats['processing_errors'] += 1
            return df

    def _stage_14_network_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 10: An√°lise de rede (coordena√ß√£o e padr√µes).
        
        Detecta padr√µes de coordena√ß√£o e comportamento de rede.
        """
        try:
            self.logger.info("üîÑ Stage 10: An√°lise de rede")
            
            # An√°lise de coordena√ß√£o b√°sica
            if 'sender' in df.columns:
                sender_counts = df['sender'].value_counts()
                df['sender_frequency'] = df['sender'].map(sender_counts)
                df['is_frequent_sender'] = df['sender_frequency'] > df['sender_frequency'].median()
            else:
                df['sender_frequency'] = 1
                df['is_frequent_sender'] = False
            
            # An√°lise de URLs compartilhadas
            if 'urls_extracted' in df.columns:
                # URLs mais compartilhadas
                all_urls = []
                for urls in df['urls_extracted'].fillna('[]'):
                    if isinstance(urls, str):
                        try:
                            url_list = eval(urls) if urls.startswith('[') else [urls]
                            all_urls.extend(url_list)
                        except:
                            pass
                
                url_counts = pd.Series(all_urls).value_counts()
                df['shared_url_frequency'] = df['urls_extracted'].apply(
                    lambda x: max([url_counts.get(url, 0) for url in (eval(x) if isinstance(x, str) and x.startswith('[') else [])], default=0)
                )
            else:
                df['shared_url_frequency'] = 0
            
            # Coordena√ß√£o temporal (mensagens em hor√°rios similares)
            if 'hour' in df.columns:
                hour_counts = df['hour'].value_counts()
                df['temporal_coordination'] = df['hour'].map(hour_counts) / len(df)
            else:
                df['temporal_coordination'] = 0.0
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 4
            
            self.logger.info(f"‚úÖ Stage 10 conclu√≠do: {len(df)} registros processados")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 10: {e}")
            self.stats['processing_errors'] += 1
            return df

    def _stage_15_domain_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 11: An√°lise de dom√≠nios.
        
        Analisa dom√≠nios e URLs para identificar padr√µes de m√≠dia.
        """
        try:
            self.logger.info("üîÑ Stage 11: An√°lise de dom√≠nios")
            
            # An√°lise de dom√≠nios
            if 'domain' in df.columns:
                df['domain_type'] = df['domain'].apply(self._classify_domain_type)
                
                domain_counts = df['domain'].value_counts()
                df['domain_frequency'] = df['domain'].map(domain_counts)
                
                # M√≠dia mainstream vs alternativa
                mainstream_domains = ['youtube.com', 'twitter.com', 'facebook.com', 'instagram.com', 'g1.globo.com', 'folha.uol.com.br']
                df['is_mainstream_media'] = df['domain'].isin(mainstream_domains)
            else:
                df['domain_type'] = 'unknown'
                df['domain_frequency'] = 0
                df['is_mainstream_media'] = False
            
            # An√°lise de URLs
            if 'urls_extracted' in df.columns:
                df['url_count'] = df['urls_extracted'].apply(
                    lambda x: len(eval(x)) if isinstance(x, str) and x.startswith('[') else (1 if x else 0)
                )
                df['has_external_links'] = df['url_count'] > 0
            else:
                df['url_count'] = 0
                df['has_external_links'] = False
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 5
            
            self.logger.info(f"‚úÖ Stage 11 conclu√≠do: {len(df)} registros processados")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 11: {e}")
            self.stats['processing_errors'] += 1
            return df


    def _stage_12_semantic_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 12: An√°lise sem√¢ntica.
        
        An√°lise sem√¢ntica e de sentimento dos textos.
        """
        try:
            self.logger.info("üîÑ Stage 12: An√°lise sem√¢ntica")
            
            text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
            
            # An√°lise de sentimento b√°sica
            df['sentiment_polarity'] = df[text_column].apply(self._calculate_sentiment_polarity)
            df['sentiment_label'] = df['sentiment_polarity'].apply(
                lambda x: 'positive' if x > 0.1 else ('negative' if x < -0.1 else 'neutral')
            )
            
            # An√°lise de emo√ß√µes b√°sicas
            df['emotion_intensity'] = df[text_column].apply(self._calculate_emotion_intensity)
            df['has_aggressive_language'] = df[text_column].apply(self._detect_aggressive_language)
            
            # Complexidade sem√¢ntica
            if 'tokens' in df.columns:
                df['semantic_diversity'] = df['tokens'].apply(
                    lambda x: len(set(x)) / len(x) if isinstance(x, list) and len(x) > 0 else 0
                )
            else:
                df['semantic_diversity'] = df[text_column].apply(
                    lambda x: len(set(str(x).split())) / len(str(x).split()) if len(str(x).split()) > 0 else 0
                )
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 5
            
            self.logger.info(f"‚úÖ Stage 12 conclu√≠do: {len(df)} registros processados")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 12: {e}")
            self.stats['processing_errors'] += 1
            return df

    def _stage_16_event_context(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 13: An√°lise de contexto de eventos.
        
        Detecta contextos pol√≠ticos e eventos relevantes.
        """
        try:
            self.logger.info("üîÑ Stage 13: An√°lise de contexto de eventos")
            
            text_column = 'normalized_text' if 'normalized_text' in df.columns else 'body'
            
            # Contextos pol√≠ticos brasileiros
            df['political_context'] = df[text_column].apply(self._detect_political_context)
            df['mentions_government'] = df[text_column].apply(self._mentions_government)
            df['mentions_opposition'] = df[text_column].apply(self._mentions_opposition)
            
            # Eventos espec√≠ficos (elei√ß√µes, manifesta√ß√µes, etc.)
            df['election_context'] = df[text_column].apply(self._detect_election_context)
            df['protest_context'] = df[text_column].apply(self._detect_protest_context)
            
            # An√°lise temporal de eventos
            if 'datetime' in df.columns:
                df['is_weekend'] = df['day_of_week'].isin([5, 6]) if 'day_of_week' in df.columns else False
                df['is_business_hours'] = df['hour'].between(9, 17) if 'hour' in df.columns else False
            else:
                df['is_weekend'] = False
                df['is_business_hours'] = True
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 7
            
            self.logger.info(f"‚úÖ Stage 13 conclu√≠do: {len(df)} registros processados")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 13: {e}")
            self.stats['processing_errors'] += 1
            return df

    def _stage_17_channel_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Stage 14: An√°lise de canais/fontes.
        
        Classifica canais e fontes de informa√ß√£o.
        """
        try:
            self.logger.info("üîÑ Stage 14: An√°lise de canais")
            
            # An√°lise de canais
            if 'channel' in df.columns:
                df['channel_type'] = df['channel'].apply(self._classify_channel_type)
                
                channel_counts = df['channel'].value_counts()
                df['channel_activity'] = df['channel'].map(channel_counts)
                df['is_active_channel'] = df['channel_activity'] > df['channel_activity'].median()
            else:
                df['channel_type'] = 'unknown'
                df['channel_activity'] = 1
                df['is_active_channel'] = False
            
            # An√°lise de m√≠dia
            if 'media_type' in df.columns:
                df['content_type'] = df['media_type'].fillna('text')
                df['has_media'] = df['media_type'].notna()
            else:
                df['content_type'] = 'text'
                df['has_media'] = False
            
            # Padr√µes de forwarding
            if 'is_fwrd' in df.columns:
                df['is_forwarded'] = df['is_fwrd'].fillna(False)
                forwarded_ratio = df['is_forwarded'].mean()
                df['forwarding_context'] = forwarded_ratio
            else:
                df['is_forwarded'] = False
                df['forwarding_context'] = 0.0
            
            # Influ√™ncia do canal
            if 'sender' in df.columns and 'channel' in df.columns:
                sender_channel_counts = df.groupby(['sender', 'channel']).size()
                df['sender_channel_influence'] = df.apply(
                    lambda row: sender_channel_counts.get((row['sender'], row['channel']), 0), axis=1
                )
            else:
                df['sender_channel_influence'] = 1
            
            self.stats['stages_completed'] += 1
            self.stats['features_extracted'] += 7
            
            self.logger.info(f"‚úÖ Stage 14 conclu√≠do: {len(df)} registros processados")
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro Stage 14: {e}")
            self.stats['processing_errors'] += 1
            return df

    # ==========================================
    # HELPER METHODS FOR ANALYSIS STAGES
    # ==========================================

    def _classify_political_orientation(self, text: str) -> str:
        """Classifica orienta√ß√£o pol√≠tica do texto."""
        if not text or pd.isna(text):
            return 'neutral'
        
        text_lower = str(text).lower()
        
        # Palavras-chave para classifica√ß√£o pol√≠tica brasileira
        extrema_direita = ['bolsonaro', 'mito', 'capit√£o', 'comunista', 'petralha', 'globalismo']
        direita = ['conservador', 'tradicional', 'fam√≠lia', 'ordem', 'progresso']
        centro_direita = ['liberal', 'empreendedor', 'mercado', 'economia']
        centro = ['moderado', 'equilibrio', 'consenso']
        centro_esquerda = ['social', 'trabalhador', 'direitos']
        esquerda = ['lula', 'pt', 'socialismo', 'igualdade', 'justi√ßa social']
        
        scores = {
            'extrema-direita': sum(1 for word in extrema_direita if word in text_lower),
            'direita': sum(1 for word in direita if word in text_lower),
            'centro-direita': sum(1 for word in centro_direita if word in text_lower),
            'centro': sum(1 for word in centro if word in text_lower),
            'centro-esquerda': sum(1 for word in centro_esquerda if word in text_lower),
            'esquerda': sum(1 for word in esquerda if word in text_lower)
        }
        
        return max(scores.items(), key=lambda x: x[1])[0] if max(scores.values()) > 0 else 'neutral'

    def _extract_political_keywords(self, text: str) -> list:
        """Extrai palavras-chave pol√≠ticas do texto."""
        if not text or pd.isna(text):
            return []
        
        political_keywords = [
            'bolsonaro', 'lula', 'pt', 'psl', 'mdb', 'psdb',
            'elei√ß√£o', 'voto', 'democracia', 'ditadura', 'golpe',
            'esquerda', 'direita', 'conservador', 'liberal'
        ]
        
        text_lower = str(text).lower()
        found_keywords = [word for word in political_keywords if word in text_lower]
        
        return found_keywords[:5]  # M√°ximo 5 palavras-chave

    def _calculate_political_intensity(self, text: str) -> float:
        """Calcula intensidade do discurso pol√≠tico."""
        if not text or pd.isna(text):
            return 0.0
        
        intensity_words = [
            'sempre', 'nunca', 'jamais', 'obrigat√≥rio', 'proibido',
            'urgente', 'imediato', 'crucial', 'fundamental', 'essencial'
        ]
        
        text_lower = str(text).lower()
        intensity_count = sum(1 for word in intensity_words if word in text_lower)
        
        return min(intensity_count / 10.0, 1.0)  # Normalizar entre 0 e 1

    def _classify_domain_type(self, domain: str) -> str:
        """Classifica tipo de dom√≠nio."""
        if not domain or pd.isna(domain):
            return 'unknown'
        
        domain_lower = str(domain).lower()
        
        if any(x in domain_lower for x in ['youtube', 'youtu.be']):
            return 'video'
        elif any(x in domain_lower for x in ['twitter', 'facebook', 'instagram']):
            return 'social'
        elif any(x in domain_lower for x in ['globo', 'folha', 'estadao', 'uol']):
            return 'mainstream_news'
        elif any(x in domain_lower for x in ['blog', 'wordpress', 'medium']):
            return 'blog'
        else:
            return 'other'

    def _calculate_sentiment_polarity(self, text: str) -> float:
        """Calcula polaridade de sentimento b√°sica."""
        if not text or pd.isna(text):
            return 0.0
        
        positive_words = ['bom', '√≥timo', 'excelente', 'maravilhoso', 'perfeito', 'amor', 'feliz']
        negative_words = ['ruim', 'p√©ssimo', 'terr√≠vel', '√≥dio', 'raiva', 'triste', 'infeliz']
        
        text_lower = str(text).lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        total_words = len(text_lower.split())
        if total_words == 0:
            return 0.0
        
        return (positive_count - negative_count) / total_words

    def _calculate_emotion_intensity(self, text: str) -> float:
        """Calcula intensidade emocional."""
        if not text or pd.isna(text):
            return 0.0
        
        # Contagem de pontua√ß√£o emocional
        emotion_markers = text.count('!') + text.count('?') + text.count('...') 
        caps_words = sum(1 for word in str(text).split() if word.isupper() and len(word) > 2)
        
        return min((emotion_markers + caps_words) / 10.0, 1.0)

    def _detect_aggressive_language(self, text: str) -> bool:
        """Detecta linguagem agressiva."""
        if not text or pd.isna(text):
            return False
        
        aggressive_words = [
            '√≥dio', 'matar', 'destruir', 'eliminar', 'acabar',
            'burro', 'idiota', 'imbecil', 'est√∫pido'
        ]
        
        text_lower = str(text).lower()
        return any(word in text_lower for word in aggressive_words)

    def _detect_political_context(self, text: str) -> str:
        """Detecta contexto pol√≠tico."""
        if not text or pd.isna(text):
            return 'none'
        
        text_lower = str(text).lower()
        
        if any(word in text_lower for word in ['elei√ß√£o', 'voto', 'urna', 'candidato']):
            return 'electoral'
        elif any(word in text_lower for word in ['governo', 'ministro', 'presidente']):
            return 'government'
        elif any(word in text_lower for word in ['manifesta√ß√£o', 'protesto', 'greve']):
            return 'protest'
        elif any(word in text_lower for word in ['economia', 'infla√ß√£o', 'desemprego']):
            return 'economic'
        else:
            return 'general'

    def _mentions_government(self, text: str) -> bool:
        """Verifica se menciona governo."""
        if not text or pd.isna(text):
            return False
        
        government_terms = ['governo', 'presidente', 'ministro', 'secret√°rio', 'federal']
        text_lower = str(text).lower()
        
        return any(term in text_lower for term in government_terms)

    def _mentions_opposition(self, text: str) -> bool:
        """Verifica se menciona oposi√ß√£o."""
        if not text or pd.isna(text):
            return False
        
        opposition_terms = ['oposi√ß√£o', 'contra', 'resist√™ncia', 'impeachment']
        text_lower = str(text).lower()
        
        return any(term in text_lower for term in opposition_terms)

    def _detect_election_context(self, text: str) -> bool:
        """Detecta contexto eleitoral."""
        if not text or pd.isna(text):
            return False
        
        election_terms = ['elei√ß√£o', 'voto', 'urna', 'candidato', 'campanha', 'debate']
        text_lower = str(text).lower()
        
        return any(term in text_lower for term in election_terms)

    def _detect_protest_context(self, text: str) -> bool:
        """Detecta contexto de protesto."""
        if not text or pd.isna(text):
            return False
        
        protest_terms = ['manifesta√ß√£o', 'protesto', 'greve', 'ocupa√ß√£o', 'ato']
        text_lower = str(text).lower()
        
        return any(term in text_lower for term in protest_terms)

    def _classify_channel_type(self, channel: str) -> str:
        """Classifica tipo de canal."""
        if not channel or pd.isna(channel):
            return 'unknown'
        
        channel_lower = str(channel).lower()
        
        if any(word in channel_lower for word in ['news', 'not√≠cia', 'jornal']):
            return 'news'
        elif any(word in channel_lower for word in ['brasil', 'patriota', 'conservador']):
            return 'political'
        elif any(word in channel_lower for word in ['humor', 'meme', 'engra√ßado']):
            return 'entertainment'
        else:
            return 'general'


def main():
    """Teste do analyzer limpo."""
    logging.basicConfig(level=logging.INFO)

    # Teste com dados de exemplo
    test_data = pd.DataFrame({
        'body': [
            'Este √© um texto sobre pol√≠tica brasileira com bolsonaro',
            'Discuss√£o sobre economia e mercado financeiro liberal',
            'An√°lise social progressista sobre direitos humanos',
            'Texto neutro sobre tecnologia e ci√™ncia',
            'Debate pol√≠tico conservador sobre tradi√ß√µes'
        ],
        'timestamp': [
            '2023-01-01 10:00:00',
            '2023-01-01 11:00:00',
            '2023-01-01 10:30:00',
            '2023-01-02 15:00:00',
            '2023-01-01 10:15:00'
        ]
    })

    analyzer = CleanScientificAnalyzer()
    result = analyzer.analyze_dataset(test_data)

    print(f"\n‚úÖ An√°lise conclu√≠da:")
    print(f"üìä Colunas geradas: {result['columns_generated']}")
    print(f"üéØ Stages completados: {result['stats']['stages_completed']}")
    print(f"üîß Features extra√≠das: {result['stats']['features_extracted']}")

    return result


if __name__ == "__main__":
    main()