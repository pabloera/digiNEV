"""
Integra√ß√£o do Data Validator no Pipeline (Fase 2)

Este m√≥dulo integra o Data Validator como o primeiro est√°gio obrigat√≥rio
do pipeline, conforme especificado no plano de aprimoramento.
"""

import logging
from pathlib import Path
from typing import List, Dict, Any

from ..data.data_validator import DataValidator, ValidationResult
from ..common.config_loader import get_config_loader

logger = logging.getLogger(__name__)

class DataValidationStage:
    """
    Est√°gio de valida√ß√£o de dados como primeiro passo do pipeline.
    
    Este est√°gio implementa o padr√£o "Gatekeeper" para garantir que
    apenas dados v√°lidos sejam processados pelo pipeline.
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Inicializa o est√°gio de valida√ß√£o.
        
        Args:
            config: Configura√ß√£o do sistema
        """
        self.config = config
        self.validator = DataValidator(config)
        
    def execute(self, datasets: List[str]) -> Dict[str, Any]:
        """
        Executa a valida√ß√£o de dados como primeiro est√°gio do pipeline.
        
        Args:
            datasets: Lista de caminhos para datasets
            
        Returns:
            Resultado da valida√ß√£o com datasets v√°lidos
            
        Raises:
            ValueError: Se nenhum dataset v√°lido for encontrado
        """
        logger.info("üîç STAGE 0: Data Validation (Gatekeeper)")
        logger.info("=" * 50)
        
        valid_datasets = []
        all_results = {}
        
        for dataset_path in datasets:
            dataset_path_obj = Path(dataset_path)
            
            if dataset_path_obj.is_file():
                # Validar arquivo individual
                result = self.validator.validate_file_structure(dataset_path_obj)
                all_results[str(dataset_path_obj)] = result
                
                if result.is_valid:
                    valid_datasets.append(str(dataset_path_obj))
                    logger.info(f"‚úÖ {dataset_path_obj.name}: V√ÅLIDO")
                else:
                    logger.warning(f"‚ùå {dataset_path_obj.name}: INV√ÅLIDO - movido para quarentena")
                    try:
                        self.validator.quarantine_file(dataset_path_obj, result)
                    except Exception as e:
                        logger.error(f"Erro ao mover {dataset_path_obj.name} para quarentena: {e}")
                        
            elif dataset_path_obj.is_dir():
                # Validar diret√≥rio
                dir_results = self.validator.validate_dataset(dataset_path_obj)
                all_results.update(dir_results)
                
                # Obter arquivos v√°lidos do diret√≥rio
                dir_valid_files = self.validator.get_valid_files(dataset_path_obj)
                valid_datasets.extend([str(f) for f in dir_valid_files])
                
            else:
                logger.error(f"Caminho inv√°lido: {dataset_path}")
        
        # Verificar se temos dados v√°lidos para continuar
        if not valid_datasets:
            error_msg = (
                "‚ùå PIPELINE INTERROMPIDO: Nenhum dataset v√°lido encontrado!\n"
                "Todos os arquivos falharam na valida√ß√£o e foram movidos para quarentena.\n"
                "Verifique o diret√≥rio de quarentena para mais detalhes."
            )
            logger.error(error_msg)
            raise ValueError("Nenhum dataset v√°lido encontrado ap√≥s valida√ß√£o")
        
        # Estat√≠sticas finais
        total_files = len(all_results)
        valid_files = len(valid_datasets)
        invalid_files = total_files - valid_files
        
        logger.info("üìä RESULTADOS DA VALIDA√á√ÉO:")
        logger.info(f"   Total de arquivos: {total_files}")
        logger.info(f"   Arquivos v√°lidos: {valid_files}")
        logger.info(f"   Arquivos em quarentena: {invalid_files}")
        logger.info(f"   Taxa de sucesso: {(valid_files/total_files)*100:.1f}%")
        
        return {
            'stage_id': '00_data_validation',
            'stage_name': 'Data Validation (Gatekeeper)', 
            'success': True,
            'valid_datasets': valid_datasets,
            'validation_results': all_results,
            'statistics': {
                'total_files': total_files,
                'valid_files': valid_files,
                'invalid_files': invalid_files,
                'success_rate': (valid_files/total_files) if total_files > 0 else 0.0
            },
            'records_processed': sum(
                result.row_count for result in all_results.values() 
                if result.is_valid
            ),
            'execution_time': 0,  # Ser√° preenchido pelo controlador
            'output_file': str(self.validator.quarantine_dir / "validation_log.txt"),
            'message': f"Valida√ß√£o conclu√≠da: {valid_files}/{total_files} arquivos v√°lidos"
        }

def integrate_data_validation_in_pipeline(original_execute_method):
    """
    Decorator para integrar valida√ß√£o de dados no pipeline existente.
    
    Args:
        original_execute_method: M√©todo execute original do pipeline
        
    Returns:
        M√©todo execute modificado com valida√ß√£o
    """
    def execute_with_validation(self, datasets: List[str], config: Dict[str, Any] = None):
        """
        Executa pipeline com valida√ß√£o de dados como primeiro est√°gio.
        
        Args:
            datasets: Lista de datasets
            config: Configura√ß√£o do pipeline
            
        Returns:
            Resultado da execu√ß√£o completa
        """
        logger.info("üöÄ INICIANDO PIPELINE COM VALIDA√á√ÉO DE DADOS")
        
        # STAGE 0: Data Validation (Gatekeeper)
        try:
            validation_stage = DataValidationStage(config or {})
            validation_result = validation_stage.execute(datasets)
            
            # Usar apenas datasets v√°lidos para o restante do pipeline
            valid_datasets = validation_result['valid_datasets']
            
            logger.info(f"‚úÖ Valida√ß√£o conclu√≠da: {len(valid_datasets)} datasets v√°lidos")
            logger.info("üîÑ Prosseguindo com pipeline principal...")
            
            # Executar pipeline original com datasets validados
            pipeline_result = original_execute_method(self, valid_datasets, config)
            
            # Adicionar resultados da valida√ß√£o ao resultado final
            if isinstance(pipeline_result, dict):
                pipeline_result['validation_stage'] = validation_result
                
                # Atualizar estat√≠sticas globais
                if 'stage_results' not in pipeline_result:
                    pipeline_result['stage_results'] = {}
                pipeline_result['stage_results']['00_data_validation'] = validation_result
                
            return pipeline_result
            
        except ValueError as e:
            # Falha na valida√ß√£o - n√£o executar pipeline
            logger.error(f"Pipeline interrompido devido a falha na valida√ß√£o: {e}")
            return {
                'overall_success': False,
                'error': str(e),
                'validation_stage': {
                    'success': False,
                    'error': str(e)
                },
                'message': 'Pipeline interrompido: nenhum dataset v√°lido encontrado'
            }
        except Exception as e:
            logger.error(f"Erro inesperado na valida√ß√£o: {e}")
            # Tentar continuar com datasets originais se valida√ß√£o falhar
            logger.warning("‚ö†Ô∏è Continuando sem valida√ß√£o devido a erro inesperado")
            return original_execute_method(self, datasets, config)
    
    return execute_with_validation