"""
Page: 7 - An√°lise Lingu√≠stica (Stage 07)
Named Entity Recognition (NER) para discurso pol√≠tico brasileiro

Visualiza√ß√µes:
1. Word cloud de entidades por tipo (PERSON, ORG, GPE)
2. Rede de conex√µes entre entidades pol√≠ticas
"""

import streamlit as st
import pandas as pd
import sys
import os

# Adicionar src ao path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from dashboard.stage07_linguistic_dashboard import main_dashboard, LinguisticAnalyzer
from dashboard.data_analysis_dashboard import load_processed_data, get_available_datasets

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="An√°lise Lingu√≠stica - Stage 07",
    page_icon="üî§",
    layout="wide",
    initial_sidebar_state="expanded"
)

def main():
    """Fun√ß√£o principal da p√°gina."""
    st.title("üî§ Stage 07 - An√°lise Lingu√≠stica")
    st.markdown("""
    **Processamento lingu√≠stico com Named Entity Recognition (NER)**

    Esta an√°lise utiliza o modelo spaCy portugu√™s para extrair e visualizar entidades nomeadas
    relevantes para o discurso pol√≠tico brasileiro:

    - **Pessoas (PERSON)**: Pol√≠ticos, l√≠deres, figuras p√∫blicas
    - **Organiza√ß√µes (ORG)**: Partidos pol√≠ticos, institui√ß√µes, empresas
    - **Locais Geopol√≠ticos (GPE)**: Estados, cidades, pa√≠ses
    """)

    # Verificar se h√° dados processados dispon√≠veis
    try:
        available_datasets = get_available_datasets()

        if not available_datasets:
            st.error("üìÅ Nenhum dataset processado encontrado")
            st.info("""
            Para usar esta an√°lise:
            1. Execute o pipeline de processamento: `python run_pipeline.py`
            2. Ou carregue dados atrav√©s da p√°gina Home
            """)
            return

        # Sidebar para sele√ß√£o de dataset
        st.sidebar.header("üîß Configura√ß√µes")

        selected_dataset = st.sidebar.selectbox(
            "üìä Selecionar Dataset:",
            options=list(available_datasets.keys()),
            format_func=lambda x: f"{x} ({available_datasets[x]['size']})"
        )

        if st.sidebar.button("üîÑ Recarregar Dados"):
            st.cache_data.clear()
            st.experimental_rerun()

        # Carregar dados selecionados
        with st.spinner(f"Carregando {selected_dataset}..."):
            df = load_processed_data(selected_dataset)

        if df is None or df.empty:
            st.error(f"‚ùå Erro ao carregar dataset: {selected_dataset}")
            return

        # Verificar se h√° dados lingu√≠sticos processados
        linguistic_columns = [col for col in df.columns if 'spacy' in col.lower() or 'entities' in col.lower()]

        if not linguistic_columns:
            st.warning("""
            ‚ö†Ô∏è **Dados lingu√≠sticos n√£o encontrados**

            Este dataset n√£o possui processamento lingu√≠stico (Stage 07).
            """)

            # Op√ß√£o para processar entidades em tempo real
            if st.button("üîÑ Processar Entidades Agora"):
                analyzer = LinguisticAnalyzer()

                # Verificar se spaCy est√° dispon√≠vel
                if not analyzer.nlp:
                    st.error("‚ùå Modelo spaCy portugu√™s n√£o dispon√≠vel")
                    st.info("Instale com: `python -m spacy download pt_core_news_lg`")
                    return

                # Processar amostra pequena
                sample_size = min(100, len(df))
                df_sample = df.sample(n=sample_size, random_state=42)

                with st.spinner("Extraindo entidades..."):
                    df_processed = analyzer.extract_entities_from_dataframe(df_sample)

                st.success(f"‚úÖ Entidades extra√≠das para {len(df_sample)} mensagens")

                # Executar dashboard com dados processados
                main_dashboard(df_processed)

            return

        # Informa√ß√µes do dataset
        st.sidebar.markdown("---")
        st.sidebar.subheader("üìä Informa√ß√µes do Dataset")
        st.sidebar.write(f"**Registros:** {len(df):,}")
        st.sidebar.write(f"**Colunas:** {len(df.columns)}")

        # Verificar colunas lingu√≠sticas dispon√≠veis
        if linguistic_columns:
            st.sidebar.write("**Processamento Lingu√≠stico:**")
            for col in linguistic_columns:
                if 'count' in col:
                    avg_value = df[col].mean() if df[col].dtype in ['int64', 'float64'] else 0
                    st.sidebar.write(f"- {col}: {avg_value:.1f} m√©dia")

        # Verificar qualidade dos dados lingu√≠sticos
        has_entities = False
        if 'spacy_entities_count' in df.columns:
            total_entities = df['spacy_entities_count'].sum()
            has_entities = total_entities > 0
            st.sidebar.write(f"**Total Entidades:** {total_entities:,}")

        if not has_entities:
            st.warning("""
            ‚ö†Ô∏è **Poucas entidades encontradas no dataset**

            Isso pode indicar:
            - Textos muito curtos
            - Baixa qualidade de texto
            - Necessidade de reprocessamento
            """)

            # Mostrar amostra de dados
            st.subheader("üîç Amostra dos Dados")
            text_columns = [col for col in df.columns if col in ['body', 'normalized_text', 'text']]
            if text_columns:
                sample_df = df[text_columns + linguistic_columns].head(10)
                st.dataframe(sample_df, use_container_width=True)

        # Executar dashboard principal
        main_dashboard(df)

        # Se√ß√£o de an√°lise adicional
        st.markdown("---")
        st.subheader("üìà An√°lise Avan√ßada")

        with st.expander("üîç Explorar Dados Lingu√≠sticos"):
            # Estat√≠sticas detalhadas
            if 'spacy_tokens_count' in df.columns:
                col1, col2, col3 = st.columns(3)

                with col1:
                    st.metric(
                        "Tokens M√©dios",
                        f"{df['spacy_tokens_count'].mean():.1f}",
                        f"œÉ: {df['spacy_tokens_count'].std():.1f}"
                    )

                with col2:
                    if 'spacy_entities_count' in df.columns:
                        st.metric(
                            "Entidades M√©dias",
                            f"{df['spacy_entities_count'].mean():.1f}",
                            f"Max: {df['spacy_entities_count'].max()}"
                        )

                with col3:
                    if 'lemmatized_text' in df.columns:
                        non_empty_lemmas = df['lemmatized_text'].str.len() > 0
                        st.metric(
                            "Textos Lemmatizados",
                            f"{non_empty_lemmas.sum():,}",
                            f"{(non_empty_lemmas.mean()*100):.1f}%"
                        )

            # Distribui√ß√£o de tokens
            if 'spacy_tokens_count' in df.columns:
                st.subheader("üìä Distribui√ß√£o de Tokens")

                import plotly.express as px

                fig_hist = px.histogram(
                    df,
                    x='spacy_tokens_count',
                    nbins=30,
                    title='Distribui√ß√£o do N√∫mero de Tokens por Mensagem',
                    labels={'spacy_tokens_count': 'N√∫mero de Tokens', 'count': 'Frequ√™ncia'}
                )
                fig_hist.update_layout(showlegend=False)
                st.plotly_chart(fig_hist, use_container_width=True)

        # Informa√ß√µes t√©cnicas
        with st.expander("‚ÑπÔ∏è Informa√ß√µes T√©cnicas"):
            st.markdown("""
            **Modelo de Processamento:**
            - spaCy com modelo portugu√™s (pt_core_news_lg/sm)
            - Extra√ß√£o de entidades nomeadas (NER)
            - Lemmatiza√ß√£o e tokeniza√ß√£o

            **Tipos de Entidades:**
            - **PERSON**: Pessoas (pol√≠ticos, l√≠deres)
            - **ORG**: Organiza√ß√µes (partidos, institui√ß√µes)
            - **GPE**: Entidades geopol√≠ticas (estados, cidades)

            **Filtragem:**
            - Entidades politicamente relevantes para contexto brasileiro
            - Filtragem por frequ√™ncia m√≠nima
            - Co-ocorr√™ncia para an√°lise de rede
            """)

    except Exception as e:
        st.error(f"‚ùå Erro ao carregar p√°gina: {str(e)}")
        st.info("Tente recarregar a p√°gina ou verificar os logs do sistema")

if __name__ == "__main__":
    main()