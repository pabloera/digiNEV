"""
P√°gina de Modelagem de T√≥picos do Dashboard digiNEV
An√°lise de temas sem√¢nticos e clustering de conte√∫do
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from typing import Optional

def render_topics_page(data_loader):
    """Renderiza a p√°gina de modelagem de t√≥picos"""
    
    st.markdown('<div class="page-header"><h2>üé® Modelagem de T√≥picos</h2></div>', unsafe_allow_html=True)
    
    if not data_loader:
        st.error("Sistema de dados n√£o dispon√≠vel")
        return
    
    # Carregar dados de t√≥picos e clustering
    topic_data = data_loader.load_data('topic_modeling')
    clustering_data = data_loader.load_data('clustering_results')
    
    if topic_data is None:
        st.warning("üìä Dados de modelagem de t√≥picos n√£o dispon√≠veis")
        st.info("Execute o pipeline principal para gerar a modelagem de t√≥picos (Stage 09)")
        return
    
    # Filtros interativos
    st.subheader("üîç Filtros de An√°lise")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Filtro por t√≥pico
        if 'topic' in topic_data.columns:
            topics = ['Todos'] + sorted(list(topic_data['topic'].unique()))
            selected_topic = st.selectbox("T√≥pico", topics)
        else:
            selected_topic = 'Todos'
    
    with col2:
        # Filtro por relev√¢ncia do t√≥pico
        if 'topic_relevance' in topic_data.columns:
            min_relevance = st.slider(
                "Relev√¢ncia M√≠nima",
                min_value=float(topic_data['topic_relevance'].min()),
                max_value=float(topic_data['topic_relevance'].max()),
                value=float(topic_data['topic_relevance'].min()),
                step=0.01
            )
        else:
            min_relevance = 0.0
    
    with col3:
        # Filtro por cluster
        if clustering_data is not None and 'cluster' in clustering_data.columns:
            clusters = ['Todos'] + sorted(list(clustering_data['cluster'].unique()))
            selected_cluster = st.selectbox("Cluster", clusters)
        else:
            selected_cluster = 'Todos'
    
    # Aplicar filtros
    filtered_data = topic_data.copy()
    
    if selected_topic != 'Todos' and 'topic' in filtered_data.columns:
        filtered_data = filtered_data[filtered_data['topic'] == selected_topic]
    
    if 'topic_relevance' in filtered_data.columns:
        filtered_data = filtered_data[filtered_data['topic_relevance'] >= min_relevance]
    
    # M√©tricas principais
    st.subheader("üìä M√©tricas de T√≥picos")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_messages = len(filtered_data)
        st.metric("Total de Mensagens", f"{total_messages:,}")
    
    with col2:
        if 'topic' in filtered_data.columns:
            unique_topics = filtered_data['topic'].nunique()
            st.metric("T√≥picos Identificados", unique_topics)
        else:
            st.metric("T√≥picos", "N/A")
    
    with col3:
        if 'topic_relevance' in filtered_data.columns:
            avg_relevance = filtered_data['topic_relevance'].mean()
            st.metric("Relev√¢ncia M√©dia", f"{avg_relevance:.3f}")
        else:
            st.metric("Relev√¢ncia", "N/A")
    
    with col4:
        if clustering_data is not None and 'cluster' in clustering_data.columns:
            unique_clusters = clustering_data['cluster'].nunique()
            st.metric("Clusters Sem√¢nticos", unique_clusters)
        else:
            st.metric("Clusters", "N/A")
    
    # Visualiza√ß√µes principais
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìä Distribui√ß√£o de T√≥picos")
        
        if 'topic' in filtered_data.columns:
            topic_counts = filtered_data['topic'].value_counts().head(10)
            
            fig_topics = go.Figure()
            fig_topics.add_trace(go.Bar(
                x=topic_counts.values,
                y=topic_counts.index,
                orientation='h',
                marker_color='lightblue',
                text=topic_counts.values,
                textposition='auto'
            ))
            
            fig_topics.update_layout(
                title="Top 10 T√≥picos por Frequ√™ncia",
                xaxis_title="N√∫mero de Mensagens",
                yaxis_title="T√≥pico",
                height=400
            )
            
            st.plotly_chart(fig_topics, use_container_width=True)
        else:
            st.info("Dados de t√≥picos n√£o dispon√≠veis")
    
    with col2:
        st.subheader("üéØ Relev√¢ncia dos T√≥picos")
        
        if 'topic_relevance' in filtered_data.columns and 'topic' in filtered_data.columns:
            # Boxplot de relev√¢ncia por t√≥pico
            top_topics = filtered_data['topic'].value_counts().head(8).index
            plot_data = filtered_data[filtered_data['topic'].isin(top_topics)]
            
            fig_relevance = px.box(
                plot_data,
                x='topic',
                y='topic_relevance',
                title="Distribui√ß√£o de Relev√¢ncia por T√≥pico"
            )
            
            fig_relevance.update_layout(
                height=400,
                xaxis_tickangle=-45
            )
            
            st.plotly_chart(fig_relevance, use_container_width=True)
        else:
            st.info("Dados de relev√¢ncia n√£o dispon√≠veis")
    
    # Visualiza√ß√£o de clustering sem√¢ntico
    if clustering_data is not None:
        st.subheader("üî¨ An√°lise de Clustering Sem√¢ntico")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if 'cluster' in clustering_data.columns:
                cluster_counts = clustering_data['cluster'].value_counts()
                
                fig_clusters = go.Figure()
                fig_clusters.add_trace(go.Pie(
                    labels=[f"Cluster {c}" for c in cluster_counts.index],
                    values=cluster_counts.values,
                    hole=0.3,
                    textinfo='label+percent'
                ))
                
                fig_clusters.update_layout(
                    title="Distribui√ß√£o de Clusters",
                    height=350
                )
                
                st.plotly_chart(fig_clusters, use_container_width=True)
            else:
                st.info("Dados de cluster n√£o dispon√≠veis")
        
        with col2:
            # Scatter plot de clustering (se tiver coordenadas 2D)
            if all(col in clustering_data.columns for col in ['embedding_2d_x', 'embedding_2d_y', 'cluster']):
                # Amostra para performance
                sample_data = clustering_data.sample(min(1000, len(clustering_data)))
                
                fig_scatter = px.scatter(
                    sample_data,
                    x='embedding_2d_x',
                    y='embedding_2d_y',
                    color='cluster',
                    title="Visualiza√ß√£o 2D dos Clusters",
                    labels={'embedding_2d_x': 'Dimens√£o 1', 'embedding_2d_y': 'Dimens√£o 2'}
                )
                
                fig_scatter.update_layout(height=350)
                st.plotly_chart(fig_scatter, use_container_width=True)
            else:
                st.info("Coordenadas de embedding n√£o dispon√≠veis")
    
    # An√°lise de termos-chave por t√≥pico
    st.subheader("üîë Termos-Chave por T√≥pico")
    
    if 'topic' in filtered_data.columns:
        # Seletor de t√≥pico espec√≠fico
        available_topics = sorted(filtered_data['topic'].unique())
        selected_topic_detail = st.selectbox(
            "Selecionar t√≥pico para an√°lise detalhada:",
            available_topics,
            key="topic_detail"
        )
        
        if selected_topic_detail:
            topic_subset = filtered_data[filtered_data['topic'] == selected_topic_detail]
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write(f"**Mensagens no t√≥pico**: {len(topic_subset):,}")
                
                if 'topic_relevance' in topic_subset.columns:
                    st.write(f"**Relev√¢ncia m√©dia**: {topic_subset['topic_relevance'].mean():.3f}")
                
                # Palavras-chave do t√≥pico (se dispon√≠vel)
                if 'topic_keywords' in topic_subset.columns:
                    keywords = topic_subset['topic_keywords'].iloc[0] if not topic_subset.empty else "N/A"
                    st.write(f"**Palavras-chave**: {keywords}")
            
            with col2:
                # Exemplos de mensagens do t√≥pico
                if 'text' in topic_subset.columns and len(topic_subset) > 0:
                    st.write("**Exemplos de mensagens:**")
                    
                    # Pegar top 3 mensagens com maior relev√¢ncia
                    if 'topic_relevance' in topic_subset.columns:
                        examples = topic_subset.nlargest(3, 'topic_relevance')
                    else:
                        examples = topic_subset.head(3)
                    
                    for idx, row in examples.iterrows():
                        text_preview = row['text'][:150] + "..." if len(row['text']) > 150 else row['text']
                        relevance = f" (Relev√¢ncia: {row['topic_relevance']:.3f})" if 'topic_relevance' in row else ""
                        st.write(f"‚Ä¢ {text_preview}{relevance}")
    
    # An√°lise temporal dos t√≥picos
    if 'date' in filtered_data.columns or 'timestamp' in filtered_data.columns:
        st.subheader("üìà Evolu√ß√£o Temporal dos T√≥picos")
        
        date_col = 'date' if 'date' in filtered_data.columns else 'timestamp'
        
        try:
            filtered_data[date_col] = pd.to_datetime(filtered_data[date_col])
            
            # Seletor de per√≠odo e top t√≥picos
            col1, col2 = st.columns(2)
            
            with col1:
                period = st.selectbox("Per√≠odo", ["Di√°rio", "Semanal", "Mensal"], index=1, key="topic_period")
            
            with col2:
                top_n_topics = st.selectbox("Top N T√≥picos", [3, 5, 8, 10], index=1)
            
            if period == "Di√°rio":
                freq = 'D'
            elif period == "Semanal":
                freq = 'W'
            else:
                freq = 'M'
            
            # Pegar top t√≥picos
            top_topics = filtered_data['topic'].value_counts().head(top_n_topics).index
            temporal_data = filtered_data[filtered_data['topic'].isin(top_topics)]
            
            # Agrupar por per√≠odo e t√≥pico
            temporal_grouped = temporal_data.groupby([
                temporal_data[date_col].dt.to_period(freq),
                'topic'
            ]).size().reset_index(name='count')
            
            temporal_grouped[date_col] = temporal_grouped[date_col].dt.to_timestamp()
            
            fig_temporal = px.line(
                temporal_grouped,
                x=date_col,
                y='count',
                color='topic',
                title=f"Evolu√ß√£o {period} dos Top {top_n_topics} T√≥picos",
                labels={'count': 'N√∫mero de Mensagens', date_col: 'Per√≠odo'}
            )
            
            fig_temporal.update_layout(height=400)
            st.plotly_chart(fig_temporal, use_container_width=True)
            
        except Exception as e:
            st.warning(f"Erro na an√°lise temporal: {e}")
    
    # Matriz de similaridade entre t√≥picos
    if 'topic' in filtered_data.columns and len(filtered_data['topic'].unique()) > 1:
        st.subheader("üîó Similaridade entre T√≥picos")
        
        # Calcular co-ocorr√™ncia de palavras entre t√≥picos (aproxima√ß√£o)
        if 'text' in filtered_data.columns:
            try:
                from sklearn.feature_extraction.text import TfidfVectorizer
                from sklearn.metrics.pairwise import cosine_similarity
                
                # Agrupar textos por t√≥pico
                topic_texts = filtered_data.groupby('topic')['text'].apply(lambda x: ' '.join(x)).to_dict()
                
                if len(topic_texts) > 1:
                    topics_list = list(topic_texts.keys())
                    texts_list = [topic_texts[topic] for topic in topics_list]
                    
                    # Calcular TF-IDF e similaridade
                    vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
                    tfidf_matrix = vectorizer.fit_transform(texts_list)
                    similarity_matrix = cosine_similarity(tfidf_matrix)
                    
                    # Criar heatmap
                    fig_similarity = go.Figure(data=go.Heatmap(
                        z=similarity_matrix,
                        x=topics_list,
                        y=topics_list,
                        colorscale='Blues',
                        texttemplate="%{z:.2f}",
                        textfont={"size": 10}
                    ))
                    
                    fig_similarity.update_layout(
                        title="Matriz de Similaridade entre T√≥picos",
                        height=400
                    )
                    
                    st.plotly_chart(fig_similarity, use_container_width=True)
                else:
                    st.info("Insuficientes t√≥picos para an√°lise de similaridade")
                    
            except ImportError:
                st.info("Biblioteca sklearn n√£o dispon√≠vel para an√°lise de similaridade")
            except Exception as e:
                st.warning(f"Erro na an√°lise de similaridade: {e}")
    
    # Estat√≠sticas detalhadas
    st.subheader("üìã Estat√≠sticas Detalhadas")
    
    if 'topic' in filtered_data.columns:
        # Tabela de estat√≠sticas por t√≥pico
        topic_stats = []
        
        for topic in filtered_data['topic'].unique():
            topic_subset = filtered_data[filtered_data['topic'] == topic]
            
            stats = {
                'T√≥pico': topic,
                'Mensagens': len(topic_subset),
                'Percentual': f"{(len(topic_subset) / len(filtered_data)) * 100:.1f}%"
            }
            
            if 'topic_relevance' in topic_subset.columns:
                stats['Relev√¢ncia M√©dia'] = f"{topic_subset['topic_relevance'].mean():.3f}"
                stats['Relev√¢ncia Max'] = f"{topic_subset['topic_relevance'].max():.3f}"
            
            if 'text' in topic_subset.columns:
                avg_length = topic_subset['text'].str.len().mean()
                stats['Tamanho M√©dio'] = f"{avg_length:.0f} chars"
            
            topic_stats.append(stats)
        
        stats_df = pd.DataFrame(topic_stats)
        stats_df = stats_df.sort_values('Mensagens', ascending=False)
        
        st.dataframe(stats_df, use_container_width=True)
    
    # Exporta√ß√£o e controles
    st.subheader("üõ†Ô∏è Ferramentas de An√°lise")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.button("üì• Exportar T√≥picos", key="export_topics"):
            export_path = data_loader.export_data('topic_modeling', 'csv')
            if export_path:
                st.success(f"Dados exportados: {export_path.name}")
    
    with col2:
        if st.button("üìä Exportar Clusters", key="export_clusters"):
            if clustering_data is not None:
                export_path = data_loader.export_data('clustering_results', 'csv')
                if export_path:
                    st.success(f"Clusters exportados: {export_path.name}")
            else:
                st.warning("Dados de clustering n√£o dispon√≠veis")
    
    with col3:
        if st.button("üîÑ Atualizar Cache", key="refresh_topics_cache"):
            data_loader.clear_cache()
            st.rerun()
    
    with col4:
        if st.button("üîç Buscar T√≥picos", key="search_topics"):
            search_term = st.text_input("Termo de busca:", key="topic_search_input")
            if search_term:
                results = data_loader.search_data(search_term, ['topic_modeling'])
                if results:
                    st.write(f"Encontrados {len(results['topic_modeling'])} resultados")
                else:
                    st.write("Nenhum resultado encontrado")
    
    # Insights autom√°ticos
    st.subheader("üí° Insights da Modelagem de T√≥picos")
    
    insights = []
    
    if 'topic' in filtered_data.columns:
        most_common_topic = filtered_data['topic'].value_counts().index[0]
        topic_percentage = (filtered_data['topic'].value_counts().iloc[0] / len(filtered_data)) * 100
        insights.append(f"‚Ä¢ **T√≥pico Dominante**: {most_common_topic} ({topic_percentage:.1f}% das mensagens)")
        
        num_topics = filtered_data['topic'].nunique()
        coverage = (filtered_data['topic'].value_counts().head(5).sum() / len(filtered_data)) * 100
        insights.append(f"‚Ä¢ **Concentra√ß√£o**: Top 5 t√≥picos cobrem {coverage:.1f}% do conte√∫do ({num_topics} t√≥picos totais)")
    
    if 'topic_relevance' in filtered_data.columns:
        high_relevance_pct = (filtered_data['topic_relevance'] > 0.7).mean() * 100
        insights.append(f"‚Ä¢ **Alta Relev√¢ncia**: {high_relevance_pct:.1f}% das mensagens t√™m relev√¢ncia > 0.7")
    
    if clustering_data is not None and 'cluster' in clustering_data.columns:
        num_clusters = clustering_data['cluster'].nunique()
        avg_cluster_size = len(clustering_data) / num_clusters
        insights.append(f"‚Ä¢ **Clustering**: {num_clusters} clusters identificados (m√©dia: {avg_cluster_size:.0f} mensagens/cluster)")
    
    if insights:
        for insight in insights:
            st.markdown(insight)
    else:
        st.info("Execute modelagem de t√≥picos completa para gerar insights autom√°ticos")