"""
Validador de Deduplica√ß√£o via API Anthropic
Garante exclus√£o de mensagens de m√≠dia da deduplica√ß√£o e valida contagem de duplicatas.
"""

import hashlib
import json
import logging
import re
import unicodedata
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import pandas as pd

from .api_error_handler import APIErrorHandler, APIQualityChecker
from .base import AnthropicBase

logger = logging.getLogger(__name__)


class DeduplicationValidator(AnthropicBase):
    """
    Validador avan√ßado de deduplica√ß√£o usando API Anthropic

    Capacidades:
    - Detec√ß√£o inteligente de mensagens de m√≠dia
    - Valida√ß√£o de exclus√£o correta da deduplica√ß√£o
    - An√°lise de padr√µes de duplica√ß√£o
    - Detec√ß√£o de coordena√ß√£o suspeita
    - Valida√ß√£o de contagem de duplicatas
    - Identifica√ß√£o de falsos positivos/negativos
    """

    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)
        self.error_handler = APIErrorHandler()
        self.quality_checker = APIQualityChecker(config)

        # Padr√µes para detectar mensagens de m√≠dia
        self.media_patterns = {
            "photo_indicators": [
                r"\[foto\]", r"\[imagem\]", r"\[image\]", r"\[photo\]",
                r"foto anexada", r"imagem anexada", r"enviou uma foto",
                r"enviou uma imagem", r"compartilhou uma foto",
                r"\.(jpg|jpeg|png|gif|bmp|webp)", r"üì∑", r"üì∏", r"üñºÔ∏è"
            ],
            "video_indicators": [
                r"\[v√≠deo\]", r"\[video\]", r"\[filme\]", r"\[movie\]",
                r"v√≠deo anexado", r"video anexado", r"enviou um v√≠deo",
                r"enviou um video", r"compartilhou um v√≠deo",
                r"\.(mp4|avi|mov|wmv|flv|webm|mkv)", r"üé•", r"üìπ", r"üé¨"
            ],
            "audio_indicators": [
                r"\[√°udio\]", r"\[audio\]", r"\[som\]", r"\[voice\]",
                r"√°udio anexado", r"audio anexado", r"enviou um √°udio",
                r"enviou um audio", r"mensagem de voz", r"nota de voz",
                r"\.(mp3|wav|ogg|m4a|aac|flac)", r"üéµ", r"üé∂", r"üîä", r"üé§"
            ],
            "document_indicators": [
                r"\[documento\]", r"\[doc\]", r"\[arquivo\]", r"\[file\]",
                r"documento anexado", r"arquivo anexado", r"enviou um arquivo",
                r"compartilhou um documento", r"\.(pdf|doc|docx|xls|xlsx|ppt|pptx)",
                r"üìÑ", r"üìÉ", r"üìã", r"üìä", r"üìà", r"üìâ"
            ],
            "sticker_indicators": [
                r"\[sticker\]", r"\[adesivo\]", r"\[figurinha\]",
                r"enviou um sticker", r"enviou uma figurinha",
                r"enviou um adesivo", r"üòÄ", r"üòÇ", r"‚ù§Ô∏è"
            ],
            "location_indicators": [
                r"\[localiza√ß√£o\]", r"\[location\]", r"\[local\]",
                r"compartilhou localiza√ß√£o", r"enviou localiza√ß√£o",
                r"üìç", r"üó∫Ô∏è", r"üåç", r"üåé", r"üåè"
            ]
        }

        # Padr√µes para detectar coordena√ß√£o suspeita
        self.coordination_patterns = {
            "timing_suspicious": "mensagens id√™nticas em intervalo < 5 minutos",
            "exact_duplicates": "texto exatamente igual em m√∫ltiplos canais",
            "template_messages": "estrutura similar com varia√ß√µes m√≠nimas",
            "bot_signatures": "padr√µes de formata√ß√£o automatizada"
        }

    def validate_deduplication_process(
        self,
        original_df: pd.DataFrame,
        deduplicated_df: pd.DataFrame,
        duplicate_count_column: str = "duplicate_count",
        text_column: str = None
    ) -> Dict[str, Any]:
        """
        Valida processo de deduplica√ß√£o completo

        Args:
            original_df: DataFrame original antes da deduplica√ß√£o
            deduplicated_df: DataFrame ap√≥s deduplica√ß√£o
            duplicate_count_column: Nome da coluna com contagem de duplicatas
            text_column: Nome da coluna de texto (detectado automaticamente se None)

        Returns:
            Relat√≥rio de valida√ß√£o da deduplica√ß√£o
        """
        logger.info(f"Validando deduplica√ß√£o: {len(original_df)} -> {len(deduplicated_df)} registros")

        # Detectar coluna de texto automaticamente se n√£o fornecida
        if text_column is None:
            text_column = self._detect_text_column(original_df)

        validation_report = {
            "timestamp": datetime.now().isoformat(),
            "original_count": len(original_df),
            "deduplicated_count": len(deduplicated_df),
            "reduction_ratio": (len(original_df) - len(deduplicated_df)) / len(original_df),
            "text_column_used": text_column,
            "media_analysis": self._analyze_media_exclusion(original_df, deduplicated_df, text_column),
            "duplicate_count_validation": self._validate_duplicate_counts(deduplicated_df, duplicate_count_column),
            "coordination_analysis": self._analyze_coordination_patterns(deduplicated_df),
            "quality_assessment": {},
            "recommendations": []
        }

        # An√°lise detalhada via API
        api_analysis = self._detailed_deduplication_analysis_api(original_df, deduplicated_df)
        validation_report["api_analysis"] = api_analysis

        # Calcular score de qualidade
        validation_report["quality_assessment"] = self._calculate_deduplication_quality(validation_report)

        # Gerar recomenda√ß√µes
        validation_report["recommendations"] = self._generate_deduplication_recommendations(validation_report)

        logger.info(f"Valida√ß√£o conclu√≠da. Qualidade: {validation_report['quality_assessment'].get('overall_score', 'N/A')}")

        return validation_report

    def detect_media_messages(
        self,
        df: pd.DataFrame,
        text_column: str = "body",
        batch_size: int = 20
    ) -> pd.DataFrame:
        """
        Detecta mensagens de m√≠dia usando padr√µes e API

        Args:
            df: DataFrame para analisar
            text_column: Coluna com texto das mensagens
            batch_size: Tamanho do lote para processamento

        Returns:
            DataFrame com flags de m√≠dia detectadas
        """
        logger.info(f"Detectando mensagens de m√≠dia em {len(df)} registros")

        result_df = df.copy()

        # Detec√ß√£o baseada em padr√µes
        result_df = self._detect_media_patterns(result_df, text_column)

        # Detec√ß√£o via API para casos amb√≠guos
        ambiguous_indices = self._find_ambiguous_media_cases(result_df, text_column)

        if ambiguous_indices:
            logger.info(f"Analisando {len(ambiguous_indices)} casos amb√≠guos via API")

            # Processar em lotes
            for i in range(0, len(ambiguous_indices), batch_size):
                batch_indices = ambiguous_indices[i:i + batch_size]
                batch_texts = result_df.loc[batch_indices, text_column].tolist()

                # Usar error handler para an√°lise com retry
                api_result = self.error_handler.execute_with_retry(
                    self._analyze_media_with_api,
                    stage="02b_deduplication_validation",
                    operation=f"media_detection_batch_{i//batch_size + 1}",
                    texts=batch_texts
                )

                if api_result.success and api_result.data:
                    self._apply_api_media_detection(result_df, batch_indices, api_result.data)

        return result_df

    def _detect_text_column(self, df: pd.DataFrame) -> str:
        """Detecta automaticamente a coluna de texto principal"""

        # CORRE√á√ÉO CR√çTICA: Verificar se o CSV foi parseado corretamente
        if len(df.columns) == 1 and ',' in df.columns[0]:
            # Header mal interpretado - CSV n√£o foi parseado corretamente
            logger.error(f"PARSING CSV INCORRETO detectado. Header: {df.columns[0][:100]}...")
            logger.info("For√ßando uso da coluna 'body' como fallback seguro")
            return 'body'

        # Verificar se colunas esperadas existem
        expected_columns = ['message_id', 'datetime', 'body', 'channel']
        if not any(col in df.columns for col in expected_columns):
            logger.error(f"Colunas esperadas n√£o encontradas. Colunas dispon√≠veis: {list(df.columns)}")
            # Tentar usar primeira coluna que pare√ßa conter texto
            for col in df.columns:
                if df[col].dtype == 'object':
                    logger.warning(f"Usando coluna {col} como fallback")
                    return col
            return 'body'  # √∫ltimo fallback

        # Candidatos conhecidos para coluna de texto (PRIORIDADE: body_cleaned > body)
        text_candidates = ['body_cleaned', 'body', 'texto_cleaned', 'texto', 'text', 'content', 'message', 'mensagem']

        # Verificar candidatos conhecidos primeiro
        for candidate in text_candidates:
            if candidate in df.columns:
                # Verificar se a coluna tem conte√∫do √∫til
                non_empty_count = df[candidate].dropna().astype(str).str.len().gt(0).sum()
                if non_empty_count > len(df) * 0.1:  # Pelo menos 10% com conte√∫do
                    logger.info(f"Coluna de texto detectada: {candidate} ({non_empty_count}/{len(df)} com conte√∫do)")
                    return candidate
                else:
                    logger.warning(f"Coluna {candidate} existe mas tem pouco conte√∫do ({non_empty_count}/{len(df)})")

        logger.warning("Nenhuma coluna de texto ideal encontrada, usando fallback 'body'")

        # Se n√£o encontrou, usar primeira coluna de texto longo
        for col in df.columns:
            if df[col].dtype == 'object':
                try:
                    sample_length = df[col].dropna().apply(str).str.len().mean()
                    if sample_length > 50:  # Texto com m√©dia > 50 caracteres
                        logger.info(f"Coluna de texto detectada por tamanho: {col}")
                        return col
                except:
                    continue

        # Fallback para primeira coluna string dispon√≠vel
        for col in df.columns:
            if df[col].dtype == 'object':
                logger.info(f"Usando primeira coluna de texto dispon√≠vel: {col}")
                return col

        # Se tudo falhar, usar primeira coluna
        fallback_col = df.columns[0] if len(df.columns) > 0 else 'body'
        logger.info(f"Usando fallback para coluna de texto: {fallback_col}")
        return fallback_col

    def _analyze_media_exclusion(
        self,
        original_df: pd.DataFrame,
        deduplicated_df: pd.DataFrame,
        text_column: str = None
    ) -> Dict[str, Any]:
        """Analisa se mensagens de m√≠dia foram corretamente exclu√≠das"""

        analysis = {
            "media_detection_original": {},
            "media_detection_deduplicated": {},
            "exclusion_validation": {}
        }

        # Detectar coluna de texto automaticamente se n√£o fornecida
        if text_column is None:
            text_column = self._detect_text_column(original_df)

        # Detectar m√≠dia no dataset original
        original_with_media = self.detect_media_messages(
            original_df.sample(n=min(1000, len(original_df))),
            text_column=text_column
        )

        # Contar tipos de m√≠dia
        for media_type in ["photo", "video", "audio", "document", "sticker", "location"]:
            flag_column = f"has_{media_type}"
            if flag_column in original_with_media.columns:
                count = original_with_media[flag_column].sum()
                total = len(original_with_media)
                analysis["media_detection_original"][media_type] = {
                    "count": count,
                    "percentage": round((count / total) * 100, 2) if total > 0 else 0
                }

        # Verificar se mensagens de m√≠dia est√£o no dataset deduplicado
        media_in_deduplicated = self._check_media_in_deduplicated(original_with_media, deduplicated_df)
        analysis["exclusion_validation"] = media_in_deduplicated

        return analysis

    def _detect_media_patterns(self, df: pd.DataFrame, text_column: str) -> pd.DataFrame:
        """Detecta mensagens de m√≠dia usando padr√µes regex"""

        result_df = df.copy()
        text_series = result_df[text_column].fillna("").astype(str)

        for media_type, patterns in self.media_patterns.items():
            flag_column = f"has_{media_type.replace('_indicators', '')}"

            # Combinar todos os padr√µes para o tipo de m√≠dia
            combined_pattern = "|".join(patterns)

            # Detectar padr√µes (case insensitive)
            result_df[flag_column] = text_series.str.contains(
                combined_pattern, case=False, regex=True, na=False
            )

        # Flag geral de m√≠dia
        media_columns = [col for col in result_df.columns if col.startswith("has_") and "indicators" not in col]
        if media_columns:
            result_df["has_any_media"] = result_df[media_columns].any(axis=1)

        return result_df

    def _find_ambiguous_media_cases(self, df: pd.DataFrame, text_column: str) -> List[int]:
        """Encontra casos amb√≠guos que precisam de an√°lise via API"""

        ambiguous_indices = []
        text_series = df[text_column].fillna("").astype(str)

        for idx, text in text_series.items():
            # Crit√©rios para casos amb√≠guos
            if len(text.strip()) > 0:
                # Textos muito curtos que podem ser legendas de m√≠dia
                if len(text.strip()) < 10 and not any(keyword in text.lower()
                                                      for keyword in ["foto", "video", "audio", "sticker", "documento"]):
                    ambiguous_indices.append(idx)

                # Textos com emojis que podem indicar m√≠dia
                emoji_count = len(re.findall(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]', text))
                if emoji_count > 3 and len(text.split()) < 5:
                    ambiguous_indices.append(idx)

                # Textos com men√ß√µes a "enviou", "compartilhou" sem indicadores claros
                if re.search(r'\b(enviou|compartilhou|mandou)\b', text.lower()) and len(text.split()) < 8:
                    ambiguous_indices.append(idx)

        return list(set(ambiguous_indices))[:50]  # Reduzir limite para evitar truncamento JSON

    def _analyze_media_with_api(self, texts: List[str]) -> Dict[str, Any]:
        """Usa API para analisar casos amb√≠guos de m√≠dia"""

        # Limitar tamanho do texto para evitar truncamento
        truncated_texts = []
        for i, text in enumerate(texts[:20]):  # M√°ximo 20 textos por vez
            # Truncar texto individual se muito longo
            truncated_text = text[:100] + "..." if len(text) > 100 else text
            truncated_texts.append(f"{i+1}. {truncated_text}")

        texts_sample = "\n".join(truncated_texts)

        prompt = f"""
Analise as seguintes mensagens do Telegram brasileiro para determinar se s√£o mensagens de m√≠dia (foto, v√≠deo, √°udio, documento, sticker, localiza√ß√£o).

MENSAGENS:
{texts_sample}

Para cada mensagem, determine:
1. Se √© uma mensagem de m√≠dia
2. Que tipo de m√≠dia (se aplic√°vel)
3. Se pode ter texto adicional al√©m da m√≠dia

Responda em formato JSON:
{{
  "media_analysis": [
    {{
      "text_id": 1,
      "is_media_message": true/false,
      "media_types": ["photo", "video", "audio", "document", "sticker", "location"],
      "has_additional_text": true/false,
      "confidence": "alto|medio|baixo",
      "reasoning": "explica√ß√£o breve"
    }}
  ]
}}

CRIT√âRIOS:
- Mensagens apenas com emoji/sticker S√ÉO m√≠dia
- Legendas curtas com m√≠dia S√ÉO m√≠dia
- Textos descritivos longos N√ÉO s√£o m√≠dia (mesmo com m√≠dia anexada)
- Considere contexto brasileiro do Telegram
"""

        try:
            response = self.create_message(
                prompt,
                stage="02b_deduplication_validation",
                operation="media_analysis"
            )

            return self.parse_json_response(response)

        except Exception as e:
            logger.error(f"Erro na an√°lise de m√≠dia via API: {e}")
            return {}

    def _apply_api_media_detection(
        self,
        df: pd.DataFrame,
        indices: List[int],
        api_results: Dict[str, Any]
    ):
        """Aplica resultados da detec√ß√£o de m√≠dia via API"""

        if "media_analysis" in api_results:
            for analysis in api_results["media_analysis"]:
                text_id = analysis.get("text_id", 1) - 1

                if text_id < len(indices):
                    actual_index = indices[text_id]
                    is_media = analysis.get("is_media_message", False)
                    media_types = analysis.get("media_types", [])
                    confidence = analysis.get("confidence", "baixo")

                    # Aplicar apenas resultados de alta confian√ßa
                    if confidence in ["alto", "medio"]:
                        # Atualizar flag geral de m√≠dia
                        df.loc[actual_index, "has_any_media"] = is_media

                        # Atualizar flags espec√≠ficos
                        for media_type in media_types:
                            flag_column = f"has_{media_type}"
                            if flag_column in df.columns:
                                df.loc[actual_index, flag_column] = True

    def _check_media_in_deduplicated(
        self,
        original_with_media: pd.DataFrame,
        deduplicated_df: pd.DataFrame
    ) -> Dict[str, Any]:
        """Verifica se mensagens de m√≠dia est√£o incorretamente no dataset deduplicado"""

        validation = {
            "media_incorrectly_included": 0,
            "total_media_messages": 0,
            "exclusion_rate": 0.0,
            "issues_found": []
        }

        if "has_any_media" in original_with_media.columns:
            media_messages = original_with_media[original_with_media["has_any_media"]]
            validation["total_media_messages"] = len(media_messages)

            # Verificar se alguma mensagem de m√≠dia est√° no dataset deduplicado
            # (usando hash do texto ou identificador √∫nico)
            if "body" in media_messages.columns and "body" in deduplicated_df.columns:
                media_texts = set(media_messages["body"].fillna("").astype(str))
                deduplicated_texts = set(deduplicated_df["body"].fillna("").astype(str))

                incorrectly_included = media_texts.intersection(deduplicated_texts)
                validation["media_incorrectly_included"] = len(incorrectly_included)

                if incorrectly_included:
                    validation["issues_found"].extend([
                        f"Mensagem de m√≠dia incorretamente inclu√≠da: {text[:50]}..."
                        for text in list(incorrectly_included)[:5]
                    ])

            # Calcular taxa de exclus√£o
            if validation["total_media_messages"] > 0:
                excluded = validation["total_media_messages"] - validation["media_incorrectly_included"]
                validation["exclusion_rate"] = round((excluded / validation["total_media_messages"]) * 100, 2)

        return validation

    def intelligent_deduplication(self, df: pd.DataFrame, text_column: str = "body") -> pd.DataFrame:
        """
        Executa deduplica√ß√£o completa: remove duplicatas e adiciona coluna de frequ√™ncia

        Args:
            df: DataFrame com dados para deduplicar
            text_column: Nome da coluna de texto principal ("body" ou "body_cleaned")

        Returns:
            DataFrame deduplicado (SEM duplicatas) + coluna 'duplicate_frequency'
        """

        logger.info(f"üîÑ INICIANDO DEDUPLICA√á√ÉO COMPLETA de {len(df)} registros")
        logger.info(f"üìù Usando coluna de texto: '{text_column}'")

        try:
            # 1. PRIORIDADE: Usar body_cleaned se dispon√≠vel, sen√£o body
            dedup_column = None
            if 'body_cleaned' in df.columns:
                dedup_column = 'body_cleaned'
                logger.info("‚úÖ Usando 'body_cleaned' para deduplica√ß√£o (texto j√° processado)")
            elif 'body' in df.columns:
                dedup_column = 'body'
                logger.info("‚úÖ Usando 'body' para deduplica√ß√£o (texto original)")
            else:
                logger.error(f"‚ùå Colunas 'body' ou 'body_cleaned' n√£o encontradas. Dispon√≠veis: {list(df.columns)}")
                return df

            # 2. Estat√≠sticas iniciais
            total_records = len(df)
            non_empty_content = df[dedup_column].dropna().astype(str).str.strip().str.len().gt(0).sum()
            logger.info(f"üìä Registros com conte√∫do em '{dedup_column}': {non_empty_content}/{total_records} ({100*non_empty_content/total_records:.1f}%)")

            if non_empty_content == 0:
                logger.warning("‚ö†Ô∏è  Nenhum conte√∫do encontrado para deduplica√ß√£o")
                df['duplicate_frequency'] = 1
                return df

            # 3. DEDUPLICA√á√ÉO SIMPLES E EFETIVA por conte√∫do de texto
            logger.info(f"üîç Preparando texto para deduplica√ß√£o usando '{dedup_column}'...")

            # Preparar coluna normalizada para deduplica√ß√£o
            df_work = df.copy()
            df_work['_normalized_text'] = (
                df_work[dedup_column]
                .fillna('')
                .astype(str)
                .str.strip()
                .str.lower()
                .str.replace(r'\s+', ' ', regex=True)  # Normalizar espa√ßos
            )

            # 4. CONTAR FREQU√äNCIAS das mensagens
            logger.info("üìä Calculando frequ√™ncias de duplicatas...")
            text_counts = df_work['_normalized_text'].value_counts()
            df_work['duplicate_frequency'] = df_work['_normalized_text'].map(text_counts)

            # 5. ESTAT√çSTICAS de duplica√ß√£o
            total_unique = len(text_counts)
            total_duplicates = len(df_work) - total_unique
            reduction_rate = (total_duplicates / len(df_work)) * 100

            logger.info(f"üìà ESTAT√çSTICAS DE DUPLICA√á√ÉO:")
            logger.info(f"   Total de registros: {len(df_work)}")
            logger.info(f"   Textos √∫nicos: {total_unique}")
            logger.info(f"   Duplicatas encontradas: {total_duplicates}")
            logger.info(f"   Taxa de duplica√ß√£o: {reduction_rate:.1f}%")

            # 6. REMOVER DUPLICATAS mantendo apenas a primeira ocorr√™ncia
            logger.info("üóëÔ∏è  Removendo duplicatas (mantendo primeira ocorr√™ncia)...")

            # Manter apenas primeira ocorr√™ncia de cada texto √∫nico
            deduplicated_df = df_work.drop_duplicates(subset=['_normalized_text'], keep='first').copy()

            # Remover coluna auxiliar
            deduplicated_df = deduplicated_df.drop('_normalized_text', axis=1)

            # 7. VALIDA√á√ÉO FINAL
            if reduction_rate < 1.0:
                logger.warning(f"‚ö†Ô∏è  Taxa de duplica√ß√£o baixa ({reduction_rate:.1f}%). Dados podem estar j√° limpos.")
            elif reduction_rate > 70.0:
                logger.warning(f"‚ö†Ô∏è  Taxa de duplica√ß√£o muito alta ({reduction_rate:.1f}%). Verificar dados.")
            else:
                logger.info(f"‚úÖ Taxa de duplica√ß√£o normal: {reduction_rate:.1f}%")

            # 8. ESTAT√çSTICAS FINAIS
            final_records = len(deduplicated_df)
            duplicates_removed = len(df) - final_records
            max_frequency = deduplicated_df['duplicate_frequency'].max()
            mean_frequency = deduplicated_df['duplicate_frequency'].mean()

            logger.info(f"üéØ DEDUPLICA√á√ÉO CONCLU√çDA:")
            logger.info(f"   ‚û§ Dataset original: {len(df)} registros")
            logger.info(f"   ‚û§ Dataset limpo: {final_records} registros")
            logger.info(f"   ‚û§ Duplicatas removidas: {duplicates_removed} ({reduction_rate:.1f}%)")
            logger.info(f"   ‚û§ Maior frequ√™ncia: {max_frequency}x")
            logger.info(f"   ‚û§ Frequ√™ncia m√©dia: {mean_frequency:.2f}x")
            logger.info(f"   ‚û§ Coluna adicionada: 'duplicate_frequency'")

            return deduplicated_df

        except Exception as e:
            logger.error(f"Erro na deduplica√ß√£o inteligente: {e}")
            # Fallback para n√£o quebrar o pipeline
            return df

    def _ai_validate_duplicates(self, df: pd.DataFrame, text_column: str) -> pd.DataFrame:
        """
        Valida duplicatas usando API Anthropic para casos amb√≠guos

        Args:
            df: DataFrame com duplicatas marcadas
            text_column: Coluna de texto

        Returns:
            DataFrame deduplicado com valida√ß√£o AI
        """

        # Para esta vers√£o, usar deduplica√ß√£o b√°sica
        # TODO: Implementar valida√ß√£o AI detalhada em vers√£o futura
        deduplicated_df = df.drop_duplicates(subset=['content_hash'], keep='first')
        deduplicated_df = deduplicated_df.drop('content_hash', axis=1)

        return deduplicated_df

    def _validate_duplicate_counts(self, df: pd.DataFrame, duplicate_count_column: str) -> Dict[str, Any]:
        """Valida contagem de duplicatas"""

        validation = {
            "column_exists": duplicate_count_column in df.columns,
            "statistics": {},
            "validation_issues": []
        }

        if validation["column_exists"]:
            count_series = df[duplicate_count_column].fillna(0)

            validation["statistics"] = {
                "min_count": int(count_series.min()),
                "max_count": int(count_series.max()),
                "mean_count": round(count_series.mean(), 2),
                "total_duplicates_counted": int(count_series.sum()),
                "records_with_duplicates": int((count_series > 1).sum()),
                "percentage_with_duplicates": round(((count_series > 1).sum() / len(df)) * 100, 2)
            }

            # Valida√ß√µes
            if validation["statistics"]["min_count"] < 1:
                validation["validation_issues"].append("Encontrados valores menores que 1 na contagem")

            if validation["statistics"]["max_count"] > 1000:
                validation["validation_issues"].append(f"Contagem muito alta encontrada: {validation['statistics']['max_count']}")

            # Verificar se soma das contagens faz sentido com dataset original
            expected_original_size = validation["statistics"]["total_duplicates_counted"]
            actual_deduplicated_size = len(df)

            validation["size_consistency"] = {
                "expected_original_size": expected_original_size,
                "actual_deduplicated_size": actual_deduplicated_size,
                "ratio": round(expected_original_size / actual_deduplicated_size, 2) if actual_deduplicated_size > 0 else 0
            }

        return validation

    def _analyze_coordination_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analisa padr√µes de coordena√ß√£o suspeita"""

        analysis = {
            "potential_coordination": [],
            "statistics": {},
            "suspicious_patterns": []
        }

        if "duplicate_count" in df.columns and "body" in df.columns:
            # Mensagens com muitas duplicatas (poss√≠vel coordena√ß√£o)
            high_duplicate_threshold = df["duplicate_count"].quantile(0.95)
            high_duplicates = df[df["duplicate_count"] > high_duplicate_threshold]

            analysis["statistics"]["high_duplicate_messages"] = len(high_duplicates)
            analysis["statistics"]["high_duplicate_threshold"] = int(high_duplicate_threshold)

            # Analisar padr√µes suspeitos
            if len(high_duplicates) > 0:
                # Mensagens muito similares
                similar_patterns = self._find_similar_message_patterns(high_duplicates)
                analysis["suspicious_patterns"].extend(similar_patterns)

                # Adicionar amostras para an√°lise
                for _, row in high_duplicates.head(5).iterrows():
                    analysis["potential_coordination"].append({
                        "text": str(row["body"])[:100] + "...",
                        "duplicate_count": int(row["duplicate_count"]),
                        "canal": row.get("canal", "N/A")
                    })

        return analysis

    def _find_similar_message_patterns(self, df: pd.DataFrame) -> List[str]:
        """Encontra padr√µes de mensagens similares que podem indicar coordena√ß√£o"""

        patterns = []

        if len(df) > 1:
            texts = df["body"].fillna("").astype(str).tolist()

            # Procurar por estruturas similares
            for i, text1 in enumerate(texts[:10]):  # Limitar para performance
                for j, text2 in enumerate(texts[i+1:11], i+1):
                    similarity = self._calculate_text_similarity(text1, text2)
                    if similarity > 0.8:  # 80% de similaridade
                        patterns.append(f"Textos muito similares: √≠ndices {i} e {j} (similaridade: {similarity:.2f})")

        return patterns

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """Calcula similaridade simples entre dois textos"""

        if not text1 or not text2:
            return 0.0

        # Tokeniza√ß√£o simples
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1 and not words2:
            return 1.0
        if not words1 or not words2:
            return 0.0

        # Jaccard similarity
        intersection = words1.intersection(words2)
        union = words1.union(words2)

        return len(intersection) / len(union) if union else 0.0

    def _detailed_deduplication_analysis_api(
        self,
        original_df: pd.DataFrame,
        deduplicated_df: pd.DataFrame
    ) -> Dict[str, Any]:
        """An√°lise detalhada via API"""

        # Preparar amostra para an√°lise
        sample_size = min(50, len(deduplicated_df))
        sample_df = deduplicated_df.sample(n=sample_size, random_state=42)

        # Usar error handler para an√°lise com retry
        result = self.error_handler.execute_with_retry(
            self._analyze_deduplication_quality_api,
            stage="02b_deduplication_validation",
            operation="quality_analysis",
            sample_df=sample_df,
            reduction_ratio=(len(original_df) - len(deduplicated_df)) / len(original_df)
        )

        if result.success:
            return result.data
        else:
            logger.warning(f"Falha na an√°lise detalhada via API: {result.error.error_message}")
            return {"error": result.error.error_message}

    def _analyze_deduplication_quality_api(
        self,
        sample_df: pd.DataFrame,
        reduction_ratio: float
    ) -> Dict[str, Any]:
        """Usa API para an√°lise de qualidade da deduplica√ß√£o"""

        # Preparar dados para an√°lise
        sample_data = []
        for idx, row in sample_df.head(20).iterrows():
            sample_data.append({
                "body": str(row.get("body", ""))[:200],
                "duplicate_count": int(row.get("duplicate_count", 1)),
                "canal": str(row.get("canal", "N/A"))
            })

        prompt = f"""
Analise a qualidade do processo de deduplica√ß√£o deste dataset brasileiro do Telegram:

ESTAT√çSTICAS:
- Taxa de redu√ß√£o: {reduction_ratio:.1%}
- Amostra de registros deduplicados:

{json.dumps(sample_data, indent=2, ensure_ascii=False)}

Avalie:
1. Se a taxa de redu√ß√£o √© apropriada
2. Se as contagens de duplicatas parecem corretas
3. Se h√° padr√µes suspeitos de coordena√ß√£o
4. Qualidade geral do processo de deduplica√ß√£o

Responda em formato JSON:
{{
  "quality_assessment": {{
    "reduction_rate_appropriate": true/false,
    "duplicate_counts_seem_accurate": true/false,
    "coordination_patterns_detected": true/false,
    "overall_quality": "excelente|bom|satisfatorio|ruim",
    "confidence": "alto|medio|baixo"
  }},
  "issues_identified": [
    "problema1", "problema2"
  ],
  "recommendations": [
    "recomendacao1", "recomendacao2"
  ],
  "coordination_analysis": {{
    "suspicious_patterns": ["padrao1", "padrao2"],
    "likely_bot_activity": true/false,
    "organic_vs_coordinated_ratio": "estimativa"
  }}
}}
"""

        try:
            response = self.create_message(
                prompt,
                stage="02b_deduplication_validation",
                operation="quality_analysis"
            )

            return self.parse_json_response(response)

        except Exception as e:
            logger.error(f"Erro na an√°lise de qualidade via API: {e}")
            return {}

    def _calculate_deduplication_quality(self, validation_report: Dict[str, Any]) -> Dict[str, Any]:
        """Calcula score de qualidade geral da deduplica√ß√£o"""

        quality_score = 1.0
        quality_factors = []

        # Fator 1: Exclus√£o adequada de m√≠dia
        media_analysis = validation_report.get("media_analysis", {})
        exclusion_validation = media_analysis.get("exclusion_validation", {})
        exclusion_rate = exclusion_validation.get("exclusion_rate", 0)

        if exclusion_rate < 90:
            quality_score -= 0.3
            quality_factors.append(f"Taxa de exclus√£o de m√≠dia baixa: {exclusion_rate}%")

        # Fator 2: Consist√™ncia das contagens
        duplicate_validation = validation_report.get("duplicate_count_validation", {})
        if duplicate_validation.get("validation_issues"):
            quality_score -= 0.2
            quality_factors.append("Problemas na contagem de duplicatas")

        # Fator 3: Taxa de redu√ß√£o apropriada (5-60% √© normal)
        reduction_ratio = validation_report.get("reduction_ratio", 0)
        if reduction_ratio < 0.05 or reduction_ratio > 0.8:
            quality_score -= 0.2
            quality_factors.append(f"Taxa de redu√ß√£o suspeita: {reduction_ratio:.1%}")

        # Fator 4: An√°lise da API
        api_analysis = validation_report.get("api_analysis", {})
        if "quality_assessment" in api_analysis:
            api_quality = api_analysis["quality_assessment"].get("overall_quality", "satisfatorio")
            if api_quality in ["ruim"]:
                quality_score -= 0.3
                quality_factors.append("Qualidade avaliada como ruim pela API")

        return {
            "overall_score": max(0.0, quality_score),
            "quality_level": ("excelente" if quality_score >= 0.9 else
                              "bom" if quality_score >= 0.7 else
                              "satisfatorio" if quality_score >= 0.5 else "ruim"),
            "quality_factors": quality_factors
        }

    def _generate_deduplication_recommendations(self, validation_report: Dict[str, Any]) -> List[str]:
        """Gera recomenda√ß√µes baseadas na valida√ß√£o"""

        recommendations = []

        # Baseado na qualidade geral
        quality_assessment = validation_report.get("quality_assessment", {})
        quality_level = quality_assessment.get("quality_level", "satisfatorio")

        if quality_level == "ruim":
            recommendations.append("Reprocessar deduplica√ß√£o com par√¢metros ajustados")

        # Baseado na exclus√£o de m√≠dia
        media_analysis = validation_report.get("media_analysis", {})
        exclusion_validation = media_analysis.get("exclusion_validation", {})

        if exclusion_validation.get("media_incorrectly_included", 0) > 0:
            recommendations.append("Melhorar detec√ß√£o de mensagens de m√≠dia antes da deduplica√ß√£o")

        # Baseado na an√°lise de coordena√ß√£o
        coordination_analysis = validation_report.get("coordination_analysis", {})
        if coordination_analysis.get("suspicious_patterns"):
            recommendations.append("Investigar padr√µes de coordena√ß√£o identificados")

        # Baseado na an√°lise da API
        api_analysis = validation_report.get("api_analysis", {})
        api_recommendations = api_analysis.get("recommendations", [])
        recommendations.extend(api_recommendations)

        return list(set(recommendations))  # Remover duplicatas

    def filter_empty_text_messages(self, df: pd.DataFrame, text_column: str = None) -> Tuple[pd.DataFrame, Dict]:
        """
        Remove mensagens sem conte√∫do textual antes da deduplica√ß√£o para otimizar performance
        
        Args:
            df: DataFrame para filtrar
            text_column: Coluna de texto (auto-detectada se None)
            
        Returns:
            Tuple com DataFrame filtrado e relat√≥rio de filtragem
        """
        logger.info(f"üîç Filtrando mensagens sem texto de {len(df)} registros")
        
        # Detectar coluna de texto automaticamente se n√£o fornecida
        if text_column is None:
            text_column = self._detect_text_column(df)
        
        filter_report = {
            "timestamp": datetime.now().isoformat(),
            "original_count": len(df),
            "text_column_used": text_column,
            "filtered_count": 0,
            "retention_count": 0,
            "media_types_removed": {},
            "performance_improvement": {}
        }
        
        if text_column not in df.columns:
            logger.warning(f"Coluna de texto '{text_column}' n√£o encontrada")
            return df, filter_report
        
        # Criar m√°scara para textos v√°lidos
        valid_text_mask = (
            (~df[text_column].isnull()) & 
            (df[text_column] != '') & 
            (df[text_column].astype(str).str.strip() != '')
        )
        
        # Contar registros com/sem texto
        valid_count = valid_text_mask.sum()
        empty_count = len(df) - valid_count
        
        # Analisar tipos de m√≠dia dos registros sem texto
        if 'media_type' in df.columns and empty_count > 0:
            empty_media_types = df[~valid_text_mask]['media_type'].value_counts()
            filter_report["media_types_removed"] = empty_media_types.to_dict()
        
        # Filtrar apenas registros com texto v√°lido
        filtered_df = df[valid_text_mask].copy().reset_index(drop=True)
        
        # Calcular m√©tricas de performance
        original_comparisons = len(df) ** 2
        filtered_comparisons = len(filtered_df) ** 2
        performance_reduction = (1 - filtered_comparisons/original_comparisons) * 100 if original_comparisons > 0 else 0
        
        filter_report.update({
            "filtered_count": empty_count,
            "retention_count": valid_count,
            "reduction_percentage": (empty_count / len(df)) * 100 if len(df) > 0 else 0,
            "performance_improvement": {
                "original_comparisons": original_comparisons,
                "filtered_comparisons": filtered_comparisons,
                "reduction_percentage": performance_reduction
            }
        })
        
        logger.info(f"‚úÖ Filtragem conclu√≠da:")
        logger.info(f"   ‚û§ Registros originais: {len(df)}")
        logger.info(f"   ‚û§ Registros com texto: {valid_count} ({100*valid_count/len(df):.1f}%)")
        logger.info(f"   ‚û§ Registros removidos: {empty_count} ({100*empty_count/len(df):.1f}%)")
        logger.info(f"   ‚û§ Melhoria de performance: {performance_reduction:.1f}% menos compara√ß√µes")
        
        return filtered_df, filter_report

    def enhance_global_deduplication(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
        """
        Deduplica√ß√£o global aprimorada com m√∫ltiplas estrat√©gias

        Args:
            df: DataFrame para deduplicar

        Returns:
            Tuple com DataFrame deduplicado e relat√≥rio de deduplica√ß√£o
        """
        logger.info(f"Iniciando deduplica√ß√£o global aprimorada para {len(df)} registros")

        deduplication_report = {
            "timestamp": datetime.now().isoformat(),
            "original_count": len(df),
            "strategies_applied": [],
            "duplicate_patterns": {},
            "final_count": 0,
            "reduction_metrics": {},
            "quality_scores": {}
        }

        # Fazer backup dos dados originais
        backup_file = f"data/interim/deduplication_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(backup_file, index=False, sep=';', encoding='utf-8')
        logger.info(f"Backup criado: {backup_file}")

        result_df = df.copy()

        # OTIMIZA√á√ÉO: Filtrar mensagens sem texto primeiro (32% performance boost)
        result_df, filter_stats = self.filter_empty_text_messages(result_df)
        deduplication_report["strategies_applied"].append("empty_text_filtering")
        deduplication_report["duplicate_patterns"]["text_filtering"] = filter_stats

        # Estrat√©gia 1: Deduplica√ß√£o por ID √∫nico
        result_df, id_stats = self._deduplicate_by_unique_id(result_df)
        deduplication_report["strategies_applied"].append("unique_id_deduplication")
        deduplication_report["duplicate_patterns"]["id_duplicates"] = id_stats

        # Estrat√©gia 2: Deduplica√ß√£o por conte√∫do sem√¢ntico
        result_df, content_stats = self._deduplicate_by_semantic_content(result_df)
        deduplication_report["strategies_applied"].append("semantic_content_deduplication")
        deduplication_report["duplicate_patterns"]["content_duplicates"] = content_stats

        # Estrat√©gia 3: Deduplica√ß√£o temporal com janela deslizante
        result_df, temporal_stats = self._deduplicate_by_temporal_window(result_df)
        deduplication_report["strategies_applied"].append("temporal_window_deduplication")
        deduplication_report["duplicate_patterns"]["temporal_duplicates"] = temporal_stats

        # Estrat√©gia 4: An√°lise de padr√µes de duplica√ß√£o
        duplicate_patterns = self._analyze_duplicate_patterns(df, result_df)
        deduplication_report["duplicate_patterns"]["analysis"] = duplicate_patterns

        # Calcular m√©tricas finais
        deduplication_report["final_count"] = len(result_df)
        deduplication_report["reduction_metrics"] = self._calculate_reduction_metrics(
            deduplication_report["original_count"],
            deduplication_report["final_count"]
        )

        # Avaliar qualidade da deduplica√ß√£o
        deduplication_report["quality_scores"] = self._assess_deduplication_quality(df, result_df)

        logger.info(f"Deduplica√ß√£o global conclu√≠da: {len(df)} -> {len(result_df)} registros")
        logger.info(f"Redu√ß√£o: {deduplication_report['reduction_metrics']['reduction_percentage']:.2f}%")

        return result_df, deduplication_report

    def _deduplicate_by_unique_id(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
        """Deduplica√ß√£o por ID √∫nico (message_id, id, etc.)"""

        stats = {
            "strategy": "unique_id",
            "duplicates_found": 0,
            "duplicates_removed": 0,
            "id_columns_used": []
        }

        # Identificar colunas de ID poss√≠veis
        id_candidates = ['message_id', 'id', 'unique_id', 'telegram_id', 'msg_id']
        id_column = None

        for candidate in id_candidates:
            if candidate in df.columns:
                # Verificar se a coluna tem valores √∫nicos suficientes
                unique_count = df[candidate].nunique()
                total_count = len(df)
                uniqueness_ratio = unique_count / total_count if total_count > 0 else 0

                if uniqueness_ratio > 0.8:  # 80% de valores √∫nicos
                    id_column = candidate
                    stats["id_columns_used"].append(candidate)
                    break

        if id_column:
            # Contar duplicatas por ID
            id_counts = df[id_column].value_counts()
            duplicates = id_counts[id_counts > 1]
            stats["duplicates_found"] = duplicates.sum() - len(duplicates)

            # Remover duplicatas mantendo primeira ocorr√™ncia
            deduplicated_df = df.drop_duplicates(subset=[id_column], keep='first')
            stats["duplicates_removed"] = len(df) - len(deduplicated_df)

            logger.info(f"Deduplica√ß√£o por ID '{id_column}': removidas {stats['duplicates_removed']} duplicatas")
            return deduplicated_df, stats
        else:
            logger.info("Nenhuma coluna de ID √∫nica encontrada")
            return df, stats

    def _deduplicate_by_semantic_content(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
        """Deduplica√ß√£o por conte√∫do sem√¢ntico (texto normalizado)"""

        stats = {
            "strategy": "semantic_content",
            "duplicates_found": 0,
            "duplicates_removed": 0,
            "text_column_used": None,
            "normalization_applied": []
        }

        # Detectar coluna de texto
        text_column = self._detect_text_column(df)
        stats["text_column_used"] = text_column

        if text_column and text_column in df.columns:
            # Criar vers√£o normalizada do texto
            normalized_texts = self._normalize_text_for_deduplication(df[text_column])
            stats["normalization_applied"] = [
                "lowercase", "whitespace_normalization", "punctuation_removal",
                "special_chars_removal", "emoji_normalization"
            ]

            # Contar duplicatas sem√¢nticas
            text_counts = normalized_texts.value_counts()
            duplicates = text_counts[text_counts > 1]
            stats["duplicates_found"] = duplicates.sum() - len(duplicates)

            # Adicionar coluna de texto normalizado tempor√°ria
            df_with_normalized = df.copy()
            df_with_normalized['_normalized_text'] = normalized_texts

            # Remover duplicatas sem√¢nticas
            deduplicated_df = df_with_normalized.drop_duplicates(subset=['_normalized_text'], keep='first')
            deduplicated_df = deduplicated_df.drop('_normalized_text', axis=1)

            stats["duplicates_removed"] = len(df) - len(deduplicated_df)

            logger.info(f"Deduplica√ß√£o sem√¢ntica: removidas {stats['duplicates_removed']} duplicatas")
            return deduplicated_df, stats
        else:
            logger.warning("Coluna de texto n√£o encontrada para deduplica√ß√£o sem√¢ntica")
            return df, stats

    def _deduplicate_by_temporal_window(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
        """Deduplica√ß√£o temporal com janela deslizante"""

        stats = {
            "strategy": "temporal_window",
            "duplicates_found": 0,
            "duplicates_removed": 0,
            "window_size_minutes": 5,
            "datetime_column_used": None
        }

        # Detectar coluna de datetime
        datetime_candidates = ['datetime', 'timestamp', 'date', 'created_at', 'sent_at']
        datetime_column = None

        for candidate in datetime_candidates:
            if candidate in df.columns:
                try:
                    # Tentar converter para datetime
                    pd.to_datetime(df[candidate].dropna().head(10))
                    datetime_column = candidate
                    stats["datetime_column_used"] = candidate
                    break
                except:
                    continue

        if datetime_column and datetime_column in df.columns:
            # Converter para datetime
            df_temporal = df.copy()
            df_temporal[datetime_column] = pd.to_datetime(df_temporal[datetime_column], errors='coerce')

            # Ordenar por datetime
            df_temporal = df_temporal.sort_values(datetime_column)

            # Detectar coluna de texto para compara√ß√£o
            text_column = self._detect_text_column(df_temporal)

            if text_column and text_column in df_temporal.columns:
                # Encontrar duplicatas temporais
                duplicates_to_remove = []
                window_minutes = stats["window_size_minutes"]

                # Processamento em chunks para evitar O(n¬≤) completo
                chunk_size = 5000
                total_records = len(df_temporal)
                total_chunks = (total_records + chunk_size - 1) // chunk_size
                
                logger.info(f"Processando deduplica√ß√£o temporal em {total_chunks} chunks de {chunk_size} registros")
                
                for chunk_idx in range(total_chunks):
                    start_idx = chunk_idx * chunk_size
                    end_idx = min(start_idx + chunk_size, total_records)
                    
                    logger.info(f"Processando chunk {chunk_idx + 1}/{total_chunks} (registros {start_idx}-{end_idx})")
                    
                    for i in range(start_idx, end_idx):
                        if i in duplicates_to_remove:
                            continue

                        current_row = df_temporal.iloc[i]
                        current_time = current_row[datetime_column]
                        current_text = str(current_row[text_column]).strip().lower()

                        if pd.isna(current_time) or not current_text:
                            continue

                        # Buscar duplicatas apenas na janela temporal pr√≥xima (otimiza√ß√£o)
                        # Limitar busca a pr√≥ximos 1000 registros ou at√© sair da janela temporal
                        max_search = min(i + 1000, total_records)
                        
                        for j in range(i + 1, max_search):
                            if j in duplicates_to_remove:
                                continue

                            next_row = df_temporal.iloc[j]
                            next_time = next_row[datetime_column]
                            next_text = str(next_row[text_column]).strip().lower()

                            if pd.isna(next_time):
                                continue

                            # Verificar se est√° dentro da janela temporal
                            time_diff = abs((next_time - current_time).total_seconds() / 60)
                            if time_diff > window_minutes:
                                break  # Sair da janela temporal

                            # Verificar similaridade do texto (otimizada)
                            if current_text == next_text:
                                duplicates_to_remove.append(j)
                                stats["duplicates_found"] += 1
                            elif len(current_text) > 10 and len(next_text) > 10:
                                # S√≥ calcular similaridade custosa para textos maiores
                                if self._calculate_text_similarity(current_text, next_text) > 0.95:
                                    duplicates_to_remove.append(j)
                                    stats["duplicates_found"] += 1
                    
                    # Log de progresso a cada chunk
                    if (chunk_idx + 1) % 5 == 0 or chunk_idx + 1 == total_chunks:
                        logger.info(f"Progresso temporal: {chunk_idx + 1}/{total_chunks} chunks, "
                                  f"{len(duplicates_to_remove)} duplicatas encontradas")

                # Remover duplicatas temporais
                if duplicates_to_remove:
                    df_temporal = df_temporal.drop(df_temporal.index[duplicates_to_remove])
                    stats["duplicates_removed"] = len(duplicates_to_remove)

                logger.info(f"Deduplica√ß√£o temporal: removidas {stats['duplicates_removed']} duplicatas")
                return df_temporal.reset_index(drop=True), stats
            else:
                logger.warning("Coluna de texto n√£o encontrada para deduplica√ß√£o temporal")
        else:
            logger.warning("Coluna de datetime n√£o encontrada para deduplica√ß√£o temporal")

        return df, stats

    def _normalize_text_for_deduplication(self, text_series: pd.Series) -> pd.Series:
        """Normaliza texto para deduplica√ß√£o sem√¢ntica robusta"""

        normalized = text_series.fillna("").astype(str)

        # 1. Converter para min√∫sculas
        normalized = normalized.str.lower()

        # 2. Normalizar Unicode (NFKC)
        normalized = normalized.apply(lambda x: unicodedata.normalize('NFKC', x))

        # 3. Remover caracteres de controle e invis√≠veis
        normalized = normalized.str.replace(r'[\x00-\x1f\x7f-\x9f]', '', regex=True)
        normalized = normalized.str.replace(r'[\u200b-\u200d\ufeff]', '', regex=True)

        # 4. Normalizar espa√ßos em branco
        normalized = normalized.str.replace(r'\s+', ' ', regex=True)
        normalized = normalized.str.strip()

        # 5. Remover pontua√ß√£o redundante (manter estrutura b√°sica)
        normalized = normalized.str.replace(r'[.]{2,}', '.', regex=True)
        normalized = normalized.str.replace(r'[!]{2,}', '!', regex=True)
        normalized = normalized.str.replace(r'[?]{2,}', '?', regex=True)

        # 6. Normalizar emojis repetidos
        normalized = normalized.str.replace(r'([\U0001F600-\U0001F64F])\1+', r'\1', regex=True)

        # 7. Remover URLs (podem variar mas conte√∫do √© igual)
        normalized = normalized.str.replace(r'https?://\S+', '[URL]', regex=True)

        # 8. Normalizar men√ß√µes e hashtags
        normalized = normalized.str.replace(r'@\w+', '[MENTION]', regex=True)
        normalized = normalized.str.replace(r'#\w+', '[HASHTAG]', regex=True)

        return normalized

    def _analyze_duplicate_patterns(self, original_df: pd.DataFrame, deduplicated_df: pd.DataFrame) -> Dict[str, Any]:
        """Analisa padr√µes de duplica√ß√£o encontrados"""

        analysis = {
            "total_reduction": len(original_df) - len(deduplicated_df),
            "reduction_percentage": ((len(original_df) - len(deduplicated_df)) / len(original_df)) * 100 if len(original_df) > 0 else 0,
            "duplicate_distribution": {},
            "temporal_patterns": {},
            "content_patterns": {},
            "suspicious_patterns": []
        }

        # Analisar distribui√ß√£o de duplicatas por frequ√™ncia
        if 'duplicate_frequency' in deduplicated_df.columns:
            freq_dist = deduplicated_df['duplicate_frequency'].value_counts().sort_index()
            analysis["duplicate_distribution"] = {
                "frequency_counts": freq_dist.to_dict(),
                "max_frequency": int(deduplicated_df['duplicate_frequency'].max()),
                "mean_frequency": float(deduplicated_df['duplicate_frequency'].mean()),
                "messages_with_high_frequency": int((deduplicated_df['duplicate_frequency'] > 10).sum())
            }

        # Analisar padr√µes temporais se coluna de data dispon√≠vel
        datetime_column = None
        for col in ['datetime', 'timestamp', 'date']:
            if col in deduplicated_df.columns:
                datetime_column = col
                break

        if datetime_column:
            try:
                deduplicated_df[datetime_column] = pd.to_datetime(deduplicated_df[datetime_column], errors='coerce')

                # Agrupar por hora para detectar picos
                hourly_counts = deduplicated_df.groupby(deduplicated_df[datetime_column].dt.hour).size()
                daily_counts = deduplicated_df.groupby(deduplicated_df[datetime_column].dt.date).size()

                analysis["temporal_patterns"] = {
                    "peak_hour": int(hourly_counts.idxmax()) if not hourly_counts.empty else None,
                    "peak_hour_count": int(hourly_counts.max()) if not hourly_counts.empty else 0,
                    "peak_day_count": int(daily_counts.max()) if not daily_counts.empty else 0,
                    "temporal_distribution": "normal" if daily_counts.std() < daily_counts.mean() else "irregular"
                }
            except Exception as e:
                logger.warning(f"Erro na an√°lise temporal: {e}")

        # Detectar padr√µes suspeitos
        if 'duplicate_frequency' in deduplicated_df.columns:
            high_freq_messages = deduplicated_df[deduplicated_df['duplicate_frequency'] > 50]
            if len(high_freq_messages) > 0:
                analysis["suspicious_patterns"].append(f"Encontradas {len(high_freq_messages)} mensagens com frequ√™ncia > 50")

            # Detectar poss√≠vel atividade automatizada
            very_high_freq = deduplicated_df[deduplicated_df['duplicate_frequency'] > 100]
            if len(very_high_freq) > 0:
                analysis["suspicious_patterns"].append(f"Poss√≠vel atividade automatizada: {len(very_high_freq)} mensagens com frequ√™ncia > 100")

        return analysis

    def _calculate_reduction_metrics(self, original_count: int, final_count: int) -> Dict[str, Any]:
        """Calcula m√©tricas de redu√ß√£o"""

        if original_count == 0:
            return {"error": "Original count is zero"}

        reduction_count = original_count - final_count
        reduction_percentage = (reduction_count / original_count) * 100

        return {
            "original_count": original_count,
            "final_count": final_count,
            "reduction_count": reduction_count,
            "reduction_percentage": reduction_percentage,
            "retention_percentage": 100 - reduction_percentage,
            "efficiency_score": min(1.0, reduction_percentage / 30)  # Score baseado em 30% como efici√™ncia √≥tima
        }

    def _assess_deduplication_quality(self, original_df: pd.DataFrame, deduplicated_df: pd.DataFrame) -> Dict[str, Any]:
        """Avalia qualidade geral da deduplica√ß√£o"""

        quality_assessment = {
            "overall_score": 0.0,
            "quality_factors": {},
            "recommendations": []
        }

        reduction_percentage = ((len(original_df) - len(deduplicated_df)) / len(original_df)) * 100 if len(original_df) > 0 else 0

        # Fator 1: Taxa de redu√ß√£o apropriada (5-60% √© considerado normal)
        if 5 <= reduction_percentage <= 60:
            quality_assessment["quality_factors"]["reduction_rate"] = {"score": 1.0, "status": "optimal"}
        elif reduction_percentage < 5:
            quality_assessment["quality_factors"]["reduction_rate"] = {"score": 0.5, "status": "low_reduction"}
            quality_assessment["recommendations"].append("Taxa de redu√ß√£o baixa - verificar crit√©rios de deduplica√ß√£o")
        else:
            quality_assessment["quality_factors"]["reduction_rate"] = {"score": 0.3, "status": "high_reduction"}
            quality_assessment["recommendations"].append("Taxa de redu√ß√£o muito alta - verificar falsos positivos")

        # Fator 2: Preserva√ß√£o de dados importantes
        important_columns = ['datetime', 'channel', 'canal', 'message_id']
        preserved_columns = sum(1 for col in important_columns if col in deduplicated_df.columns)
        preservation_score = preserved_columns / len(important_columns)
        quality_assessment["quality_factors"]["data_preservation"] = {"score": preservation_score, "preserved_columns": preserved_columns}

        # Fator 3: Consist√™ncia de frequ√™ncias (se dispon√≠vel)
        if 'duplicate_frequency' in deduplicated_df.columns:
            freq_stats = deduplicated_df['duplicate_frequency'].describe()
            if freq_stats['min'] >= 1 and freq_stats['max'] < 1000:
                quality_assessment["quality_factors"]["frequency_consistency"] = {"score": 1.0, "status": "consistent"}
            else:
                quality_assessment["quality_factors"]["frequency_consistency"] = {"score": 0.7, "status": "inconsistent"}
                quality_assessment["recommendations"].append("Verificar consist√™ncia das frequ√™ncias de duplicatas")

        # Calcular score geral
        factor_scores = [factor["score"] for factor in quality_assessment["quality_factors"].values()]
        quality_assessment["overall_score"] = sum(factor_scores) / len(factor_scores) if factor_scores else 0.0

        # Determinar n√≠vel de qualidade
        if quality_assessment["overall_score"] >= 0.9:
            quality_assessment["quality_level"] = "excellent"
        elif quality_assessment["overall_score"] >= 0.7:
            quality_assessment["quality_level"] = "good"
        elif quality_assessment["overall_score"] >= 0.5:
            quality_assessment["quality_level"] = "satisfactory"
        else:
            quality_assessment["quality_level"] = "poor"

        return quality_assessment
