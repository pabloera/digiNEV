"""
Base class for Anthropic API integrations with configuration management and cost monitoring.

Provides centralized access to Anthropic Claude API with stage-specific settings,
fallback strategies, and budget tracking.
"""

import json
import logging
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

from anthropic import Anthropic
from dotenv import load_dotenv
import yaml

# Encontrar diret√≥rio raiz do projeto
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent  # sobe 2 n√≠veis: src/anthropic_integration -> src -> projeto
env_file = project_root / '.env'

if env_file.exists():
    load_dotenv(env_file)
else:
    # Fallback para carregar do diret√≥rio atual
    load_dotenv()

# Import cost monitor
try:
    from .cost_monitor import get_cost_monitor
    COST_MONITOR_AVAILABLE = True
except ImportError:
    COST_MONITOR_AVAILABLE = False

@dataclass
class AnthropicConfig:
    """Configura√ß√£o para API Anthropic"""
    api_key: str
    model: str = "claude-3-5-sonnet-20241022"  # üîß UPGRADE: Modelo fixo reproduz√≠vel
    max_tokens: int = 2000
    temperature: float = 0.3

class EnhancedConfigLoader:
    """
    Carregador de configura√ß√µes enhanced integrado (consolidado)
    
    Funcionalidades integradas:
    - Carregamento de enhanced_model_settings.yaml
    - Mapeamento de opera√ß√µes para stage_id
    - Fallback strategies
    - Performance modes
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """Inicializa o loader de configura√ß√µes enhanced"""
        if config_path:
            self.config_path = Path(config_path)
        else:
            # Carregar do settings.yaml principal (consolidado)
            self.config_path = project_root / "config" / "settings.yaml"
        
        self.config = self._load_config()
        # Configura√ß√µes consolidadas est√£o agora dentro da se√ß√£o anthropic
        anthropic_config = self.config.get('anthropic', {})
        self.stage_configs = anthropic_config.get('stage_specific_configs', {})
        self.fallback_strategies = anthropic_config.get('fallback_strategies', {})
        
    def _load_config(self) -> Dict[str, Any]:
        """Carrega configura√ß√£o do arquivo YAML"""
        try:
            if self.config_path.exists():
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                logging.getLogger(__name__).info(f"Consolidated configuration loaded: {self.config_path}")
                return config
            else:
                logging.getLogger(__name__).warning(f"‚ö†Ô∏è Arquivo de configura√ß√£o n√£o encontrado: {self.config_path}")
                return {}
        except Exception as e:
            logging.getLogger(__name__).error(f"‚ùå Erro ao carregar configura√ß√µes: {e}")
            return {}
    
    def get_stage_config(self, stage_id: str) -> Dict[str, Any]:
        """Obt√©m configura√ß√£o espec√≠fica para um stage"""
        if stage_id in self.stage_configs:
            config = self.stage_configs[stage_id].copy()
            logging.getLogger(__name__).info(f"üéØ Configura√ß√£o espec√≠fica para {stage_id}: {config.get('model', 'N/A')}")
            return config
        else:
            # Configura√ß√£o padr√£o se stage espec√≠fico n√£o encontrado
            anthropic_config = self.config.get('anthropic', {})
            default_config = {
                'model': anthropic_config.get('model', 'claude-3-5-sonnet-20241022'),
                'temperature': anthropic_config.get('temperature', 0.3),
                'max_tokens': anthropic_config.get('max_tokens', 3000),
                'batch_size': 20
            }
            logging.getLogger(__name__).warning(f"‚ö†Ô∏è Stage {stage_id} n√£o encontrado, usando configura√ß√£o padr√£o")
            return default_config.copy()
    
    def get_stage_from_operation(self, operation: str) -> str:
        """Mapeia opera√ß√£o para stage_id"""
        operation_mapping = {
            'political_analysis': 'stage_05_political',
            'sentiment_analysis': 'stage_08_sentiment',
            'network_analysis': 'stage_15_network',
            'qualitative_analysis': 'stage_16_qualitative',
            'pipeline_review': 'stage_17_review',
            'topic_interpretation': 'stage_18_topics',
            'validation': 'stage_20_validation'
        }
        
        stage_id = operation_mapping.get(operation, f'stage_{operation}')
        logging.getLogger(__name__).debug(f"üîó Opera√ß√£o '{operation}' mapeada para '{stage_id}'")
        return stage_id
    
    def get_fallback_models(self, primary_model: str) -> List[str]:
        """Obt√©m lista de modelos fallback para um modelo prim√°rio"""
        fallbacks = self.fallback_strategies.get(primary_model, [])
        if fallbacks:
            logging.getLogger(__name__).info(f"üîÑ Fallbacks para {primary_model}: {fallbacks}")
        return fallbacks

# Singleton instance do enhanced config loader
_enhanced_config_loader = None

def get_enhanced_config_loader(config_path: Optional[str] = None) -> EnhancedConfigLoader:
    """Obt√©m inst√¢ncia singleton do EnhancedConfigLoader"""
    global _enhanced_config_loader
    
    if _enhanced_config_loader is None:
        _enhanced_config_loader = EnhancedConfigLoader(config_path)
        logging.getLogger(__name__).info("üöÄ EnhancedConfigLoader inicializado")
    
    return _enhanced_config_loader

def load_operation_config(operation: str) -> Dict[str, Any]:
    """Fun√ß√£o de conveni√™ncia para carregar configura√ß√£o por opera√ß√£o"""
    loader = get_enhanced_config_loader()
    stage_id = loader.get_stage_from_operation(operation)
    return loader.get_stage_config(stage_id)

class AnthropicBase:
    """
    Base class for all Anthropic API integrations in the pipeline.
    
    **Class Purpose:**
        Provides standardized Anthropic API access with enhanced configuration management,
        cost monitoring, fallback strategies, and error handling for all pipeline stages.
    
    **Key Features:**
        - Stage-specific configuration loading from enhanced config system
        - Cost monitoring and budget enforcement
        - Automatic fallback strategies for model reliability
        - Rate limiting and retry logic with exponential backoff
        - Comprehensive logging and error handling
        - Support for multiple configuration sources (enhanced config, YAML, env vars)
    
    **Configuration Priority (highest to lowest):**
        1. Enhanced stage-specific config (stage_operation parameter)
        2. YAML configuration file (config parameter)
        3. Environment variables (.env file)
        4. Default values
    
    **Attributes:**
        client (Anthropic): Initialized Anthropic API client
        model (str): Claude model to use (default: claude-3-5-sonnet-20241022)
        max_tokens (int): Maximum tokens per request (default: 3000)
        temperature (float): Model temperature 0.0-1.0 (default: 0.3)
        batch_size (int): Number of requests per batch (default: 20)
        api_available (bool): Whether API client is ready for use
        cost_monitor (CostMonitor): Cost tracking and budget enforcement
        enhanced_config_available (bool): Whether enhanced config is loaded
        
    **Methods:**
        get_api_client() -> Anthropic: Returns authenticated API client
        is_api_available() -> bool: Checks if API is ready for use
        get_effective_config() -> Dict: Returns current effective configuration
        log_api_usage(tokens_used: int, cost: float): Logs API usage for monitoring
        
    **Usage Example:**
        ```python
        # Basic usage with environment variables
        processor = AnthropicBase()
        
        # Stage-specific configuration
        processor = AnthropicBase(stage_operation="political_analysis")
        
        # Custom configuration
        config = {"anthropic": {"model": "claude-3-5-haiku-20241022"}}
        processor = AnthropicBase(config=config)
        
        # Use the client
        if processor.is_api_available():
            response = processor.client.messages.create(
                model=processor.model,
                max_tokens=processor.max_tokens,
                messages=[{"role": "user", "content": "Hello!"}]
            )
        ```
    
    **Error Handling:**
        The class handles various API errors gracefully:
        - Missing API keys: Logs warning, sets api_available=False
        - Invalid API keys: Raises AuthenticationError
        - Network issues: Implements retry logic with backoff
        - Rate limits: Automatically retries with appropriate delays
        - Cost limits: Enforces budget limits via cost monitor
    
    **Configuration Files:**
        - config/settings.yaml: Main consolidated configuration
        - config/anthropic.yaml: API-specific settings
        - .env: API keys (ANTHROPIC_API_KEY)
        
    **Stage Operations Supported:**
        - "political_analysis": Political classification (Stage 05)
        - "sentiment_analysis": Sentiment analysis (Stage 08)
        - "network_analysis": Network analysis (Stage 15)
        - "qualitative_analysis": Qualitative analysis (Stage 16)
        - "pipeline_review": Pipeline review (Stage 17)
        - "topic_interpretation": Topic interpretation (Stage 18)
        - "validation": Final validation (Stage 20)
    
    **Dependencies:**
        - anthropic>=0.40.0: Official Anthropic Python client
        - python-dotenv: Environment variable loading
        - pyyaml: Configuration file parsing
        
    **Thread Safety:**
        This class is thread-safe for read operations. For write operations
        (like cost monitoring), proper synchronization should be implemented
        in the inheriting classes.
        
    **Version:** v5.0.0 (TASK-025 API Documentation)
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None, stage_operation: Optional[str] = None):
        """
        Inicializa cliente Anthropic com configura√ß√£o enhanced integrada

        Args:
            config: Dicion√°rio de configura√ß√£o (se None, usa vari√°veis de ambiente)
            stage_operation: Opera√ß√£o/stage para configura√ß√£o espec√≠fica
        """
        self.config = config or {}
        self.stage_operation = stage_operation
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # Carregar configura√ß√£o enhanced se dispon√≠vel
        self.enhanced_config: Dict[str, Any] = {}
        self.enhanced_config_available = False
        
        if stage_operation:
            try:
                loader = get_enhanced_config_loader()
                self.enhanced_config = load_operation_config(stage_operation)
                self.enhanced_config_available = True
                self.logger.info(f"Enhanced config carregada para {stage_operation}: {self.enhanced_config.get('model', 'N/A')}")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro ao carregar enhanced config para {stage_operation}: {e}")

        # Configurar API - prioridade: enhanced_config > config > env
        if self.enhanced_config_available and self.enhanced_config:
            # Usar configura√ß√£o enhanced espec√≠fica do stage
            api_key = os.getenv('ANTHROPIC_API_KEY')
            self.model = self.enhanced_config.get('model', 'claude-3-5-sonnet-20241022')
            self.max_tokens = self.enhanced_config.get('max_tokens', 3000)
            self.temperature = self.enhanced_config.get('temperature', 0.3)
            self.batch_size = self.enhanced_config.get('batch_size', 20)
            self.logger.info(f"üéØ Usando enhanced config: {self.model} (temp={self.temperature}, tokens={self.max_tokens})")
        elif self.config and 'anthropic' in self.config:
            anthro_config = self.config['anthropic']
            config_api_key = anthro_config.get('api_key', '')

            # Verificar se √© uma refer√™ncia de vari√°vel de ambiente (${VAR_NAME})
            if config_api_key.startswith('${') and config_api_key.endswith('}'):
                var_name = config_api_key[2:-1]  # Remove ${ e }
                api_key = os.getenv(var_name)
            elif config_api_key and not config_api_key.startswith('${'):
                api_key = config_api_key
            else:
                api_key = os.getenv('ANTHROPIC_API_KEY')

            self.model = anthro_config.get('model', 'claude-3-5-sonnet-20241022')
            self.max_tokens = anthro_config.get('max_tokens_per_request', 2000)
            self.temperature = anthro_config.get('temperature', 0.3)
        else:
            api_key = os.getenv('ANTHROPIC_API_KEY')
            self.model = os.getenv('ANTHROPIC_MODEL', 'claude-3-5-sonnet-20241022')
            self.max_tokens = 3000  # Aumentado padr√£o para nova configura√ß√£o
            self.temperature = 0.3
            self.batch_size = 20

        if not api_key:
            self.logger.warning("API key Anthropic n√£o encontrada. Modo tradicional ser√° usado.")
            self.client = None
            self.api_available = False
        else:
            try:
                self.client = Anthropic(api_key=api_key)
                self.api_available = True
                self.logger.info(f"Cliente Anthropic inicializado com modelo: {self.model}")
            except Exception as e:
                self.logger.error(f"Falha ao inicializar cliente Anthropic: {e}")
                self.client = None
                self.api_available = False

        # Configurar monitor de custos (consolidado)
        self.cost_monitor = None
        if COST_MONITOR_AVAILABLE:
            try:
                # Tentar usar enhanced config para cost monitor se dispon√≠vel
                cost_config = None
                if self.enhanced_config_available:
                    loader = get_enhanced_config_loader()
                    cost_config = loader.config.get('anthropic', {}).get('cost_optimization', {})
                
                self.cost_monitor = get_cost_monitor(project_root, cost_config)
            except Exception as e:
                self.logger.warning(f"N√£o foi poss√≠vel inicializar monitor de custos: {e}")

        # Initialize smart cache and performance monitoring
        self.smart_claude_cache = None
        self.performance_monitor = None
        self.week2_cache_available = False
        try:
            from ..optimized.smart_claude_cache import get_global_claude_cache, ClaudeRequest, ClaudeResponse
            from ..optimized.performance_monitor import get_global_performance_monitor
            self.smart_claude_cache = get_global_claude_cache()
            self.performance_monitor = get_global_performance_monitor()
            self.week2_cache_available = True
            self.logger.info("üß† Smart Claude Cache habilitado para semantic caching")
        except ImportError:
            self.logger.info("‚ö†Ô∏è Smart Claude Cache n√£o dispon√≠vel - usando modo padr√£o")

    def get_recommended_model(self, preferred_model: str = None) -> str:
        """
        Obt√©m modelo recomendado com auto-downgrade se necess√°rio
        
        Args:
            preferred_model: Modelo preferido (usa self.model se None)
            
        Returns:
            Modelo recomendado (pode ser downgrade)
        """
        if preferred_model is None:
            preferred_model = self.model
            
        # Verificar auto-downgrade via cost monitor
        if self.cost_monitor and hasattr(self.cost_monitor, 'get_recommended_model'):
            return self.cost_monitor.get_recommended_model(preferred_model)
        
        return preferred_model

    def get_fallback_models(self, model: str = None) -> List[str]:
        """
        Obt√©m lista de modelos fallback
        
        Args:
            model: Modelo para obter fallbacks (usa self.model se None)
            
        Returns:
            Lista de modelos fallback
        """
        if model is None:
            model = self.model
            
        if self.enhanced_config_available:
            try:
                loader = get_enhanced_config_loader()
                return loader.get_fallback_models(model)
            except Exception as e:
                self.logger.warning(f"Erro ao obter fallbacks: {e}")
        
        # Fallbacks padr√£o
        fallback_map = {
            "claude-sonnet-4-20250514": ["claude-3-5-sonnet-20241022", "claude-3-5-haiku-20241022"],
            "claude-3-5-sonnet-20241022": ["claude-3-5-haiku-20241022"],
            "claude-3-5-haiku-20241022": ["claude-3-5-sonnet-20241022"]
        }
        return fallback_map.get(model, [])
    
    def _calculate_cost_estimate(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """Calculate cost estimate for API usage"""
        if model == "claude-sonnet-4-20250514":
            # Claude Sonnet 4: $3.00 entrada, $15.00 sa√≠da (per million tokens)
            input_cost = (input_tokens / 1_000_000) * 3.00
            output_cost = (output_tokens / 1_000_000) * 15.00
            return input_cost + output_cost
        elif model == "claude-3-5-haiku-20241022":
            # Claude 3.5 Haiku: $0.25 entrada, $1.25 sa√≠da (per million tokens)
            input_cost = (input_tokens / 1_000_000) * 0.25
            output_cost = (output_tokens / 1_000_000) * 1.25
            return input_cost + output_cost
        elif "sonnet" in model.lower():
            # Default for Sonnet models
            input_cost = (input_tokens / 1_000_000) * 3.00
            output_cost = (output_tokens / 1_000_000) * 15.00
            return input_cost + output_cost
        else:
            # Default for other models
            input_cost = (input_tokens / 1_000_000) * 0.25
            output_cost = (output_tokens / 1_000_000) * 1.25
            return input_cost + output_cost

    def create_message(self, prompt: str, stage: str = 'unknown', operation: str = 'general', **kwargs) -> str:
        """
        Cria mensagem usando API Anthropic com fallback

        Args:
            prompt: Texto do prompt
            stage: Etapa do pipeline (para rastreamento)
            operation: Opera√ß√£o espec√≠fica (para rastreamento)
            **kwargs: Par√¢metros adicionais para API

        Returns:
            Resposta da API como string ou mensagem de fallback
        """
        # Verificar se API est√° dispon√≠vel
        if not self.api_available or not self.client:
            fallback_message = kwargs.get('fallback_response',
                f"API indispon√≠vel para {stage}:{operation}. Usando processamento tradicional.")
            self.logger.warning(f"API indispon√≠vel, usando fallback para {stage}:{operation}")
            return fallback_message

        # Obter modelo recomendado (com auto-downgrade se necess√°rio)
        model = kwargs.get('model', self.get_recommended_model())
        max_tokens = kwargs.get('max_tokens', self.max_tokens)
        temperature = kwargs.get('temperature', self.temperature)
        
        # Check semantic cache first if available
        if self.week2_cache_available and self.smart_claude_cache:
            try:
                from ..optimized.smart_claude_cache import ClaudeRequest, ClaudeResponse
                
                # Create cache request
                cache_request = ClaudeRequest(
                    prompt=prompt,
                    stage=stage,
                    operation=operation,
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                # Check cache for similar response
                cached_response = self.smart_claude_cache.get_response(cache_request)
                
                if cached_response:
                    # Record performance metrics
                    if self.performance_monitor:
                        self.performance_monitor.record_stage_completion(
                            stage_name=f"{stage}_{operation}",
                            records_processed=1,
                            processing_time=cached_response.response_time,
                            success_rate=1.0,
                            api_calls=0,  # Cache hit = no API call
                            cost_usd=0.0  # Cache hit = no cost
                        )
                    
                    self.logger.info(f"üß† Smart Cache {cached_response.cache_level.upper()}: {operation} "
                                   f"(similarity: {cached_response.semantic_similarity:.2f})")
                    return cached_response.content
                    
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Smart cache failed, proceeding with API call: {e}")
        
        try:
            # Make API call
            response = self.client.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )
            
            # Store response in Smart Claude Cache
            if self.week2_cache_available and self.smart_claude_cache and response.usage:
                try:
                    from ..optimized.smart_claude_cache import ClaudeRequest, ClaudeResponse
                    
                    # Calculate cost estimate
                    input_tokens = response.usage.input_tokens
                    output_tokens = response.usage.output_tokens
                    cost_estimate = self._calculate_cost_estimate(model, input_tokens, output_tokens)
                    
                    # Create cache request and response
                    cache_request = ClaudeRequest(
                        prompt=prompt,
                        stage=stage,
                        operation=operation,
                        model=model,
                        temperature=temperature,
                        max_tokens=max_tokens
                    )
                    
                    cache_response = ClaudeResponse(
                        content=response.content[0].text,
                        model=model,
                        stage=stage,
                        operation=operation,
                        tokens_used=input_tokens + output_tokens,
                        cost_usd=cost_estimate,
                        response_time=0.0,  # Would need timing measurement
                        confidence_score=1.0,
                        cache_hit=False
                    )
                    
                    # Store in cache
                    self.smart_claude_cache.store_response(cache_request, cache_response)
                    
                except Exception as e:
                    self.logger.warning(f"Failed to store response in smart cache: {e}")

            # Rastrear custos se monitor dispon√≠vel
            if self.cost_monitor and response.usage:
                try:
                    input_tokens = response.usage.input_tokens
                    output_tokens = response.usage.output_tokens
                    self.cost_monitor.record_usage(
                        model=model,
                        input_tokens=input_tokens,
                        output_tokens=output_tokens,
                        stage=stage,
                        operation=operation
                    )
                    
                    # Record performance metrics
                    if self.performance_monitor:
                        cost_estimate = self._calculate_cost_estimate(model, input_tokens, output_tokens)
                        self.performance_monitor.record_stage_completion(
                            stage_name=f"{stage}_{operation}",
                            records_processed=1,
                            processing_time=0.0,  # Would need timing measurement
                            success_rate=1.0,
                            api_calls=1,
                            cost_usd=cost_estimate
                        )
                        
                except Exception as e:
                    self.logger.warning(f"Erro ao registrar custos: {e}")

            return response.content[0].text

        except Exception as e:
            self.logger.error(f"Erro na API Anthropic: {e}")
            
            # Tentar fallback models se dispon√≠veis
            fallback_models = self.get_fallback_models(model)
            for fallback_model in fallback_models:
                try:
                    self.logger.info(f"üîÑ Tentando fallback: {model} ‚Üí {fallback_model}")
                    response = self.client.messages.create(
                        model=fallback_model,
                        max_tokens=kwargs.get('max_tokens', self.max_tokens),
                        temperature=kwargs.get('temperature', self.temperature),
                        messages=[{
                            "role": "user",
                            "content": prompt
                        }]
                    )
                    
                    # Rastrear custos do fallback
                    if self.cost_monitor and response.usage:
                        try:
                            self.cost_monitor.record_usage(
                                model=fallback_model,
                                input_tokens=response.usage.input_tokens,
                                output_tokens=response.usage.output_tokens,
                                stage=stage,
                                operation=operation
                            )
                        except Exception as cost_e:
                            self.logger.warning(f"Erro ao registrar custos fallback: {cost_e}")
                    
                    self.logger.info(f"Fallback bem-sucedido: {fallback_model}")
                    return response.content[0].text
                    
                except Exception as fallback_e:
                    self.logger.warning(f"Fallback {fallback_model} tamb√©m falhou: {fallback_e}")
                    continue
            
            # Marcar API como indispon√≠vel temporariamente se todos os fallbacks falharam
            self.api_available = False

            # Retornar fallback em caso de erro
            fallback_message = kwargs.get('fallback_response',
                f"Erro na API para {stage}:{operation}. Usando processamento tradicional.")
            self.logger.warning(f"API falhou, usando fallback para {stage}:{operation}")
            return fallback_message

    def parse_json_response(self, response: str) -> Dict[str, Any]:
        """
        Extrai JSON de resposta da API com tratamento robusto
        (mant√©m implementa√ß√£o original completa)
        """
        if not response or not response.strip():
            if hasattr(self, 'logger') and self.logger:
                self.logger.error("Resposta vazia da API")
            return {"error": "Resposta vazia da API", "encoding_issues": [], "overall_assessment": {}, "results": []}

        # Log para debug de respostas truncadas
        if hasattr(self, 'logger') and self.logger:
            self.logger.debug(f"Parsing JSON de {len(response)} chars: {response[:100]}...")

        try:
            # Tentar parse direto
            return json.loads(response)
        except json.JSONDecodeError as e:
            if hasattr(self, 'logger') and self.logger:
                self.logger.debug(f"Parse JSON direto falhou: {e}. Aplicando corre√ß√µes robustas...")

            # Clean Claude introductory text from response
            import re
            claude_intro_patterns = [
                r"^Aqui est√° a an√°lise detalhada[^{]*",
                r"^Vou analisar[^{]*",
                r"^An√°lise detalhada[^{]*",
                r"^Segue a an√°lise[^{]*",
                r"^Com base[^{]*",
                r"^Baseado[^{]*"
            ]

            cleaned_response = response
            for pattern in claude_intro_patterns:
                match = re.match(pattern, cleaned_response, re.IGNORECASE | re.DOTALL)
                if match:
                    # Encontrar onde o JSON realmente come√ßa
                    json_start = cleaned_response.find('{')
                    if json_start > 0:
                        if hasattr(self, 'logger') and self.logger:
                            intro_text = cleaned_response[:json_start].strip()
                            self.logger.info(f"üîß Removendo texto introdut√≥rio: '{intro_text[:50]}...'")
                        cleaned_response = cleaned_response[json_start:]
                        try:
                            result = json.loads(cleaned_response)
                            if hasattr(self, 'logger') and self.logger:
                                self.logger.info("JSON parseado ap√≥s remo√ß√£o de introdu√ß√£o")
                            return result
                        except json.JSONDecodeError:
                            pass
                    break

            # Continuar com outras estrat√©gias de parsing...
            # (implementa√ß√£o completa mantida do original)
            
            # Retornar estrutura padr√£o que n√£o quebra o processamento
            return {
                "error": "JSON parse failed after all attempts",
                "response_length": len(response),
                "response_preview": response[:100],
                "encoding_issues": [],
                "overall_assessment": {},
                "results": []
            }

    def parse_json_response_robust(self, response: str, expected_structure: str = "results") -> Dict[str, Any]:
        """Parser JSON ultra-robusto para uso em todos os componentes (mant√©m implementa√ß√£o original)"""
        if not response or not response.strip():
            if hasattr(self, 'logger') and self.logger:
                self.logger.warning("Resposta vazia da API")
            return {expected_structure: []}

        try:
            # Usar o parser principal j√° melhorado
            result = self.parse_json_response(response)

            # Verificar se tem a estrutura esperada
            if isinstance(result, dict):
                if expected_structure in result:
                    if hasattr(self, 'logger') and self.logger:
                        self.logger.info(f"JSON parseado com sucesso - estrutura '{expected_structure}' encontrada")
                    return result
                else:
                    # Tentar adaptar estrutura
                    if hasattr(self, 'logger') and self.logger:
                        self.logger.warning(f"Estrutura '{expected_structure}' n√£o encontrada, adaptando...")

                    # Se result parece ser um item individual, transformar em lista
                    if isinstance(result, dict) and len(result) > 0:
                        adapted_result = {expected_structure: [result]}
                        if hasattr(self, 'logger') and self.logger:
                            self.logger.info(f"üìù Estrutura adaptada: item individual -> lista")
                        return adapted_result
                    else:
                        # Estrutura vazia ou inesperada
                        return {expected_structure: []}
            else:
                if hasattr(self, 'logger') and self.logger:
                    self.logger.warning(f"Resposta n√£o √© dicion√°rio: {type(result)}")
                return {expected_structure: []}

        except Exception as e:
            if hasattr(self, 'logger') and self.logger:
                self.logger.error(f"Erro no parser robusto: {e}")
                self.logger.error(f"Resposta (primeiros 200 chars): {response[:200]}")
            return {expected_structure: []}

    def parse_claude_response_safe(self, response: str, expected_keys: Optional[List[str]] = None) -> Dict[str, Any]:
        """Parser ultra-seguro para qualquer resposta da Claude API (mant√©m implementa√ß√£o original)"""
        if expected_keys is None:
            expected_keys = ["results"]

        try:
            # Usar o parser robusto existente
            parsed = self.parse_json_response(response)

            # Verificar se √© um dicion√°rio v√°lido
            if not isinstance(parsed, dict):
                if hasattr(self, 'logger'):
                    self.logger.warning(f"Resposta n√£o √© dicion√°rio: {type(parsed)}")
                return self._create_safe_response(expected_keys)

            # Verificar se tem as chaves esperadas
            missing_keys = [key for key in expected_keys if key not in parsed]
            if missing_keys:
                if hasattr(self, 'logger'):
                    self.logger.warning(f"Chaves ausentes: {missing_keys}")
                # Adicionar chaves faltantes
                for key in missing_keys:
                    parsed[key] = [] if key in ['results', 'items', 'data'] else {}

            if hasattr(self, 'logger'):
                self.logger.info("Claude response parseada com sucesso")
            return parsed

        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.error(f"Erro cr√≠tico no parsing Claude: {e}")
            return self._create_safe_response(expected_keys)

    def _create_safe_response(self, expected_keys: List[str]) -> Dict[str, Any]:
        """Cria resposta segura com estrutura padr√£o"""
        safe_response = {
            "error": "Failed to parse Claude response",
            "parsed_successfully": False
        }

        # Adicionar chaves esperadas com valores padr√£o
        for key in expected_keys:
            if key in ['results', 'items', 'data', 'analysis_results']:
                safe_response[key] = []
            elif key in ['analysis', 'assessment', 'statistics', 'summary']:
                safe_response[key] = {}
            else:
                safe_response[key] = None

        return safe_response

    def process_batch(self, items: List[Any], batch_size: int, process_func, **kwargs) -> List[Any]:
        """Processa itens em lotes (mant√©m implementa√ß√£o original)"""
        results = []
        total_batches = (len(items) + batch_size - 1) // batch_size

        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            batch_num = i // batch_size + 1

            self.logger.info(f"Processando lote {batch_num}/{total_batches}")

            try:
                batch_results = process_func(batch, **kwargs)
                if batch_results:
                    results.extend(batch_results)
            except Exception as e:
                self.logger.error(f"Erro no lote {batch_num}: {e}")
                # Continuar com pr√≥ximo lote
                continue

        return results

class APIUsageTracker:
    """Rastreador de uso da API para controle de custos (mant√©m implementa√ß√£o original)"""

    def __init__(self):
        self.usage = {
            'total_requests': 0,
            'total_tokens': 0,
            'cost_estimate': 0.0,
            'requests_by_module': {}
        }

    def log_request(self, module: str, tokens: int, model: str = "claude-3-5-haiku-20241022"):
        """Registra uso da API"""
        self.usage['total_requests'] += 1
        self.usage['total_tokens'] += tokens

        # Estimativa de custo (ajustar conforme pricing atual)
        if model == "claude-sonnet-4-20250514":
            # Claude Sonnet 4: $3.00 entrada, $15.00 sa√≠da
            # Assumindo propor√ß√£o 2:1 (entrada:sa√≠da)
            avg_cost_per_token = 7.00 / 1_000_000  # M√©dia ponderada
            cost = tokens * avg_cost_per_token
        elif model == "claude-3-5-haiku-20241022":
            # $0.25 por milh√£o de tokens de entrada
            cost = (tokens / 1_000_000) * 0.25
        else:
            cost = 0

        self.usage['cost_estimate'] += cost

        if module not in self.usage['requests_by_module']:
            self.usage['requests_by_module'][module] = {
                'requests': 0,
                'tokens': 0,
                'cost': 0.0
            }

        self.usage['requests_by_module'][module]['requests'] += 1
        self.usage['requests_by_module'][module]['tokens'] += tokens
        self.usage['requests_by_module'][module]['cost'] += cost

    def get_summary(self) -> Dict[str, Any]:
        """Retorna resumo do uso"""
        return self.usage

    def save_report(self, filepath: str):
        """Salva relat√≥rio de uso"""
        with open(filepath, 'w') as f:
            json.dump(self.usage, f, indent=2)