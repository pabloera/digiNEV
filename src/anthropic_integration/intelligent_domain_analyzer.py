"""
Intelligent Domain Analyzer com API Anthropic

Módulo avançado para análise de domínios com classificação semântica.
Analisa URLs, classifica fontes e identifica padrões de consumo de mídia.
"""

import json
import logging
import re
from collections import Counter, defaultdict
from typing import Any, Dict, List, Optional, Set, Tuple
from urllib.parse import urlparse

import numpy as np
import pandas as pd

from .base import AnthropicBase

logger = logging.getLogger(__name__)

class IntelligentDomainAnalyzer(AnthropicBase):
    """
    Analisador inteligente de domínios usando API Anthropic

    Funcionalidades:
    - Extração e classificação automática de domínios
    - Análise semântica de credibilidade de fontes
    - Identificação de padrões de desinformação
    - Classificação de tipos de mídia
    - Análise de redes de compartilhamento
    """

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.logger = logging.getLogger(self.__class__.__name__)

        # Configurações específicas
        domain_config = config.get('domain_analysis', {})
        self.min_domain_frequency = domain_config.get('min_frequency', 5)
        self.classification_batch_size = domain_config.get('batch_size', 30)
        self.credibility_threshold = domain_config.get('credibility_threshold', 0.7)

    def analyze_domains_intelligent(self, df: pd.DataFrame, url_column: str = 'urls') -> Dict[str, Any]:
        """
        Análise inteligente completa de domínios

        Args:
            df: DataFrame com dados
            url_column: Coluna contendo URLs

        Returns:
            Análise completa de domínios
        """
        self.logger.info("Iniciando análise inteligente de domínios")

        if url_column not in df.columns:
            return {'error': f'Coluna {url_column} não encontrada'}

        # Etapa 1: Extração de domínios
        domain_extraction = self._extract_domains_comprehensive(df, url_column)

        # Etapa 2: Classificação AI dos domínios
        domain_classification = self._classify_domains_with_ai(domain_extraction['domain_stats'])

        # Etapa 3: Análise de credibilidade
        credibility_analysis = self._analyze_source_credibility(
            domain_extraction['top_domains'],
            domain_classification
        )

        # Etapa 4: Padrões de compartilhamento
        sharing_patterns = self._analyze_sharing_patterns(df, url_column, domain_extraction)

        # Etapa 5: Insights contextuais
        contextual_insights = self._generate_domain_insights(
            domain_extraction, domain_classification, credibility_analysis, sharing_patterns
        )

        return {
            'domain_extraction': domain_extraction,
            'domain_classification': domain_classification,
            'credibility_analysis': credibility_analysis,
            'sharing_patterns': sharing_patterns,
            'contextual_insights': contextual_insights,
            'analysis_summary': self._generate_domain_summary(
                domain_extraction, domain_classification, credibility_analysis
            )
        }

    def _extract_domains_comprehensive(self, df: pd.DataFrame, url_column: str) -> Dict[str, Any]:
        """
        Extração abrangente de domínios

        Args:
            df: DataFrame com dados
            url_column: Coluna de URLs

        Returns:
            Estatísticas de domínios extraídos
        """
        self.logger.info("Extraindo domínios das URLs")

        # Coletar todas as URLs
        all_urls = []
        url_to_index = {}  # Mapear URL para índices do DataFrame

        for idx, urls_str in enumerate(df[url_column].fillna('')):
            if urls_str and isinstance(urls_str, str):
                # Extrair URLs da string
                urls = self._extract_urls_from_text(urls_str)
                for url in urls:
                    all_urls.append(url)
                    if url not in url_to_index:
                        url_to_index[url] = []
                    url_to_index[url].append(idx)

        # Extrair domínios
        domain_stats = defaultdict(lambda: {'count': 0, 'urls': set(), 'first_seen_index': None})

        for url in all_urls:
            domain = self._extract_domain(url)
            if domain:
                domain_stats[domain]['count'] += 1
                domain_stats[domain]['urls'].add(url)
                if domain_stats[domain]['first_seen_index'] is None:
                    domain_stats[domain]['first_seen_index'] = min(url_to_index[url])

        # Converter para formato serializável
        domain_list = []
        for domain, stats in domain_stats.items():
            domain_list.append({
                'domain': domain,
                'count': stats['count'],
                'unique_urls': len(stats['urls']),
                'sample_urls': list(stats['urls'])[:5],
                'first_seen_index': stats['first_seen_index']
            })

        # Ordenar por frequência
        domain_list.sort(key=lambda x: x['count'], reverse=True)

        # Filtrar por frequência mínima
        frequent_domains = [d for d in domain_list if d['count'] >= self.min_domain_frequency]

        return {
            'total_urls_found': len(all_urls),
            'unique_domains': len(domain_list),
            'frequent_domains_count': len(frequent_domains),
            'domain_stats': frequent_domains,
            'top_domains': frequent_domains[:50],  # Top 50 para análise AI
            'url_to_index_mapping': url_to_index
        }

    def _extract_urls_from_text(self, text: str) -> List[str]:
        """
        Extrai URLs de um texto

        Args:
            text: Texto contendo URLs

        Returns:
            Lista de URLs encontradas
        """
        # Padrão para URLs
        url_pattern = r'https?://[^\s<>"{}|\\^`[\]]+|www\.[^\s<>"{}|\\^`[\]]+'
        urls = re.findall(url_pattern, text, re.IGNORECASE)

        # Limpar URLs
        cleaned_urls = []
        for url in urls:
            url = url.strip('.,;:!?)')
            if not url.startswith('http'):
                url = 'http://' + url
            cleaned_urls.append(url)

        return cleaned_urls

    def _extract_domain(self, url: str) -> Optional[str]:
        """
        Extrai domínio de uma URL

        Args:
            url: URL para extrair domínio

        Returns:
            Domínio extraído ou None
        """
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()

            # Remover 'www.' se presente
            if domain.startswith('www.'):
                domain = domain[4:]

            return domain if domain else None

        except Exception:
            return None

    def _classify_domains_with_ai(self, domain_stats: List[Dict]) -> Dict[str, Any]:
        """
        Classifica domínios usando AI

        Args:
            domain_stats: Estatísticas de domínios

        Returns:
            Classificação de domínios
        """
        if not domain_stats:
            return {'classifications': []}

        self.logger.info("Classificando domínios com AI")

        # Processar em batches
        all_classifications = []
        total_batches = (len(domain_stats) + self.classification_batch_size - 1) // self.classification_batch_size

        for batch_idx in range(min(total_batches, 5)):  # Limitar a 5 batches
            start_idx = batch_idx * self.classification_batch_size
            end_idx = min(start_idx + self.classification_batch_size, len(domain_stats))

            batch_domains = domain_stats[start_idx:end_idx]
            batch_classifications = self._classify_domain_batch(batch_domains)

            if batch_classifications:
                all_classifications.extend(batch_classifications)

            self.logger.info(f"Batch {batch_idx + 1}/{min(total_batches, 5)} classificado")

        # Agregar resultados
        classification_summary = self._aggregate_classifications(all_classifications)

        return {
            'classifications': all_classifications,
            'summary': classification_summary,
            'total_classified': len(all_classifications)
        }

    def _classify_domain_batch(self, domains: List[Dict]) -> List[Dict]:
        """
        Classifica um batch de domínios

        Args:
            domains: Lista de domínios para classificar

        Returns:
            Lista de classificações
        """
        domain_list = [f"{d['domain']} (freq: {d['count']})" for d in domains]

        prompt = f"""
Classifique os seguintes domínios brasileiros encontrados em mensagens do Telegram político (2019-2023):

DOMÍNIOS: {', '.join(domain_list)}

Para cada domínio, determine:
1. Tipo de mídia (jornal_tradicional, site_notícias, blog, rede_social, governo, alternativa, questionável)
2. Orientação política (esquerda, centro-esquerda, centro, centro-direita, direita, extrema-direita, neutro)
3. Credibilidade (alta, média, baixa, questionável)
4. Especialização (política, geral, saúde, economia, entretenimento, desinformação)
5. Alcance (nacional, regional, local, internacional)

Contexto: Período inclui governo Bolsonaro, pandemia COVID-19, eleições 2022.

Responda em JSON:
{{
    "domain_classifications": [
        {{
            "domain": "exemplo.com.br",
            "media_type": "jornal_tradicional",
            "political_orientation": "centro",
            "credibility": "alta",
            "specialization": "política",
            "reach": "nacional",
            "notes": "observações_específicas"
        }}
    ]
}}
"""

        try:
            response = self.create_message(
                prompt=prompt,
                stage='09_domain_classification',
                operation='classify_domains'
            )

            result = self.parse_claude_response_safe(response, ["domain_classifications"])
            return result.get('domain_classifications', [])

        except Exception as e:
            self.logger.error(f"Erro na classificação de domínios: {e}")
            return []

    def _analyze_source_credibility(self, top_domains: List[Dict],
                                  classifications: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analisa credibilidade das fontes

        Args:
            top_domains: Top domínios por frequência
            classifications: Classificações de domínios

        Returns:
            Análise de credibilidade
        """
        if not classifications.get('classifications'):
            return {'analysis': 'no_classifications_available'}

        # Preparar dados para análise
        credibility_data = []
        for classification in classifications['classifications']:
            domain = classification.get('domain', '')
            credibility = classification.get('credibility', 'unknown')
            media_type = classification.get('media_type', 'unknown')

            # Encontrar frequência do domínio
            domain_freq = 0
            for d in top_domains:
                if d['domain'] == domain:
                    domain_freq = d['count']
                    break

            credibility_data.append({
                'domain': domain,
                'credibility': credibility,
                'media_type': media_type,
                'frequency': domain_freq
            })

        prompt = f"""
Analise a credibilidade das fontes de informação mais compartilhadas:

DADOS DE CREDIBILIDADE:
{json.dumps(credibility_data[:20], ensure_ascii=False, indent=2)}

Analise:
1. Distribuição de credibilidade das fontes
2. Correlação entre frequência e credibilidade
3. Predominância de tipos de mídia
4. Riscos de desinformação
5. Qualidade geral do ecossistema informacional

Responda em JSON:
{{
    "credibility_distribution": {{
        "alta": 5,
        "média": 8,
        "baixa": 4,
        "questionável": 3
    }},
    "risk_assessment": {{
        "desinformation_risk": "alto|médio|baixo",
        "echo_chamber_risk": "alto|médio|baixo",
        "polarization_risk": "alto|médio|baixo"
    }},
    "key_findings": [
        "descoberta_principal_1",
        "descoberta_principal_2"
    ],
    "recommendations": [
        "recomendação_1",
        "recomendação_2"
    ]
}}
"""

        try:
            response = self.create_message(
                prompt=prompt,
                stage='09_credibility_analysis',
                operation='analyze_credibility'
            )

            analysis = self.parse_claude_response_safe(response, ["credibility_distribution", "risk_assessment", "key_findings", "recommendations"])
            return analysis

        except Exception as e:
            self.logger.error(f"Erro na análise de credibilidade: {e}")
            return {'error': str(e)}

    def _analyze_sharing_patterns(self, df: pd.DataFrame, url_column: str,
                                domain_extraction: Dict) -> Dict[str, Any]:
        """
        Analisa padrões de compartilhamento de domínios

        Args:
            df: DataFrame original
            url_column: Coluna de URLs
            domain_extraction: Dados de extração de domínios

        Returns:
            Análise de padrões de compartilhamento
        """
        # Análise temporal se coluna de timestamp disponível
        temporal_analysis = {}
        if 'timestamp' in df.columns:
            temporal_analysis = self._analyze_temporal_domain_patterns(df, url_column)

        # Análise por canal se disponível
        channel_analysis = {}
        if 'canal' in df.columns:
            channel_analysis = self._analyze_domain_by_channel(df, url_column)

        # Top domínios compartilhados
        top_shared = domain_extraction['top_domains'][:20]

        return {
            'temporal_patterns': temporal_analysis,
            'channel_patterns': channel_analysis,
            'top_shared_domains': top_shared,
            'sharing_diversity': self._calculate_sharing_diversity(domain_extraction)
        }

    def _analyze_temporal_domain_patterns(self, df: pd.DataFrame, url_column: str) -> Dict[str, Any]:
        """Analisa padrões temporais de compartilhamento"""
        # Implementação simplificada
        return {'note': 'temporal_analysis_placeholder'}

    def _analyze_domain_by_channel(self, df: pd.DataFrame, url_column: str) -> Dict[str, Any]:
        """Analisa domínios por canal"""
        # Implementação simplificada
        return {'note': 'channel_analysis_placeholder'}

    def _calculate_sharing_diversity(self, domain_extraction: Dict) -> Dict[str, Any]:
        """Calcula diversidade de compartilhamento"""
        domains = domain_extraction['domain_stats']

        if not domains:
            return {'diversity_score': 0}

        # Calcular índice de concentração (Herfindahl)
        total_shares = sum(d['count'] for d in domains)
        concentrations = [(d['count'] / total_shares) ** 2 for d in domains]
        herfindahl_index = sum(concentrations)

        # Diversidade (1 - concentração)
        diversity_score = 1 - herfindahl_index

        return {
            'diversity_score': diversity_score,
            'total_domains': len(domains),
            'concentration_index': herfindahl_index,
            'interpretation': 'alta_diversidade' if diversity_score > 0.8 else 'baixa_diversidade' if diversity_score < 0.3 else 'média_diversidade'
        }

    def _generate_domain_insights(self, domain_extraction: Dict, classification: Dict,
                                credibility: Dict, sharing: Dict) -> Dict[str, Any]:
        """
        Gera insights contextuais sobre domínios

        Args:
            domain_extraction: Dados de extração
            classification: Classificações
            credibility: Análise de credibilidade
            sharing: Padrões de compartilhamento

        Returns:
            Insights contextuais
        """
        # Preparar dados para insights
        total_domains = domain_extraction['unique_domains']
        top_domains = domain_extraction['top_domains'][:10]
        credibility_summary = credibility.get('credibility_distribution', {})

        prompt = f"""
Gere insights sobre o ecossistema de informação baseado na análise de domínios:

DADOS:
- Total de domínios únicos: {total_domains}
- Top 10 domínios: {[d['domain'] for d in top_domains]}
- Distribuição de credibilidade: {json.dumps(credibility_summary, ensure_ascii=False)}

CONTEXTO: Mensagens do Telegram brasileiro (2019-2023) sobre política, incluindo período Bolsonaro, pandemia, eleições.

Gere insights sobre:
1. Qualidade do ecossistema informacional
2. Riscos de desinformação
3. Padrões de consumo de mídia
4. Implicações para democracia
5. Recomendações para pesquisa

Responda em JSON:
{{
    "ecosystem_health": {{
        "overall_quality": "alta|média|baixa",
        "diversity_level": "alta|média|baixa",
        "desinformation_risk": "alto|médio|baixo"
    }},
    "key_insights": [
        "insight_principal_1",
        "insight_principal_2"
    ],
    "democratic_implications": [
        "implicação_1",
        "implicação_2"
    ],
    "research_recommendations": [
        "recomendação_pesquisa_1",
        "recomendação_pesquisa_2"
    ]
}}
"""

        try:
            response = self.create_message(
                prompt=prompt,
                stage='09_domain_insights',
                operation='generate_insights'
            )

            insights = self.parse_claude_response_safe(response, ["ecosystem_health", "key_insights", "democratic_implications", "research_recommendations"])
            return insights

        except Exception as e:
            self.logger.error(f"Erro na geração de insights: {e}")
            return {'error': str(e)}

    def _aggregate_classifications(self, classifications: List[Dict]) -> Dict[str, Any]:
        """
        Agrega resultados de classificação

        Args:
            classifications: Lista de classificações

        Returns:
            Resumo agregado
        """
        if not classifications:
            return {}

        # Contar por categoria
        media_types = Counter(c.get('media_type', 'unknown') for c in classifications)
        credibility_levels = Counter(c.get('credibility', 'unknown') for c in classifications)
        political_orientations = Counter(c.get('political_orientation', 'unknown') for c in classifications)

        return {
            'media_types': dict(media_types),
            'credibility_levels': dict(credibility_levels),
            'political_orientations': dict(political_orientations),
            'total_classified': len(classifications)
        }

    def _generate_domain_summary(self, extraction: Dict, classification: Dict,
                               credibility: Dict) -> Dict[str, Any]:
        """
        Gera resumo final da análise de domínios
        """
        return {
            'total_urls_analyzed': extraction['total_urls_found'],
            'unique_domains_found': extraction['unique_domains'],
            'domains_classified': classification.get('total_classified', 0),
            'credibility_analysis_completed': 'credibility_distribution' in credibility,
            'analysis_quality': 'ai_enhanced' if classification.get('classifications') else 'basic_extraction',
            'high_frequency_domains': len(extraction['domain_stats']),
            'methodology': 'intelligent_domain_analysis_with_ai'
        }

def get_intelligent_domain_analyzer(config: Dict[str, Any]) -> IntelligentDomainAnalyzer:
    """
    Factory function para criar instância do IntelligentDomainAnalyzer
    """
    return IntelligentDomainAnalyzer(config)
