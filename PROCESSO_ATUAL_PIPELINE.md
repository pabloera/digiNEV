# PROCESSO ATUAL DO PIPELINE - digiNEV v.final

**Data de atualizaÃ§Ã£o:** 04 de outubro de 2025
**Status:** âœ… Operacional e consolidado
**VersÃ£o:** v.final com otimizaÃ§Ãµes 5.0.0

## ğŸ¯ STATUS ATUAL DO SISTEMA

### Pipeline Consolidado (17 Stages)
```
âœ… FUNCIONAL: 17 stages executando sequencialmente
âœ… VALIDADO: 102 colunas geradas com dados reais
âœ… TESTADO: ValidaÃ§Ã£o completa com controlled_test_100.csv
âœ… OTIMIZADO: 5/5 semanas de otimizaÃ§Ã£o ativas (100%)
```

### Arquivos Principais Funcionais
- `src/analyzer.py` - Pipeline principal (17 stages sequenciais)
- `run_pipeline.py` - Executor principal com descoberta automÃ¡tica
- `test_clean_analyzer.py` - Sistema de validaÃ§Ã£o funcional
- `data/` - 11 datasets vÃ¡lidos (0.0 MB a 230 MB)

## ğŸš€ COMO EXECUTAR AGORA

### ExecuÃ§Ã£o PadrÃ£o (Todos os Datasets)
```bash
python run_pipeline.py
```

### ExecuÃ§Ã£o com Dataset EspecÃ­fico
```bash
python run_pipeline.py --dataset data/controlled_test_100.csv
python run_pipeline.py --dataset data/1_2019-2021-govbolso.csv
```

### Teste de ValidaÃ§Ã£o
```bash
python test_clean_analyzer.py
```

### Dashboard (VisualizaÃ§Ã£o)
```bash
python src/dashboard/start_dashboard.py
```

## ğŸ“Š PIPELINE SEQUENCIAL OTIMIZADO (17 STAGES)

### FASE 1: PREPARAÃ‡ÃƒO E ESTRUTURA (01-02)
```
STAGE 01: Feature Extraction
- DetecÃ§Ã£o automÃ¡tica de colunas (text, timestamp)
- ExtraÃ§Ã£o de features bÃ¡sicas (hashtags, URLs, mentions, emojis)
- PadronizaÃ§Ã£o de datetime

STAGE 02: Text Preprocessing
- NormalizaÃ§Ã£o de texto em portuguÃªs
- Limpeza bÃ¡sica
- ValidaÃ§Ã£o de features
```

### FASE 2: REDUÃ‡ÃƒO DE VOLUME (03-06) - CRÃTICO
```
STAGE 03: Cross-Dataset Deduplication
- ReduÃ§Ã£o: 40-50% (300k â†’ 180k)
- Agrupa textos idÃªnticos, mantÃ©m mais antigo
- Contador dupli_freq

STAGE 04: Statistical Analysis
- ComparaÃ§Ã£o antes/depois reduÃ§Ã£o
- EstatÃ­sticas de qualidade e duplicaÃ§Ã£o
- DetecÃ§Ã£o de padrÃµes

STAGE 05: Content Quality Filter
- ReduÃ§Ã£o: 15-25% (180k â†’ 135k)
- Filtros: comprimento, emoji_ratio, caps_ratio, idioma
- Score de qualidade 0-100

STAGE 06: Political Relevance Filter
- ReduÃ§Ã£o: 30-40% (135k â†’ 80k)
- ClassificaÃ§Ã£o polÃ­tica brasileira
- Manter apenas conteÃºdo polÃ­tico relevante
```

### FASE 3: ANÃLISE LINGUÃSTICA (07-09) - VOLUME OTIMIZADO
```
STAGE 07: Linguistic Processing (spaCy)
- Processamento com pt_core_news_lg
- Tokens, lemmas, POS tags, entidades

STAGE 08: Political Classification
- ClassificaÃ§Ã£o polÃ­tica brasileira detalhada
- extrema-direita, direita, centro, esquerda, neutral

STAGE 09: TF-IDF Vectorization
- VetorizaÃ§Ã£o com tokens spaCy
- Top termos por documento
```

### FASE 4: ANÃLISES AVANÃ‡ADAS (10-17)
```
STAGE 10: Clustering Analysis
- K-Means clustering
- AnÃ¡lise de distÃ¢ncias

STAGE 11: Topic Modeling
- LDA topic modeling
- Probabilidades por tÃ³pico

STAGE 12: Semantic Analysis
- AnÃ¡lise semÃ¢ntica avanÃ§ada
- Conectivos e modalidade

STAGE 13: Temporal Analysis
- AnÃ¡lise temporal (hour, day, month)
- PadrÃµes temporais

STAGE 14: Network Analysis
- CoordenaÃ§Ã£o de rede
- PadrÃµes de propagaÃ§Ã£o

STAGE 15: Domain Analysis
- AnÃ¡lise de domÃ­nios e URLs
- ClassificaÃ§Ã£o de fontes

STAGE 16: Event Context Analysis
- Contexto de eventos polÃ­ticos brasileiros
- DetecÃ§Ã£o de contextos eleitorais

STAGE 17: Channel Analysis
- AnÃ¡lise de canais/fontes
- ClassificaÃ§Ã£o de autoridade
```

## ğŸ“ DATASETS DISPONÃVEIS

### Datasets Principais (data/)
```
1. controlled_test_100.csv (0.0 MB) - Teste validado
2. 1_2019-2021-govbolso.csv (135.9 MB) - PerÃ­odo Bolsonaro
3. 2_2021-2022-pandemia.csv (230.0 MB) - Pandemia
4. 3_2022-2023-poseleic.csv (93.2 MB) - PÃ³s-eleiÃ§Ã£o
5. 4_2022-2023-elec.csv (54.2 MB) - EleiÃ§Ãµes
6. 5_2022-2023-elec-extra.csv (25.2 MB) - Dados extras
```

### Datasets Processados (data/processed/)
```
- processed_1_2019-2021-govbolso.csv
- processed_2_2021-2022-pandemia.csv
```

## âš¡ OTIMIZAÃ‡Ã•ES ATIVAS (5.0.0)

### Week 1-2: Emergency Cache + Advanced Caching
- âœ… Cache inteligente de stages
- âœ… Checkpoints automÃ¡ticos

### Week 3: Parallelization + Streaming
- âœ… Processamento paralelo integrado
- âœ… Streaming de dados grandes

### Week 4: Real-time Monitoring
- âœ… Monitoramento em tempo real
- âœ… Logs detalhados

### Week 5: Memory Management
- âœ… GestÃ£o de memÃ³ria otimizada
- âœ… Auto-chunking para datasets grandes

## ğŸ”§ SAÃDA DE DADOS (102 COLUNAS)

### Colunas Estruturais
```
id, body, channel, user_id, message_id, datetime
main_text_column, timestamp_column, has_timestamp
```

### Features ExtraÃ­das
```
hashtags_extracted, hashtags_count, urls_extracted, urls_count
mentions_extracted, mentions_count, emojis_extracted, emojis_count
```

### Processamento de Texto
```
normalized_text, text_cleaned, dupli_freq, channels_found
char_count, word_count, emoji_ratio, caps_ratio, repetition_ratio
likely_portuguese, content_quality_score, language_confidence
```

### AnÃ¡lise PolÃ­tica
```
political_orientation, political_keywords, political_intensity
political_relevance_score, political_terms_found
```

### AnÃ¡lise LinguÃ­stica (spaCy)
```
spacy_tokens, spacy_lemmas, spacy_pos_tags, spacy_entities
spacy_tokens_count, spacy_entities_count, lemmatized_text
```

### TF-IDF e Clustering
```
tfidf_score_mean, tfidf_score_max, tfidf_top_terms
cluster_id, cluster_distance, cluster_size
```

### Topic Modeling
```
dominant_topic, topic_probability, topic_keywords
```

### AnÃ¡lise SemÃ¢ntica
```
sentiment_polarity, sentiment_label, emotion_intensity
has_aggressive_language, semantic_diversity
```

### AnÃ¡lise Temporal
```
hour, day_of_week, month, year, day_of_year
is_weekend, is_business_hours
```

### AnÃ¡lise de Rede
```
sender_frequency, is_frequent_sender, shared_url_frequency
temporal_coordination
```

### AnÃ¡lise de DomÃ­nios
```
domain_type, domain_frequency, is_mainstream_media
url_count, has_external_links
```

### AnÃ¡lise de Contexto
```
political_context, mentions_government, mentions_opposition
election_context, protest_context
```

### AnÃ¡lise de Canais
```
channel_type, channel_activity, is_active_channel
content_type, has_media, is_forwarded, forwarding_context
sender_channel_influence
```

### Metadados
```
processing_timestamp, stages_completed, features_extracted
```

## ğŸ§ª VALIDAÃ‡ÃƒO ATUAL

### Teste Funcional (test_clean_analyzer.py)
```bash
ğŸ”¬ TESTE: Analyzer v.final com dados reais
============================================================
ğŸ“„ Dataset real carregado: 100 registros, 6 colunas
âœ… RESULTADO DA ANÃLISE:
ğŸ“Š Colunas geradas: 102
ğŸ¯ Stages completados: 17/10
ğŸ”§ Features extraÃ­das: 81

ğŸ”— VERIFICAÃ‡ÃƒO DE INTERLIGAÃ‡ÃƒO ENTRE STAGES:
âœ… Todos os stages executados sequencialmente
âœ… Cada stage usa dados dos stages anteriores
âœ… Nenhum reprocessamento desnecessÃ¡rio
âœ… Todas as 102 colunas contÃªm dados reais
âœ… Pipeline totalmente interligado
```

## ğŸ“ˆ PERFORMANCE ATUAL

### Processamento Sequencial Otimizado
- **ReduÃ§Ã£o de Volume:** 40-50% â†’ 15-25% â†’ 30-40% = ~80% reduÃ§Ã£o final
- **Stages LinguÃ­sticos:** Apenas no volume otimizado (economia de 80% de processamento)
- **MemÃ³ria:** Auto-chunking para datasets > 4GB
- **Tempo:** Processamento inteligente por fases

### Exemplo de ExecuÃ§Ã£o
```
Initial: 300,000 registros
â†’ Stage 03: 180,000 (deduplicaÃ§Ã£o)
â†’ Stage 05: 135,000 (qualidade)
â†’ Stage 06: 80,000 (relevÃ¢ncia polÃ­tica)
â†’ Stages 07-17: Processamento linguÃ­stico otimizado
```

## ğŸ›¡ï¸ VALIDAÃ‡ÃƒO E CONTROLE

### Checkpoints AutomÃ¡ticos
- Salvamento automÃ¡tico entre stages
- Retomada de execuÃ§Ã£o em caso de falha

### ValidaÃ§Ã£o de Dados
- VerificaÃ§Ã£o de integridade em cada stage
- Logs detalhados de transformaÃ§Ãµes

### ProteÃ§Ã£o de Stages
- Stages crÃ­ticos protegidos contra reprocessamento
- Sistema de flags de proteÃ§Ã£o

## ğŸš¨ RESOLUÃ‡ÃƒO DE PROBLEMAS

### Erro "Error tokenizing data"
```bash
# Usar dataset menor para teste
python run_pipeline.py --dataset data/controlled_test_100.csv
```

### Erro de memÃ³ria
```bash
# O sistema usa auto-chunking automaticamente
# Configurado para datasets atÃ© 4GB
```

### Pipeline nÃ£o encontra datasets
```bash
# Verificar se os arquivos estÃ£o em data/
ls data/*.csv
```

## ğŸ“ LOGS E MONITORAMENTO

### Logs Detalhados
```
INFO:Analyzer:ğŸ”¬ Iniciando anÃ¡lise OTIMIZADA: X registros
INFO:Analyzer:ğŸ” STAGE 01: Feature Extraction
INFO:Analyzer:ğŸ“… Padronizando datetime...
INFO:Analyzer:âœ… Stage XX concluÃ­do: Y registros processados
```

### MÃ©tricas de Performance
```
â±ï¸ Total duration: X.Xs
ğŸ“Š Datasets processed: X
ğŸ“ˆ Records processed: X
ğŸ”§ Stages executed: 17
```

## ğŸ¯ PRÃ“XIMOS PASSOS

1. **ExecuÃ§Ã£o com Datasets Completos:**
   ```bash
   python run_pipeline.py --dataset data/1_2019-2021-govbolso.csv
   ```

2. **AnÃ¡lise dos Resultados:**
   ```bash
   python src/dashboard/start_dashboard.py
   ```

3. **Processamento em Lote:**
   ```bash
   python run_pipeline.py  # Todos os datasets
   ```

---

**Status:** âœ… Pipeline operacional e documentado
**Ãšltima validaÃ§Ã£o:** 04/10/2025
**Commit:** d9acb89 - feat: Resume and consolidate pipeline processing