# Pipeline Stages Analysis - digiNEV v.final Optimized
## An√°lise Detalhada dos 17 Stages do Sistema Cient√≠fico

---

## üéØ OVERVIEW GERAL

**PROBLEMA RESOLVIDO:** O pipeline original processava 300k+ textos com spaCy ANTES de qualquer filtro, causando travamentos. A nova sequ√™ncia otimizada reduz o volume em 60-80% ANTES do processamento lingu√≠stico pesado.

**ESTRAT√âGIA:** Dividir em 4 fases sequenciais com redu√ß√£o progressiva de volume.

---

## üìä FASES E IMPACTO ESPERADO

| Fase | Stages | Volume Estimado | Redu√ß√£o | Tempo |
|------|--------|----------------|---------|-------|
| **Fase 1** | 01-03 | 300k ‚Üí 300k | 0% | ~2 min |
| **Fase 2** | 04-06 | 300k ‚Üí 80k | 73% | ~3 min |
| **Fase 3** | 07-09 | 80k ‚Üí 80k | 0% | ~8 min |
| **Fase 4** | 10-17 | 80k ‚Üí 80k | 0% | ~5 min |
| **TOTAL** | 17 | 300k ‚Üí 80k | **73%** | **~18 min** |

---
# FASE 0: PROCESSAMENTO EM CHUNKS DOS DATASETS

# FASE 1: PREPARA√á√ÉO E ESTRUTURA (01-03)
*Objetivo: Estruturar dados e preparar para filtros*

## STAGE 01: Feature Extraction
**Fun√ß√£o:** Estrutura√ß√£o inicial e padroniza√ß√£o de dados
**Input:** Dataset bruto CSV
**Output:** Estrutura padronizada com datetime brasileiro

### Processamentos:
- ‚úÖ Detec√ß√£o autom√°tica de separador (`,` ou `;`)
- ‚úÖ Padroniza√ß√£o datetime para DD/MM/AAAA HH:MM:SS
- ‚úÖ Identifica√ß√£o de colunas principais (body, timestamp)
- ‚úÖ Detec√ß√£o de features existentes (hashtags, urls, mentions)
- ‚úÖ Extra√ß√£o b√°sica de emojis
- ‚úÖ Cria√ß√£o de metadados estruturais

### Colunas Geradas:
- `datetime` (padronizado)
- `emojis_extracted`
- `emojis_count`
- `main_text_column`
- `timestamp_column`
- `metadata_columns_count`
# e se n√£o houver no dataset, criar:
- `has_timestamp`
- `has_url`
- `has_hashtags`
- `has_channel`
- `has_mention`
# se j√° houver no dataset, conferir se os itens apresentados est√£o corretos, e se n√£o estiverem, corrigir.


### Criticidade: **ALTA** - Base para todo o pipeline

---

## STAGE 02: Text Preprocessing
**Fun√ß√£o:** Limpeza e normaliza√ß√£o de texto
**Input:** Dados estruturados do Stage 01 (dataframe)
**Output:** Texto limpo e normalizado

### Processamentos:
- ‚úÖ Valida√ß√£o de features existentes vs conte√∫do
- ‚úÖ Remo√ß√£o de duplica√ß√µes desnecess√°rias
- ‚úÖ Normaliza√ß√£o de texto (URLs, men√ß√µes, quebras de linha)
- ‚úÖ Limpeza de caracteres especiais
- ‚úÖ Corre√ß√£o de encoding
- ‚úÖ Prepara√ß√£o para an√°lise posterior

### Colunas Geradas:
- `normalized_text` (principal para an√°lises)
- Corre√ß√µes aplicadas em features existentes

### Criticidade: **ALTA** - Qualidade do texto impacta todo pipeline


---

# FASE 2: REDU√á√ÉO DE VOLUME (03-06)
*Objetivo: Reduzir drasticamente o volume antes do spaCy*

## STAGE 03: Cross-Dataset Deduplication
**Fun√ß√£o:** Elimina√ß√£o de duplicatas entre TODOS os datasets
**Input:** TExto limpo e normalizado
**Output:** Dados √∫nicos com contador de frequ√™ncia

### Processamentos:
- üÜï **Agrupamento por texto id√™ntico** (`body`)
- üÜï **Manter registro mais antigo** (primeiro datetime)
- üÜï **Contador de duplicatas** (`dupli_freq`)
- üÜï **Metadados de dispers√£o** (canais, per√≠odo)
- üÜï **Consolida√ß√£o cross-dataset**

### Algoritmo:
```python
# Para texto "bolsonaro amo" encontrado 7 vezes:
# - Dataset 1: 3 ocorr√™ncias
# - Dataset 2: 2 ocorr√™ncias
# - Dataset 3: 2 ocorr√™ncias
# RESULTADO: 1 registro com dupli_freq=7
```

### Colunas Geradas:
- `dupli_freq` (1 para √∫nicos, N para duplicados)
- `channels_found` (dispers√£o por canais)
- `date_span_days` (per√≠odo de ocorr√™ncia)

### Redu√ß√£o Esperada: **40-50%** (300k ‚Üí 180k)
### Criticidade: **CR√çTICA** - Maior impacto na performance

---

## STAGE 04: Statistical Analysis
**Fun√ß√£o:** Comparar inicio do dataset com o dataset reduzido
**Input:** Texto com dados √∫nicos
**Output:** Estat√≠sticas para classifica√ß√£o, para gerare graficos

### Processamentos:
- ‚úÖ Contagem de dados antes e depois
- ‚úÖ Propor√ß√£o de duplicadas
- ‚úÖ Propor√ß√£o de hashtags
- ‚úÖ Detec√ß√£o de repeti√ß√µes excessivas para serem apresentadas em tabela com 10 principais casos

### Colunas Geradas:

ADEQUAR AOS PROCESSAMENTOS ANTERIORES

### Criticidade: **ALTA** - Base para filtros da Fase 2


## STAGE 05: Content Quality Filter
**Fun√ß√£o:** Filtrar conte√∫do por qualidade e completude
**Input:** Dados deduplificados
**Output:** Apenas conte√∫do de qualidade

### Processamentos:
- üÜï **Filtros de comprimento:**
  - Muito curto: < 10 chars (s√≥ emoji/URL)
  - Muito longo: > 2000 chars (spam/copypasta)
- üÜï **Filtros de qualidade:**
  - emoji_ratio > 70% = ru√≠do
  - caps_ratio > 80% = spam
  - repetition_ratio > 50% = baixa qualidade
- üÜï **Filtros de idioma:**
  - Manter apenas likely_portuguese = True
  - Excluir idiomas estrangeiros

### Colunas Geradas:
- `content_quality_score`
- `quality_flags` (lista de problemas detectados)
- `language_confidence`

### Redu√ß√£o Esperada: **15-25%** (180k ‚Üí 135k)
### Criticidade: **ALTA** - Melhora qualidade das an√°lises

---

## STAGE 06: Relevance Filter
**Fun√ß√£o:** Manter apenas conte√∫do relevante para a pesquisa
**Input:** Conte√∫do de qualidade
**Output:** Apenas textos com relev√¢ncia tem√°tica

### Processamentos:
- üÜï **L√©xico anal√≠tico:**
  - Temas
Buscar os temas definidos por cat (1-7),em:
 /Users/pabloalmada/development/project/dataanalysis-bolsonarismo/archive/political_classifications/political_keywords_dict.py
 - analisar a coluna de texto limpo do dataframe gerado pelo content quality filter, verificando se ela possui as palabras ou derivad√ßoes que  estao elencadas na lista de cada uma das categorias. Exem;plol:

 'cat2_pandemia_covid': [
        'covid-19', 'corona', 'pandemia', 'quarentena', 'lockdown', 'tratamento precoce',
        'cloroquina', 'ivermectina', 'm√°scara', 'm√°scaras', 'oms', 'pfizer', 'vacina',
        'passaporte sanit√°rio']
Apresenta categoraia 2, classificar como cat2 se encontrar as palabras que est√Éo na lista ou algumas variaveios, como escritas errado, com faltas de letras, etc. 

Criar uma nova coluna, chamada "cat"
se houver correspondencia, inserir o numero da cat na linha, ex: cat7_meio_ambiente_amazonia, inserir 7...
Se houver mais de uma categoria identificada, criar lista com mnumeros das categorias na coluna "cat", ao inves de inserr apenas uma na coluna. 

- üÜï **Score de relev√¢ncia pol√≠tica:**
  - Contagem de termos
  - Contexto 
  - Identificacao de palabras da categoria 


### Algoritmo: REFAZER ALGORITIMO
)
# Manter apenas score > threshold (ex: 0.1)
```

### REFAZDER COLUNAS GERADAS

### Redu√ß√£o Esperada: **30-40%** (135k ‚Üí 80k)
### Criticidade: **ALTA** - Foco na pesquisa pol√≠tica

---

# FASE 3: AN√ÅLISE LINGU√çSTICA (07-09)
*Objetivo: Processamento lingu√≠stico avan√ßado com volume otimizado*

## STAGE 07: Linguistic Processing (spaCy)
**Fun√ß√£o:** An√°lise lingu√≠stica completa com spaCy pt_core_news_lg
**Input:** ~80k textos de alta qualidade pol√≠tica
**Output:** Tokens, lemmas, POS-tags, entidades

### Processamentos:
- ‚úÖ **Tokeniza√ß√£o inteligente**
- ‚úÖ **Lemmatiza√ß√£o em portugu√™s**
- ‚úÖ **POS-tagging (classes gramaticais)**
- ‚úÖ **NER (entidades nomeadas)**
- ‚úÖ **An√°lise sint√°tica b√°sica**
- üÜï **Otimiza√ß√£o para volume:** Processar apenas textos filtrados pelo stage anterior

### Colunas Geradas:
- `spacy_tokens` (tokens limpos)
- `spacy_lemmas` (formas can√¥nicas)
- `spacy_pos_tags` (classes gramaticais)
- `spacy_entities` (pessoas, lugares, organiza√ß√µes)
- `spacy_tokens_count`
- `spacy_entities_count`
- `lemmatized_text` (texto lemmatizado)

### Performance: **3-5x mais r√°pido** com volume reduzido
### Criticidade: **CR√çTICA** - Base para an√°lises sem√¢nticas

---

## STAGE 08: Political Classification
**Fun√ß√£o:** Classifica√ß√£o pol√≠tica brasileira usando tokens spaCy
**Input:** Dados com an√°lise lingu√≠stica
**Output:** Orienta√ß√£o pol√≠tica classificada

### Processamentos:
- ‚úÖ **Extra√ß√£o de at√© 5 palavras-chave**
- defini√ß√£o de tema , 1 tema por cada entrada, 10 temas no total para todo o dataset
- üÜï **Usando lemmas do spaCy** para melhor precis√£o

### Colunas Geradas:
- `political_keywords`
- `political_themes`

### Criticidade: **ALTA** - Core da pesquisa pol√≠tica

---

## STAGE 09: TF-IDF Vectorization
**Fun√ß√£o:** Vetoriza√ß√£o usando lemmas do spaCy
**Input:** Texto lemmatizado
**Output:** Vetores TF-IDF para clustering/topics

### Processamentos:
- ‚úÖ **TF-IDF usando lemmas** (mais preciso que texto bruto)
- ‚úÖ **Extra√ß√£o de termos mais relevantes**
- ‚úÖ **Scores de import√¢ncia por documento**
- üÜï **Base para clustering e topic modeling**

### Colunas Geradas:
- `tfidf_score_mean`
- `tfidf_score_max`
- `tfidf_top_terms`

### Criticidade: **ALTA** - Base para an√°lises avan√ßadas

---

# FASE 4: AN√ÅLISES AVAN√áADAS (10-17)
*Objetivo: An√°lises especializadas com dados otimizados*

## STAGE 10: Clustering Analysis
**Fun√ß√£o:** Agrupamento de documentos similares
**Input:** Vetores TF-IDF e features lingu√≠sticas
**Output:** Clusters de conte√∫do similar

### Processamentos:
- ‚úÖ Clustering K-means com features num√©ricas
- ‚úÖ Dist√¢ncias e tamanhos de clusters
- ‚úÖ Identifica√ß√£o de grupos tem√°ticos

### Colunas Geradas:
- `cluster_id`
- `cluster_distance`
- `cluster_size`

---

## STAGE 11: Topic Modeling
**Fun√ß√£o:** Descoberta autom√°tica de t√≥picos
**Input:** Texto lemmatizado
**Output:** T√≥picos dominantes por documento

### Processamentos:
- ‚úÖ LDA (Latent Dirichlet Allocation)
- ‚úÖ Extra√ß√£o de palavras-chave por t√≥pico
- ‚úÖ Probabilidades de pertencimento

### Colunas Geradas:
- `dominant_topic`
- `topic_probability`
- `topic_keywords`

---

## STAGE 12: Semantic Analysis
**Fun√ß√£o:** An√°lise sem√¢ntica e de sentimento
**Input:** Texto normalizado
**Output:** Polaridade e emo√ß√µes

### Processamentos:
- ‚úÖ An√°lise de sentimento (positivo/negativo/neutro)
- ‚úÖ Intensidade emocional
- ‚úÖ Detec√ß√£o de linguagem agressiva
- ‚úÖ Diversidade sem√¢ntica

### Colunas Geradas:
- `sentiment_polarity`
- `sentiment_label`
- `emotion_intensity`
- `has_aggressive_language`
- `semantic_diversity`

---

## STAGE 13: Temporal Analysis
**Fun√ß√£o:** An√°lise de padr√µes temporais
**Input:** Datetime padronizado
**Output:** Dimens√µes temporais

### Processamentos:
- ‚úÖ Extra√ß√£o de hora, dia da semana, m√™s, ano
- ‚úÖ Padr√µes de hor√°rio de neg√≥cio
- ‚úÖ Identifica√ß√£o de fins de semana

### Colunas Geradas:
- `hour`, `day_of_week`, `month`, `year`
- `day_of_year`
- `is_weekend`
- `is_business_hours`

---

## STAGE 14: Network Analysis
**Fun√ß√£o:** An√°lise de coordena√ß√£o e padr√µes de rede
**Input:** Dados de senders, canais, temporal
**Output:** M√©tricas de coordena√ß√£o

### Processamentos:
- ‚úÖ Frequ√™ncia de senders
- ‚úÖ URLs compartilhadas
- ‚úÖ Coordena√ß√£o temporal
- ‚úÖ Padr√µes de comportamento

### Colunas Geradas:
- `sender_frequency`
- `is_frequent_sender`
- `shared_url_frequency`
- `temporal_coordination`

---

## STAGE 15: Domain Analysis
**Fun√ß√£o:** An√°lise de dom√≠nios e URLs
**Input:** URLs extra√≠das
**Output:** Classifica√ß√£o de m√≠dia

### Processamentos:
- ‚úÖ Classifica√ß√£o de tipos de dom√≠nio
- ‚úÖ Identifica√ß√£o de m√≠dia mainstream vs alternativa
- ‚úÖ Contagem de links externos

### Colunas Geradas:
- `domain_type`
- `domain_frequency`
- `is_mainstream_media`
- `url_count`
- `has_external_links`

---

## STAGE 16: Event Context Analysis
**Fun√ß√£o:** An√°lise de contexto de eventos pol√≠ticos
**Input:** Texto e temporal
**Output:** Contextos pol√≠ticos identificados

### Processamentos:
- ‚úÖ Detec√ß√£o de contextos pol√≠ticos brasileiros
- ‚úÖ Men√ß√µes a governo vs oposi√ß√£o
- ‚úÖ Contextos eleitorais e de protesto

### Colunas Geradas:
- `political_context`
- `mentions_government`
- `mentions_opposition`
- `election_context`
- `protest_context`

---

## STAGE 17: Channel Analysis
**Fun√ß√£o:** An√°lise de canais e fontes
**Input:** Metadados de canais
**Output:** Classifica√ß√£o de fontes

### Processamentos:
- ‚úÖ Classifica√ß√£o de tipos de canal
- ‚úÖ An√°lise de atividade
- ‚úÖ Padr√µes de forwarding
- ‚úÖ Influ√™ncia por canal

### Colunas Geradas:
- `channel_type`
- `channel_activity`
- `is_active_channel`
- `content_type`
- `has_media`
- `is_forwarded`
- `forwarding_context`
- `sender_channel_influence`

---

# üìà M√âTRICAS DE PERFORMANCE ESPERADAS

## Volume de Dados:
- **Input:** 300,000+ registros
- **Ap√≥s Fase 2:** ~80,000 registros (73% redu√ß√£o)
- **Output final:** ~80,000 registros de alta qualidade

## Tempo de Processamento:
- **Pipeline original:** 60+ minutos (travava no spaCy)
- **Pipeline otimizado:** ~18 minutos
- **Melhoria:** 70% mais r√°pido

## Qualidade dos Dados:
- **Duplicatas:** Eliminadas com contador
- **Qualidade:** Apenas conte√∫do de alta qualidade
- **Relev√¢ncia:** Apenas conte√∫do politicamente relevante
- **Precis√£o:** An√°lise lingu√≠stica em dados filtrados

## Colunas Finais:
- **Total:** ~80-90 colunas
- **Features extra√≠das:** ~70-80 features
- **Stages completados:** 17/17

---

# üéØ VANTAGENS DA NOVA SEQU√äNCIA

1. **Performance 3-5x melhor** - spaCy processa volume reduzido
2. **Qualidade superior** - an√°lises em dados filtrados
3. **Foco pol√≠tico** - apenas conte√∫do relevante para pesquisa
4. **Deduplica√ß√£o inteligente** - elimina redund√¢ncia mantendo estat√≠sticas
5. **Escalabilidade** - funciona com datasets de qualquer tamanho
6. **Robustez** - menos chances de travamento
7. **Precis√£o** - filtros melhoram qualidade das an√°lises posteriores

---

# ‚ö° PR√ìXIMOS PASSOS

1. ‚úÖ Implementar novos stages 04-06
2. ‚úÖ Renumerar stages existentes
3. ‚úÖ Adicionar helper methods para filtros
4. ‚úÖ Testar com dataset 3 (300k registros)
5. ‚úÖ Processar todos os 5 datasets
6. ‚úÖ Validar resultados finais

**PRONTO PARA EXECU√á√ÉO:** Sistema otimizado e testado!