#!/usr/bin/env python3
"""
Verificar Duplica√ß√£o de Colunas
===============================

Analisa se h√° colunas duplicadas com mesmo conte√∫do ap√≥s processamento.
"""

import pandas as pd
import logging
import sys
from pathlib import Path

# Adicionar src ao path
sys.path.append('src')
from analyzer import Analyzer

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_column_duplication():
    """Verificar duplica√ß√£o de colunas ap√≥s processamento."""
    logger.info("üîç VERIFICA√á√ÉO DE DUPLICA√á√ÉO DE COLUNAS")
    logger.info("=" * 50)

    dataset_path = Path("data/1_2019-2021-govbolso.csv")

    if not dataset_path.exists():
        logger.error(f"‚ùå Dataset n√£o encontrado: {dataset_path}")
        return

    # Carregar amostra
    df_sample = pd.read_csv(dataset_path, sep=',', nrows=10, encoding='utf-8')
    logger.info(f"‚úÖ Amostra carregada: {len(df_sample)} registros")

    # Mostrar colunas originais
    logger.info(f"\nüìä COLUNAS ORIGINAIS ({len(df_sample.columns)}):")
    for col in df_sample.columns:
        print(f"   ‚Ä¢ {col}")

    # Executar processamento completo
    analyzer = Analyzer()
    df_stage01 = analyzer._stage_01_feature_extraction(df_sample.copy())
    df_final = analyzer._stage_02_text_preprocessing(df_stage01)

    logger.info(f"\nüìä COLUNAS FINAIS ({len(df_final.columns)}):")
    for col in df_final.columns:
        print(f"   ‚Ä¢ {col}")

    # === AN√ÅLISE DE DUPLICA√á√ÉO ===
    logger.info("\nüîç AN√ÅLISE DE POSS√çVEIS DUPLICA√á√ïES:")
    print("=" * 60)

    potential_duplicates = [
        ('datetime', 'datetime_standardized'),
        ('hashtag', 'hashtags_extracted'),
        ('url', 'urls_extracted'),
        ('mentions', 'mentions_extracted'),
        ('body', 'normalized_text'),
        ('body', 'body_cleaned')
    ]

    duplications_found = []

    for col1, col2 in potential_duplicates:
        if col1 in df_final.columns and col2 in df_final.columns:
            logger.info(f"\nüîπ COMPARANDO: {col1} vs {col2}")

            # Amostras dos dados
            sample1 = df_final[col1].dropna().head(3).tolist()
            sample2 = df_final[col2].dropna().head(3).tolist()

            print(f"   {col1}:")
            for i, val in enumerate(sample1):
                print(f"      {i+1}. {str(val)[:80]}...")

            print(f"   {col2}:")
            for i, val in enumerate(sample2):
                print(f"      {i+1}. {str(val)[:80]}...")

            # Verificar se s√£o funcionalmente duplicadas
            if len(sample1) > 0 and len(sample2) > 0:
                if str(sample1[0]).lower() == str(sample2[0]).lower():
                    duplications_found.append((col1, col2))
                    logger.warning(f"‚ö†Ô∏è POSS√çVEL DUPLICA√á√ÉO: {col1} ‚âà {col2}")
                else:
                    logger.info(f"‚úÖ DIFERENTES: {col1} ‚â† {col2}")

    # === AN√ÅLISE DE FEATURES ESPEC√çFICAS ===
    logger.info(f"\nüîñ AN√ÅLISE DE FEATURES ESPEC√çFICAS:")
    print("=" * 60)

    feature_analysis = {
        'hashtags': [],
        'urls': [],
        'mentions': [],
        'datetime': []
    }

    # Coletar todas as colunas relacionadas a cada feature
    for col in df_final.columns:
        if 'hashtag' in col.lower():
            feature_analysis['hashtags'].append(col)
        elif 'url' in col.lower():
            feature_analysis['urls'].append(col)
        elif 'mention' in col.lower():
            feature_analysis['mentions'].append(col)
        elif 'datetime' in col.lower() or 'time' in col.lower():
            feature_analysis['datetime'].append(col)

    for feature_type, related_columns in feature_analysis.items():
        if len(related_columns) > 1:
            logger.info(f"\nüîπ {feature_type.upper()} - {len(related_columns)} colunas:")
            for col in related_columns:
                sample_data = df_final[col].dropna().head(2).tolist()
                print(f"   ‚Ä¢ {col}: {sample_data}")

    # === RECOMENDA√á√ïES ===
    logger.info(f"\nüí° RECOMENDA√á√ïES:")
    print("=" * 40)

    if duplications_found:
        logger.warning(f"‚ö†Ô∏è {len(duplications_found)} poss√≠veis duplica√ß√µes encontradas")
        for col1, col2 in duplications_found:
            logger.info(f"   ‚Ä¢ Considerar remover: {col1} ou {col2}")
    else:
        logger.info("‚úÖ Nenhuma duplica√ß√£o real encontrada")

    # Verificar se colunas originais ainda s√£o necess√°rias
    original_features = ['hashtag', 'url', 'mentions', 'datetime']
    extracted_features = ['hashtags_extracted', 'urls_extracted', 'mentions_extracted', 'datetime_standardized']

    logger.info(f"\nüìã AN√ÅLISE DE NECESSIDADE:")
    for orig, extr in zip(original_features, extracted_features):
        if orig in df_final.columns and extr in df_final.columns:
            orig_valid = df_final[orig].notna().sum()
            extr_valid = df_final[extr].notna().sum() if extr != 'datetime_standardized' else df_final[extr].notna().sum()

            if orig_valid == 0 and extr_valid > 0:
                logger.info(f"   ‚Ä¢ {orig}: PODE SER REMOVIDA (vazia, {extr} tem dados)")
            elif orig_valid > 0 and extr_valid > 0:
                logger.info(f"   ‚Ä¢ {orig}: MANTER (dados √∫nicos complementares a {extr})")

    logger.info("\nüéâ AN√ÅLISE DE DUPLICA√á√ÉO CONCLU√çDA!")

if __name__ == "__main__":
    check_column_duplication()